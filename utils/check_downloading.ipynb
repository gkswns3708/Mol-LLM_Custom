{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae2e776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Checking TRAIN Split ====================\n",
      "Path: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/real_train/smol-forward_synthesis_subtask-smol-forward_synthesis_train\n",
      "âŒ [Missing] í´ë”ê°€ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "\n",
      "==================== Checking VAL Split ====================\n",
      "Path: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/real_train/smol-forward_synthesis_subtask-smol-forward_synthesis_val\n",
      "âŒ [Missing] í´ë”ê°€ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "\n",
      "==================== Checking TEST Split ====================\n",
      "Path: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/real_train/smol-forward_synthesis_subtask-smol-forward_synthesis_test\n",
      "âŒ [Missing] í´ë”ê°€ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import os\n",
    "import torch\n",
    "from pprint import pprint\n",
    "\n",
    "# ==========================================\n",
    "# [ì„¤ì •] config íŒŒì¼ì˜ raw_data_root ê²½ë¡œì™€ ë§žì¶”ì„¸ìš”\n",
    "# ==========================================\n",
    "RAW_DATA_ROOT = \"/home/jovyan/CHJ/Mol-LLM_Custom/dataset/real_train\"  # (ì˜ˆì‹œ) ì‹¤ì œ ë°ì´í„°ê°€ ì €ìž¥ëœ í´ë” ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”.\n",
    "TASK_NAME = \"smol-forward_synthesis\"\n",
    "# smol ë°ì´í„°ì…‹ì€ subtask ì´ë¦„ì´ task ì´ë¦„ê³¼ ê°™ì€ ê²½ìš°ê°€ ë§ŽìŠµë‹ˆë‹¤.\n",
    "SUBTASK_NAME = \"smol-forward_synthesis\" \n",
    "\n",
    "def check_split(split_name):\n",
    "    # ì €ìž¥ ë¡œì§: f\"{raw_data_root}/{task_name}_subtask-{subtask_idx}_{split}\"\n",
    "    # ì£¼ì˜: í´ë” ì´ë¦„ì´ ì •í™•í•´ì•¼ í•©ë‹ˆë‹¤. ì €ìž¥ëœ í´ë”ëª…ì„ ì§ì ‘ í™•ì¸í•´ë³´ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "    dir_name = f\"{TASK_NAME}_subtask-{SUBTASK_NAME}_{split_name}\"\n",
    "    dataset_path = os.path.join(RAW_DATA_ROOT, dir_name)\n",
    "\n",
    "    print(f\"\\n{'='*20} Checking {split_name.upper()} Split {'='*20}\")\n",
    "    print(f\"Path: {dataset_path}\")\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"âŒ [Missing] í´ë”ê°€ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        ds = datasets.load_from_disk(dataset_path)\n",
    "        print(f\"âœ… [Load Success] ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ\")\n",
    "        print(f\"ðŸ“Š [Count] ë°ì´í„° ê°œìˆ˜: {len(ds)}\")\n",
    "\n",
    "        if len(ds) == 0:\n",
    "            print(\"âš ï¸ [Empty] ë°ì´í„°ì…‹ì´ ë¹„ì–´ ìžˆìŠµë‹ˆë‹¤. (ì´ì „ ì—ëŸ¬ ë°©ì§€ ë¡œì§ì— ì˜í•´ ìƒì„±ëœ ë¹ˆ íŒŒì¼ì¼ ìˆ˜ ìžˆìŒ)\")\n",
    "            print(f\"   Features (Schema): {ds.features}\")\n",
    "            return\n",
    "\n",
    "        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
    "        print(\"\\nðŸ” [Sample Inspection - First Item]\")\n",
    "        sample = ds[0]\n",
    "        \n",
    "        # ì£¼ìš” í•„ë“œ í™•ì¸\n",
    "        required_keys = ['x', 'edge_index', 'label', 'instruction', 'input_mol_string']\n",
    "        missing_keys = [key for key in required_keys if key not in sample]\n",
    "        \n",
    "        if missing_keys:\n",
    "            print(f\"âŒ [Missing Keys] í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {missing_keys}\")\n",
    "        else:\n",
    "            print(f\"âœ… [Keys Check] í•„ìˆ˜ í•„ë“œ í¬í•¨ë¨\")\n",
    "\n",
    "        # ë°ì´í„° ë‚´ìš© ì¼ë¶€ ì¶œë ¥\n",
    "        print(f\" - Instruction: {sample.get('instruction')}\")\n",
    "        print(f\" - Label: {sample.get('label')}\")\n",
    "        print(f\" - Input Mol String: {sample.get('input_mol_string')}\")\n",
    "        \n",
    "        # Graph Tensor í™•ì¸\n",
    "        x_data = sample.get('x')\n",
    "        if x_data:\n",
    "            print(f\" - Graph Node Feature (x) shape: {torch.tensor(x_data).shape}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ [Graph Warning] Node Feature (x)ê°€ ë¹„ì–´ìžˆê±°ë‚˜ Noneìž…ë‹ˆë‹¤.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ [Error] ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Train, Valid, Test ëª¨ë‘ í™•ì¸\n",
    "    check_split(\"train\")\n",
    "    check_split(\"val\")\n",
    "    check_split(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2dfe97",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory /home/jovyan/CHJ/Mol-LLM_Custom/dataset/real_train/smol-forward_synthesis_subtask-0_val is neither a `Dataset` directory nor a `DatasetDict` directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_from_disk\n\u001b[0;32m----> 3\u001b[0m smol \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/jovyan/CHJ/Mol-LLM_Custom/dataset/real_train/smol-forward_synthesis_subtask-0_val\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m smol\n",
      "File \u001b[0;32m~/miniconda3/envs/MolDA_CHJ/lib/python3.10/site-packages/datasets/load.py:2697\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m   2696\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2697\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   2698\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is neither a `Dataset` directory nor a `DatasetDict` directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2699\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory /home/jovyan/CHJ/Mol-LLM_Custom/dataset/real_train/smol-forward_synthesis_subtask-0_val is neither a `Dataset` directory nor a `DatasetDict` directory."
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "smol = load_from_disk('/home/jovyan/CHJ/Mol-LLM_Custom/dataset/real_train/smol-forward_synthesis_subtask-0_val')\n",
    "smol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f1e2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MolDA_CHJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
