{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf6cfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLaDA Dataset Filtering (Threshold: 512) ===\n",
      "Tokenizer loaded. Vocab size: 129325\n",
      "\n",
      "Processing: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_validation_3.3M_0415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying & Calculating (num_proc=64): 100%|██████████| 70906/70906 [00:01<00:00, 48506.30 examples/s]\n",
      "Filtering (num_proc=64): 100%|██████████| 70906/70906 [00:01<00:00, 41417.21 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ------------------------------------------------\n",
      "  Original Size   : 70906\n",
      "  Filtered Size   : 70565\n",
      "  Dropped Samples : 341\n",
      "  Top Drop Reasons: [('Length Exceeded (559 > 512)', 7), ('Length Exceeded (549 > 512)', 6), ('Length Exceeded (602 > 512)', 6), ('Length Exceeded (674 > 512)', 6), ('Length Exceeded (878 > 512)', 6)]\n",
      "  ------------------------------------------------\n",
      "  Saving to: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_validation_3.3M_0415_verified_filtered_512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 70565/70565 [00:01<00:00, 38700.35 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_test_3.3M_0415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying & Calculating (num_proc=64): 100%|██████████| 70906/70906 [00:01<00:00, 48420.20 examples/s]\n",
      "Filtering (num_proc=64): 100%|██████████| 70906/70906 [00:01<00:00, 48154.94 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ------------------------------------------------\n",
      "  Original Size   : 70906\n",
      "  Filtered Size   : 70565\n",
      "  Dropped Samples : 341\n",
      "  Top Drop Reasons: [('Length Exceeded (559 > 512)', 7), ('Length Exceeded (549 > 512)', 6), ('Length Exceeded (602 > 512)', 6), ('Length Exceeded (674 > 512)', 6), ('Length Exceeded (878 > 512)', 6)]\n",
      "  ------------------------------------------------\n",
      "  Saving to: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_test_3.3M_0415_verified_filtered_512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 70565/70565 [00:01<00:00, 37885.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_train_3.3M_0415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying & Calculating (num_proc=64): 100%|██████████| 3450540/3450540 [00:46<00:00, 74325.36 examples/s] \n",
      "Filtering (num_proc=64): 100%|██████████| 3450540/3450540 [00:55<00:00, 61881.54 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ------------------------------------------------\n",
      "  Original Size   : 3450540\n",
      "  Filtered Size   : 3442043\n",
      "  Dropped Samples : 8497\n",
      "  Top Drop Reasons: [('Length Exceeded (513 > 512)', 59), ('Length Exceeded (520 > 512)', 50), ('Length Exceeded (531 > 512)', 49), ('Length Exceeded (522 > 512)', 49), ('Length Exceeded (524 > 512)', 48)]\n",
      "  ------------------------------------------------\n",
      "  Saving to: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_train_3.3M_0415_verified_filtered_512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (45/45 shards): 100%|██████████| 3442043/3442043 [01:32<00:00, 37056.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "import multiprocessing as mp\n",
    "\n",
    "# ==========================================\n",
    "# [설정] 임계값 및 경로 설정\n",
    "# ==========================================\n",
    "NUM_PROC = 64           # 병렬 프로세스 수\n",
    "THRESHOLD = 512         # 최대 토큰 길이 (Prompt + Target)\n",
    "MODEL_ID = \"GSAI-ML/LLaDA-8B-Instruct\"\n",
    "\n",
    "# 검사할 데이터셋 경로 리스트\n",
    "TARGET_PATHS = [\n",
    "    \"/home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_validation_3.3M_0415\",\n",
    "    \"/home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_test_3.3M_0415\",\n",
    "    \"/home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_train_3.3M_0415\"\n",
    "]\n",
    "\n",
    "# SELFIES 등 특수 토큰 사전 경로 (길이 계산 정확도 향상용)\n",
    "SELFIES_DICT_PATH = \"/home/jovyan/CHJ/Mol-LLM_Custom/model/selfies_dict.txt\"\n",
    "\n",
    "# 멀티프로세싱 설정\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# ==========================================\n",
    "# [1] Special Tokens 정의 (길이 계산용)\n",
    "# ==========================================\n",
    "# Mol-LLM Custom Tokens\n",
    "CUSTOM_SPECIAL_TOKENS = [\n",
    "    \"<BOOLEAN>\", \"</BOOLEAN>\", \"<FLOAT>\", \"</FLOAT>\", \"<DESCRIPTION>\", \"</DESCRIPTION>\",\n",
    "    \"<SELFIES>\", \"</SELFIES>\", \"<GRAPH>\", \"</GRAPH>\", \"<3D_CONFORMER>\", \"</3D_CONFORMER>\",\n",
    "    \"<mol>\", \"<INSTRUCTION>\", \"</INSTRUCTION>\", \"|>>|\", \"<IUPAC>\", \"</IUPAC>\", \"<MOLFORMULA>\", \"</MOLFORMULA>\"\n",
    "]\n",
    "# 숫자 토큰 추가\n",
    "CUSTOM_SPECIAL_TOKENS.extend([f\"<|{i}|>\" for i in range(10)] + [\"<|+|>\", \"<|-|>\", \"<|.|>\"])\n",
    "\n",
    "# 전역 토크나이저 변수 (Worker 프로세스용)\n",
    "global_tokenizer = None\n",
    "\n",
    "def load_selfies_tokens(path):\n",
    "    if not os.path.exists(path):\n",
    "        return []\n",
    "    with open(path, 'r') as f:\n",
    "        tokens = f.read().splitlines()\n",
    "    return [t.strip() for t in tokens if t.strip()]\n",
    "\n",
    "def init_worker():\n",
    "    \"\"\"\n",
    "    각 워커 프로세스에서 토크나이저를 로드하고 특수 토큰을 등록합니다.\n",
    "    (이렇게 해야 정확한 토큰 개수를 셀 수 있습니다.)\n",
    "    \"\"\"\n",
    "    global global_tokenizer\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "        # 1. Custom Tags 추가\n",
    "        tokens_to_add = list(set(CUSTOM_SPECIAL_TOKENS))\n",
    "        \n",
    "        # 2. SELFIES Dictionary 추가\n",
    "        selfies_tokens = load_selfies_tokens(SELFIES_DICT_PATH)\n",
    "        tokens_to_add.extend(selfies_tokens)\n",
    "        \n",
    "        # 3. 토크나이저에 추가 (중복 제거)\n",
    "        existing_vocab = set(tokenizer.get_vocab().keys())\n",
    "        final_tokens = [t for t in set(tokens_to_add) if t not in existing_vocab]\n",
    "        \n",
    "        if final_tokens:\n",
    "            tokenizer.add_tokens(final_tokens)\n",
    "            \n",
    "        global_tokenizer = tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"[Worker Error] Failed to load tokenizer: {e}\")\n",
    "\n",
    "def check_llada_format(prompt, target):\n",
    "    \"\"\"\n",
    "    LLaDA 포맷 검증 (생성 코드 기준)\n",
    "    \"\"\"\n",
    "    # 1. 텍스트 존재 여부 확인\n",
    "    if not prompt or not target:\n",
    "        return False, \"Empty Text\"\n",
    "\n",
    "    # 2. LLaDA 프롬프트 포맷 검사 (수정됨)\n",
    "    # 생성 코드: \"<|startoftext|>\", \"<|start_header_id|>system...\", \"<|eot_id|>\"\n",
    "    required_tags = [\n",
    "        \"<|startoftext|>\",  # [수정] <|begin_of_text|> -> <|startoftext|>\n",
    "        \"<|start_header_id|>system<|end_header_id|>\",\n",
    "        \"<|start_header_id|>user<|end_header_id|>\",\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    ]\n",
    "    \n",
    "    for tag in required_tags:\n",
    "        if tag not in prompt:\n",
    "            return False, f\"Missing Tag: {tag}\"\n",
    "    \n",
    "    # 3. Target EOS 검사 (수정됨)\n",
    "    # 생성 코드: formatted_target_text = ... + \"<|eot_id|>\"\n",
    "    if \"<|eot_id|>\" not in target: # [수정] <|end_of_text|> -> <|eot_id|>\n",
    "        return False, \"Missing Target EOS (<|eot_id|>)\"\n",
    "\n",
    "    return True, \"OK\"\n",
    "\n",
    "def process_batch(batch):\n",
    "    \"\"\"\n",
    "    배치 단위로 포맷 검사 및 길이 계산 수행\n",
    "    \"\"\"\n",
    "    # 워커 초기화 확인 (global_tokenizer)\n",
    "    if global_tokenizer is None:\n",
    "        init_worker()\n",
    "        \n",
    "    prompts = batch['prompt_text']\n",
    "    targets = batch['target_text']\n",
    "    # 그래프 데이터(x)가 없으면 None으로 채움 (Optional 처리)\n",
    "    xs = batch.get('x', [None] * len(prompts)) \n",
    "    \n",
    "    keeps = []\n",
    "    reasons = []\n",
    "    \n",
    "    # 1. 포맷 및 그래프 데이터 검사\n",
    "    valid_indices = []\n",
    "    texts_to_tokenize = []\n",
    "    \n",
    "    for i in range(len(prompts)):\n",
    "        p = prompts[i]\n",
    "        t = targets[i]\n",
    "        \n",
    "        # A. 포맷 검사\n",
    "        is_valid_fmt, reason = check_llada_format(p, t)\n",
    "        \n",
    "        # B. 그래프 데이터 검사 (x가 None이면 비정상)\n",
    "        # 데이터셋에 x 컬럼이 아예 없으면 위에서 None 리스트로 처리되어 여기서 걸림\n",
    "        # x 컬럼이 있지만 값이 None인 경우도 처리\n",
    "        if xs[i] is None and 'x' in batch: \n",
    "             is_valid_fmt = False\n",
    "             reason = \"Empty Graph Node Features (x)\"\n",
    "\n",
    "        if is_valid_fmt:\n",
    "            valid_indices.append(i)\n",
    "            texts_to_tokenize.append(p + t) # 길이 계산용 텍스트\n",
    "            keeps.append(True) # 임시 True (길이 검사 전)\n",
    "            reasons.append(\"OK\")\n",
    "        else:\n",
    "            keeps.append(False)\n",
    "            reasons.append(reason)\n",
    "            \n",
    "    # 2. 길이 검사 (유효한 포맷인 것만 토큰화하여 성능 최적화)\n",
    "    if texts_to_tokenize:\n",
    "        # padding=False로 실제 길이 측정\n",
    "        tokenized = global_tokenizer(texts_to_tokenize, add_special_tokens=False)\n",
    "        lengths = [len(ids) for ids in tokenized['input_ids']]\n",
    "        \n",
    "        for idx, length in zip(valid_indices, lengths):\n",
    "            if length > THRESHOLD:\n",
    "                keeps[idx] = False\n",
    "                reasons[idx] = f\"Length Exceeded ({length} > {THRESHOLD})\"\n",
    "                \n",
    "    return {\n",
    "        \"keep\": keeps,\n",
    "        \"drop_reason\": reasons\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    print(f\"=== LLaDA Dataset Filtering (Threshold: {THRESHOLD}) ===\")\n",
    "    \n",
    "    # 메인 프로세스에서 토크나이저 테스트\n",
    "    init_worker()\n",
    "    if global_tokenizer:\n",
    "        print(f\"Tokenizer loaded. Vocab size: {len(global_tokenizer)}\")\n",
    "    else:\n",
    "        print(\"Failed to load tokenizer in main process.\")\n",
    "        return\n",
    "\n",
    "    for path in TARGET_PATHS:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"\\n[Skip] Path not found: {path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing: {path}\")\n",
    "        \n",
    "        try:\n",
    "            dataset = load_from_disk(path)\n",
    "            original_size = len(dataset)\n",
    "            \n",
    "            # 1. Map: 검증 및 필터링 플래그 생성\n",
    "            # (load_from_cache_file=False로 설정하여 이전의 잘못된 캐시 사용 방지)\n",
    "            processed = dataset.map(\n",
    "                process_batch,\n",
    "                batched=True,\n",
    "                num_proc=NUM_PROC,\n",
    "                desc=\"Verifying & Calculating\",\n",
    "                load_from_cache_file=False\n",
    "            )\n",
    "            \n",
    "            # 2. 통계 집계\n",
    "            drop_reasons = processed['drop_reason']\n",
    "            stats = Counter(drop_reasons)\n",
    "            if \"OK\" in stats: del stats[\"OK\"]\n",
    "            \n",
    "            # 3. Filter: keep=True인 것만 남김\n",
    "            filtered_dataset = processed.filter(\n",
    "                lambda x: x['keep'],\n",
    "                num_proc=NUM_PROC,\n",
    "                desc=\"Filtering\"\n",
    "            )\n",
    "            \n",
    "            # 4. 임시 컬럼 제거\n",
    "            final_dataset = filtered_dataset.remove_columns(['keep', 'drop_reason'])\n",
    "            \n",
    "            filtered_size = len(final_dataset)\n",
    "            dropped_count = original_size - filtered_size\n",
    "            \n",
    "            print(f\"  ------------------------------------------------\")\n",
    "            print(f\"  Original Size   : {original_size}\")\n",
    "            print(f\"  Filtered Size   : {filtered_size}\")\n",
    "            print(f\"  Dropped Samples : {dropped_count}\")\n",
    "            if dropped_count > 0:\n",
    "                # 상위 5개 드랍 사유 출력\n",
    "                print(f\"  Top Drop Reasons: {stats.most_common(5)}\")\n",
    "            print(f\"  ------------------------------------------------\")\n",
    "            \n",
    "            # 5. 저장\n",
    "            if dropped_count > 0 or original_size > 0:\n",
    "                save_path = path.rstrip('/') + f\"_verified_filtered_{THRESHOLD}\"\n",
    "                print(f\"  Saving to: {save_path}\")\n",
    "                final_dataset.save_to_disk(save_path)\n",
    "            else:\n",
    "                print(\"  No data to save.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  [Error] Failed to process {path}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Multiprocessing 시작 방식 설정\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0eb2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MolDA_CHJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
