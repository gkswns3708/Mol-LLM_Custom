{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd61c4c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f3d82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/miniconda3/envs/MolDA_CHJ/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/jovyan/miniconda3/envs/MolDA_CHJ/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLaDA Dataset Verification & Filtering (Threshold: 512, Process: 32) ===\n",
      "Loading Tokenizer from: GSAI-ML/LLaDA-8B-Instruct\n",
      "\n",
      "Processing Dataset: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_validation_3.3M_0415\n",
      "  > Verifying and Checking Length (Num Proc: 32)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " analyzing (num_proc=32): 100%|██████████| 79884/79884 [00:06<00:00, 13107.89 examples/s]\n",
      " filtering (num_proc=32): 100%|██████████| 79884/79884 [00:02<00:00, 33910.09 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ------------------------------------------------\n",
      "  Original Size   : 79884\n",
      "  Filtered Size   : 76164\n",
      "  Dropped Samples : 3720\n",
      "  Drop Reasons    : {'Length Exceeded': 3720}\n",
      "  ------------------------------------------------\n",
      "  Saving filtered dataset to: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_validation_3.3M_0415_verified_filtered_512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 76164/76164 [00:01<00:00, 38670.05 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Dataset: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_test_3.3M_0415\n",
      "  > Verifying and Checking Length (Num Proc: 32)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " analyzing (num_proc=32): 100%|██████████| 79884/79884 [00:06<00:00, 12711.72 examples/s]\n",
      " filtering (num_proc=32): 100%|██████████| 79884/79884 [00:02<00:00, 32842.58 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ------------------------------------------------\n",
      "  Original Size   : 79884\n",
      "  Filtered Size   : 76164\n",
      "  Dropped Samples : 3720\n",
      "  Drop Reasons    : {'Length Exceeded': 3720}\n",
      "  ------------------------------------------------\n",
      "  Saving filtered dataset to: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_test_3.3M_0415_verified_filtered_512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 76164/76164 [00:01<00:00, 39963.77 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Dataset: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_train_3.3M_0415\n",
      "  > Verifying and Checking Length (Num Proc: 32)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " analyzing (num_proc=32): 100%|██████████| 3450540/3450540 [02:37<00:00, 21886.27 examples/s]\n",
      " filtering (num_proc=32): 100%|██████████| 3450540/3450540 [01:34<00:00, 36338.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ------------------------------------------------\n",
      "  Original Size   : 3450540\n",
      "  Filtered Size   : 3275725\n",
      "  Dropped Samples : 174815\n",
      "  Drop Reasons    : {'Length Exceeded': 174815}\n",
      "  ------------------------------------------------\n",
      "  Saving filtered dataset to: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_train_3.3M_0415_verified_filtered_512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (43/43 shards): 100%|██████████| 3275725/3275725 [01:36<00:00, 34087.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# ==========================================\n",
    "# [설정] 임계값 및 경로 설정\n",
    "# ==========================================\n",
    "# 병렬 프로세스 개수 (CPU 코어 수에 맞춰 조절)\n",
    "NUM_PROC = 32\n",
    "\n",
    "# 필터링 할 최대 토큰 길이 (Prompt + Target)\n",
    "THRESHOLD = 512 \n",
    "\n",
    "# 검사할 데이터셋 경로 리스트\n",
    "TARGET_PATHS = [\n",
    "    \"/home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_validation_3.3M_0415\",\n",
    "    \"/home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_test_3.3M_0415\",\n",
    "    \"/home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_train_3.3M_0415\"\n",
    "]\n",
    "\n",
    "# 토크나이저 경로\n",
    "TOKENIZER_PATH = \"GSAI-ML/LLaDA-8B-Instruct\" \n",
    "\n",
    "# 멀티프로세싱 시 토크나이저 병렬 처리 충돌 방지\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def check_llada_format(example):\n",
    "    \"\"\"\n",
    "    LLaDA Instruction Format 및 데이터 무결성을 검사합니다.\n",
    "    \"\"\"\n",
    "    prompt = example.get('prompt_text', '')\n",
    "    target = example.get('target_text', '')\n",
    "    \n",
    "    # 1. 텍스트 존재 여부 확인\n",
    "    if not prompt or not target:\n",
    "        return False, \"Empty Text\"\n",
    "\n",
    "    # 2. LLaDA 프롬프트 포맷 검사\n",
    "    required_tags = [\n",
    "        \"<|begin_of_text|>\",\n",
    "        \"<|start_header_id|>system<|end_header_id|>\",\n",
    "        \"<|start_header_id|>user<|end_header_id|>\",\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    ]\n",
    "    for tag in required_tags:\n",
    "        if tag not in prompt:\n",
    "            return False, f\"Missing Tag: {tag}\"\n",
    "    \n",
    "    # 3. LLaDA 타겟 포맷 검사 (<|end_of_text|>)\n",
    "    if \"<|end_of_text|>\" not in target:\n",
    "        return False, \"Missing Target EOS\"\n",
    "\n",
    "    # 4. 그래프 데이터 확인 (string+graph 모드)\n",
    "    # PyG Data 객체가 분해되어 저장되어 있음 (x, edge_index 등)\n",
    "    # Note: batched=True로 넘어올 때는 list 형태이므로 길이 체크 시 주의 (여기서는 단일 객체 기준 로직)\n",
    "    if example.get('x') is None:\n",
    "        return False, \"Empty Graph Node Features (x)\"\n",
    "    if example.get('edge_index') is None:\n",
    "        return False, \"Empty Graph Edge Index\"\n",
    "\n",
    "    return True, \"OK\"\n",
    "\n",
    "def process_batch_verify(batch, tokenizer, threshold):\n",
    "    \"\"\"\n",
    "    배치 단위로 무결성 및 길이를 검사하여 'keep' 여부와 'drop_reason'을 반환합니다.\n",
    "    \"\"\"\n",
    "    batch_size = len(batch['prompt_text'])\n",
    "    keeps = []\n",
    "    reasons = []\n",
    "    \n",
    "    prompts = batch['prompt_text']\n",
    "    targets = batch['target_text']\n",
    "    \n",
    "    # 1. 포맷 검사 (Loop)\n",
    "    texts_to_tokenize = []\n",
    "    indices_to_tokenize = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # 딕셔너리 형태로 재구성하여 검사 함수 호출\n",
    "        # (batch 딕셔너리의 값들은 리스트임)\n",
    "        example = {k: v[i] for k, v in batch.items()}\n",
    "        \n",
    "        is_valid, reason = check_llada_format(example)\n",
    "        \n",
    "        if is_valid:\n",
    "            keeps.append(True) # 일단 True로 설정 (길이 검사 전)\n",
    "            reasons.append(\"OK\")\n",
    "            # 길이 검사를 위해 텍스트 준비\n",
    "            texts_to_tokenize.append(prompts[i] + targets[i])\n",
    "            indices_to_tokenize.append(i)\n",
    "        else:\n",
    "            keeps.append(False)\n",
    "            reasons.append(reason)\n",
    "    \n",
    "    # 2. 길이 필터링 (Batch Tokenization)\n",
    "    if texts_to_tokenize:\n",
    "        # padding=False, truncation=False로 실제 길이 계산\n",
    "        tokenized = tokenizer(texts_to_tokenize, add_special_tokens=False)\n",
    "        lengths = [len(ids) for ids in tokenized['input_ids']]\n",
    "        \n",
    "        for idx, length in zip(indices_to_tokenize, lengths):\n",
    "            if length > threshold:\n",
    "                keeps[idx] = False\n",
    "                reasons[idx] = \"Length Exceeded\"\n",
    "                \n",
    "    return {\n",
    "        \"keep\": keeps,\n",
    "        \"drop_reason\": reasons\n",
    "    }\n",
    "\n",
    "def filter_dataset(dataset, tokenizer, threshold):\n",
    "    \"\"\"\n",
    "    병렬 처리를 이용해 데이터셋을 검증하고 필터링합니다.\n",
    "    \"\"\"\n",
    "    print(f\"  > Verifying and Checking Length (Num Proc: {NUM_PROC})...\")\n",
    "\n",
    "    # 1. Map: 각 샘플에 대해 keep 여부와 이유를 판별 (병렬 처리)\n",
    "    processed_dataset = dataset.map(\n",
    "        process_batch_verify,\n",
    "        batched=True,\n",
    "        num_proc=NUM_PROC,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer, \"threshold\": threshold},\n",
    "        desc=\" analyzing\"\n",
    "    )\n",
    "    \n",
    "    # 2. 통계 집계\n",
    "    # drop_reason 컬럼을 통해 통계를 냅니다.\n",
    "    # (주의: 대용량 데이터셋에서 Counter는 약간의 시간이 걸릴 수 있음)\n",
    "    drop_reasons = processed_dataset['drop_reason']\n",
    "    stats = Counter(drop_reasons)\n",
    "    del stats[\"OK\"] # 정상 데이터 카운트는 제외\n",
    "    \n",
    "    # 3. Filter: keep=True인 것만 남김\n",
    "    filtered_dataset = processed_dataset.filter(\n",
    "        lambda example: example['keep'],\n",
    "        num_proc=NUM_PROC,\n",
    "        desc=\" filtering\"\n",
    "    )\n",
    "    \n",
    "    # 4. 임시 컬럼 제거 (원본 스키마 복구)\n",
    "    final_dataset = filtered_dataset.remove_columns(['keep', 'drop_reason'])\n",
    "    \n",
    "    return final_dataset, stats\n",
    "\n",
    "def main():\n",
    "    print(f\"=== LLaDA Dataset Verification & Filtering (Threshold: {THRESHOLD}, Process: {NUM_PROC}) ===\")\n",
    "    \n",
    "    print(f\"Loading Tokenizer from: {TOKENIZER_PATH}\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, trust_remote_code=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load tokenizer: {e}\")\n",
    "        return\n",
    "\n",
    "    for path in TARGET_PATHS:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"\\n[Skipping] Path not found: {path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing Dataset: {path}\")\n",
    "        \n",
    "        try:\n",
    "            dataset = load_from_disk(path)\n",
    "            original_size = len(dataset)\n",
    "            \n",
    "            # 필터링 수행\n",
    "            filtered_dataset, stats = filter_dataset(dataset, tokenizer, THRESHOLD)\n",
    "            \n",
    "            filtered_size = len(filtered_dataset)\n",
    "            dropped_count = original_size - filtered_size\n",
    "            \n",
    "            print(f\"  ------------------------------------------------\")\n",
    "            print(f\"  Original Size   : {original_size}\")\n",
    "            print(f\"  Filtered Size   : {filtered_size}\")\n",
    "            print(f\"  Dropped Samples : {dropped_count}\")\n",
    "            if dropped_count > 0:\n",
    "                print(f\"  Drop Reasons    : {dict(stats)}\")\n",
    "            print(f\"  ------------------------------------------------\")\n",
    "            \n",
    "            # 저장\n",
    "            if dropped_count > 0:\n",
    "                save_path = path.rstrip('/') + f\"_verified_filtered_{THRESHOLD}\"\n",
    "                print(f\"  Saving filtered dataset to: {save_path}\")\n",
    "                filtered_dataset.save_to_disk(save_path)\n",
    "            else:\n",
    "                print(\"  No samples dropped. Skipping save.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  [Fatal Error] Failed to process {path}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e90248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MolDA_CHJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
