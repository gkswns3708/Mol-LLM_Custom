{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec816b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['x', 'edge_index', 'edge_attr', 'label', 'input_mol_string', 'task_subtask_pair', 'instruction', 'additional_x', 'additional_edge_index', 'additional_edge_attr', 'task', 'prompt_text', 'target_text'],\n",
       "    num_rows: 2850834\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# dataset = load_from_disk(\"/appdataset/train/mistralai-Mistral-7B-Instruct-v0.3_string+graph_q32_test_3.3M_0415\")\n",
    "train_dataset = load_from_disk(\"/app/Mol-LLM_Custom/dataset/real_train/mistralai-Mistral-7B-Instruct-v0.3_string+graph_q32_train_3.3M_0415\")\n",
    "validation_dataset = load_from_disk(\"/app/Mol-LLM_Custom/dataset/real_train/mistralai-Mistral-7B-Instruct-v0.3_string+graph_q32_validation_3.3M_0415\")\n",
    "test_dataset = load_from_disk(\"/app/Mol-LLM_Custom/dataset/real_train/mistralai-Mistral-7B-Instruct-v0.3_string+graph_q32_test_3.3M_0415\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f040707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    description_guided_molecule_design: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'metadata'],\n",
       "        num_rows: 298319\n",
       "    })\n",
       "    forward_reaction_prediction: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'metadata'],\n",
       "        num_rows: 125384\n",
       "    })\n",
       "    molecular_description_generation: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'metadata'],\n",
       "        num_rows: 298319\n",
       "    })\n",
       "    property_prediction: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'metadata'],\n",
       "        num_rows: 362100\n",
       "    })\n",
       "    reagent_prediction: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'metadata'],\n",
       "        num_rows: 125384\n",
       "    })\n",
       "    retrosynthesis: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'metadata'],\n",
       "        num_rows: 129684\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_from_disk(\"/appdataset/train/mistralai-Mistral-7B-Instruct-v0.3_string+graph_q32_test_3.3M_0415\")\n",
    "mol_instruction_dataset = load_dataset(\n",
    "            \"zjunlp/Mol-Instructions\",\n",
    "            \"Molecule-oriented Instructions\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "mol_instruction_dataset# download_dataset.py의 processed_data_root 아래 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e894f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Please suggest a potential product based on the given reactants and reagents.',\n",
       " 'input': '[O][=C][C][=C][C][Branch1][=Branch1][N+1][=Branch1][C][=O][O-1][=C][Branch1][C][F][C][=C][Ring1][#Branch2][F].[C][C][C][O][C][Ring1][Branch1].[Cl].[BH4-1].[Na+1]',\n",
       " 'output': '[O][=N+1][Branch1][C][O-1][C][=C][C][Branch1][Ring1][C][O][=C][Branch1][C][F][C][=C][Ring1][=Branch2][F]',\n",
       " 'metadata': \"{'task': 'forward reaction prediction', 'split': 'train'}\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mol_instruction_dataset['forward_reaction_prediction'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73c2802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# dataset = load_from_disk(\"/appdataset/train/mistralai-Mistral-7B-Instruct-v0.3_string+graph_q32_test_3.3M_0415\")\n",
    "train_dataset = load_from_disk(\"/app/Mol-LLM_Custom/dataset/real_train/mistralai-Mistral-7B-Instruct-v0.3_string+graph_q32_train_3.3M_0415\")\n",
    "validation_dataset = load_from_disk(\"/app/Mol-LLM_Custom/dataset/real_train/mistralai-Mistral-7B-Instruct-v0.3_string+graph_q32_validation_3.3M_0415\")\n",
    "test_dataset = load_from_disk(\"/app/Mol-LLM_Custom/dataset/real_train/mistralai-Mistral-7B-Instruct-v0.3_string+graph_q32_test_3.3M_0415\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f889b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2850834, 36814, 36814)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(validation_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d49cdda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['bace', 'chebi-20-mol2text/0', 'chebi-20-text2mol/0',\n",
       "        'forward_reaction_prediction', 'qm9_homo', 'qm9_homo_lumo_gap',\n",
       "        'qm9_lumo', 'reagent_prediction', 'retrosynthesis',\n",
       "        'smol-forward_synthesis/0', 'smol-molecule_captioning/0',\n",
       "        'smol-molecule_generation/0', 'smol-property_prediction-bbbp/0',\n",
       "        'smol-property_prediction-clintox/0',\n",
       "        'smol-property_prediction-esol/0',\n",
       "        'smol-property_prediction-hiv/0',\n",
       "        'smol-property_prediction-lipo/0',\n",
       "        'smol-property_prediction-sider/0', 'smol-retrosynthesis/0'],\n",
       "       dtype='<U34'),\n",
       " array([  1210,  18777,  18777, 121896, 117660, 117660, 117708, 121896,\n",
       "        126110, 971766,  56497,  56498,   1569,   1144,    888,  32863,\n",
       "          3360,  22820, 941735]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(train_dataset['task'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bebd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import deepchem as dc\n",
    "import numpy as np\n",
    "import selfies as sf\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from rdkit import Chem\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, ConcatDataset\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.data.separate import separate\n",
    "from torch_geometric.loader.dataloader import Collater\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import instructions_smol\n",
    "import model.added_tokens as added_tokens\n",
    "from pytorch_lightning import LightningDataModule\n",
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "from data_utils import (\n",
    "    CLASSIFICATION_BENCHMARKS,\n",
    "    MOL2TEXT_BENCHMARKS,\n",
    "    REGRESSION_BENCHMARKS,\n",
    "    REACTION_BENCHMARKS,\n",
    "    TEXT2MOL_BENCHMARKS,\n",
    ")\n",
    "\n",
    "# token added to implement a custom sequence tokenization. This token is added at\n",
    "# corpus cleaning step and removed in pretokenization. The digits are added to increase the chance\n",
    "# that they do not occur in the corpus. The digits are escaped so that the token does not appear\n",
    "# literally in the source code in case we ever include it in the training data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "\n",
    "def wrap_label(label, task):\n",
    "\n",
    "    if task in CLASSIFICATION_BENCHMARKS:\n",
    "        label_tokens = added_tokens.BOOL\n",
    "    elif task in REGRESSION_BENCHMARKS:\n",
    "        label_tokens = added_tokens.FLOAT\n",
    "    elif task in [\"smol-name_conversion-s2f\", \"smol-name_conversion-i2f\"]:\n",
    "        label_tokens = added_tokens.MOLFORMULA\n",
    "    elif task == \"smol-name_conversion-s2i\":\n",
    "        label_tokens = added_tokens.IUPAC\n",
    "    elif task in MOL2TEXT_BENCHMARKS:\n",
    "        label_tokens = added_tokens.DESCRIPTION\n",
    "    elif task in TEXT2MOL_BENCHMARKS + REACTION_BENCHMARKS:\n",
    "        label_tokens = added_tokens.SELFIES\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if task in CLASSIFICATION_BENCHMARKS:\n",
    "        if isinstance(label, str):\n",
    "            if \"true\" in label.lower() or \"yes\" in label.lower():\n",
    "                label = \"True\"\n",
    "            elif \"false\" in label.lower() or \"no\" in label.lower():\n",
    "                label = \"False\"\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    f\"Label: {label} is not supported in classification task\"\n",
    "                )\n",
    "            label = label_tokens[0] + label + label_tokens[1]\n",
    "        elif isinstance(label, list):\n",
    "            label_language = \", \".join(label)\n",
    "            label_boolean = \"True\" * len(label)\n",
    "            label = label_language + label_tokens[0] + label_boolean + label_tokens[1]\n",
    "        else:\n",
    "            label = \"True\" if label else \"False\"\n",
    "            label = label_tokens[0] + label + label_tokens[1]\n",
    "        return label\n",
    "    elif task in REGRESSION_BENCHMARKS:\n",
    "        if isinstance(label, float):\n",
    "            label = \"{:.10f}\".format(label)\n",
    "        else:\n",
    "            label = format(float(label), \".10f\")\n",
    "\n",
    "        # force to predict the sign of label first\n",
    "        if \"-\" not in label and \"+\" not in label:\n",
    "            label = \"+\" + label\n",
    "        # unify the length of label to 7\n",
    "        label = label[:7]\n",
    "        converted_label = \"\".join([f\"<|{char}|>\" for char in label])\n",
    "        return label_tokens[0] + \" \" + converted_label + \" \" + label_tokens[1]\n",
    "    elif task in REACTION_BENCHMARKS + MOL2TEXT_BENCHMARKS + TEXT2MOL_BENCHMARKS:\n",
    "        return label_tokens[0] + label + label_tokens[1]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "\n",
    "\n",
    "# TODO use task or refactor it\n",
    "# getitem shoul return enough information that what is label, and what is the label meaning (to format instruction)\n",
    "class MoleculeNetDatasetDeepChem(Dataset):\n",
    "    def __init__(self, data, task_subtask_pair, subtask_idx=0, prompt=None):\n",
    "        self.data = data\n",
    "        self.subtask_idx = subtask_idx\n",
    "        self.task_subtask_pair = task_subtask_pair\n",
    "        self.task, self.subtask = task_subtask_pair.split(\"/\")\n",
    "\n",
    "        if self.task in CLASSIFICATION_BENCHMARKS:\n",
    "            self.instruction_templates = getattr(instructions_smol, self.task)\n",
    "            self.label_tokens = added_tokens.BOOL\n",
    "        elif self.task in REGRESSION_BENCHMARKS:\n",
    "            if self.task in [\"qm9_additional_label\"]:\n",
    "                subtask_full_name_dict = {\n",
    "                    \"mu\": \"dipole_moment\",\n",
    "                    \"alpha\": \"isotropic_polarizability\",\n",
    "                    \"r2\": \"electronic_spatial_extent\",\n",
    "                    \"zpve\": \"zero_point_vibrational_energy\",\n",
    "                    \"cv\": \"heat_capacity_298K\",\n",
    "                    \"u298\": \"internal_energy_298K\",\n",
    "                    \"h298\": \"enthalpy_298K\",\n",
    "                    \"g298\": \"free_energy_298K\",\n",
    "                }\n",
    "                task = self.task.replace(\"_additional_label\", \"\")\n",
    "                subtask_full_name = subtask_full_name_dict[self.subtask]\n",
    "                self.instruction_templates = getattr(\n",
    "                    instructions_smol, f\"{task}_{subtask_full_name}\"\n",
    "                )\n",
    "                assert len(self.instruction_templates) > 1, \"Instruction is not enough\"\n",
    "            else:\n",
    "                self.instruction_templates = getattr(instructions_smol, f\"{self.task}_{self.subtask}\".lower())\n",
    "            self.label_tokens = added_tokens.FLOAT\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.set_necessary_data()\n",
    "\n",
    "    def get_necessary_data(self, index):\n",
    "        instruction = np.random.choice(self.instruction_templates)\n",
    "        smiles = self.smiles_list[index]\n",
    "        # set molecule string representation as selfies\n",
    "        input_mol_string = sf.encoder(smiles)\n",
    "        input_mol_string = (\n",
    "            added_tokens.SELFIES[0] + input_mol_string + added_tokens.SELFIES[1]\n",
    "        )\n",
    "        if self.subtask_idx == \"multi_label_classification\":\n",
    "            label = self.raw_outputs[index]\n",
    "            label = [self.label_full_name[i] for i in range(len(label)) if label[i]]\n",
    "            if len(label) == 0:\n",
    "                label = \"No toxicity identified. \" + wrap_label(\"False\", self.task)\n",
    "            else:\n",
    "                label = wrap_label(label, self.task)\n",
    "        else:\n",
    "            label = self.raw_outputs[index]\n",
    "            label = wrap_label(label, self.task)\n",
    "\n",
    "        graph = smiles2data(smiles)\n",
    "        # randomly select one instruction from list\n",
    "        return graph, label, input_mol_string, instruction\n",
    "\n",
    "    def set_label_fullname(self):\n",
    "        self.label_full_name = None\n",
    "        if self.task == \"tox21\":\n",
    "            self.label_full_name = [\n",
    "                \"androgen receptor, full (AR, full)\",  # AR\n",
    "                \"androgen receptor, LBD (AR, LBD)\",  # AR, LBD\n",
    "                \"aryl hydrocarbon receptor (AhR)\",  # AhR\n",
    "                \"aromatase\",\n",
    "                \"estrogen receptor alpha, full (ER, full)\",  # ER\n",
    "                \"estrogen receptor alpha, LBD (ER, LBD)\",  # ER, LBD\n",
    "                \"peroxisome proliferator-activated receptor gamma (PPAR-gamma)\",  # PPAR-gamma\n",
    "                \"nuclear factor (erythroid-derived 2)-like 2/antioxidant responsive element (Nrf2/ARE)\",\n",
    "                \"ATPase family AAA domain containing 5 (ATAD5)\",\n",
    "                \"heat shock factor response element (HSE)\",  # HSE\n",
    "                \"mitochondrial membrane potential (MMP)\",  # MMP\n",
    "                \"tumor suppressor protein p53\",\n",
    "            ]\n",
    "\n",
    "    def set_necessary_data(self):\n",
    "        self.raw_inputs = self.data.X\n",
    "        if self.subtask_idx == \"multi_label_classification\":\n",
    "            self.set_label_fullname()\n",
    "            self.raw_outputs = self.data.y\n",
    "        else:\n",
    "            self.raw_outputs = self.data.y[:, self.subtask_idx]\n",
    "\n",
    "        self.smiles_list = []\n",
    "        for mol in self.raw_inputs:\n",
    "            self.smiles_list.append(Chem.MolToSmiles(mol))\n",
    "\n",
    "        self.label_list = []\n",
    "        self.input_mol_string_list = []\n",
    "        self.graph_list = []\n",
    "        self.instruction_list = []\n",
    "\n",
    "        self.count_invalid_smiles = 0\n",
    "\n",
    "        iter_bar = tqdm(\n",
    "            range(len(self.raw_inputs)),\n",
    "            total=len(self.raw_inputs),\n",
    "            desc=f\"{self.task}-{self.subtask_idx}\",\n",
    "        )\n",
    "        for i in iter_bar:\n",
    "            try:\n",
    "                graph, label, input_mol_string, instruction = self.get_necessary_data(i)\n",
    "                self.label_list.append(label)\n",
    "                self.input_mol_string_list.append(input_mol_string)\n",
    "                self.graph_list.append(graph)\n",
    "                self.instruction_list.append(instruction)\n",
    "            except Exception as e:\n",
    "                self.count_invalid_smiles += 1\n",
    "        if self.count_invalid_smiles > 0:\n",
    "            print(f\"{self.task}: Number of invalid smiles: {self.count_invalid_smiles}\")\n",
    "            print(\n",
    "                f\"{self.task}: Invalid smiles ratio: {self.count_invalid_smiles/len(self.raw_inputs)}\"\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        graph = self.graph_list[index]\n",
    "        label = self.label_list[index]\n",
    "        input_mol_string = self.input_mol_string_list[index]\n",
    "        instruction = self.instruction_list[index]\n",
    "\n",
    "        return graph, label, input_mol_string, self.task_subtask_pair, instruction\n",
    "\n",
    "\n",
    "class MolInstructionDatset(Dataset):\n",
    "    def __init__(self, data, task_subtask_pair, **kwargs):\n",
    "        self.data = data\n",
    "        self.task = task_subtask_pair\n",
    "        #self.task, self.subtask = task_subtask_pair.split(\"/\")\n",
    "\n",
    "        self.set_necesary_data()\n",
    "\n",
    "    def set_necesary_data(self):\n",
    "        if self.task == \"bace\":\n",
    "            self.input_list = self.data[\"SELFIES\"][:]\n",
    "            self.label_list = self.data[\"label\"][:]\n",
    "        else:\n",
    "            self.input_list = self.data[\"input\"][:]\n",
    "            self.label_list = self.data[\"output\"][:]\n",
    "        self.instruction_templates = getattr(instructions_smol, self.task)\n",
    "\n",
    "        input_list = []\n",
    "        label_list = []\n",
    "        input_mol_string_list = []\n",
    "        graph_list = []\n",
    "        instruction_list = []\n",
    "\n",
    "        self.count_invalid_smiles = 0\n",
    "        iter_bar = tqdm(\n",
    "            range(len(self.input_list)), total=len(self.input_list), desc=self.task\n",
    "        )\n",
    "        for i in iter_bar:\n",
    "            try:\n",
    "                graph, label, input_mol_string, instruction = self.get_necessary_data(i)\n",
    "                input_list.append(self.input_list[i])\n",
    "                label_list.append(label)\n",
    "                input_mol_string_list.append(input_mol_string)\n",
    "                graph_list.append(graph)\n",
    "                instruction_list.append(instruction)\n",
    "            except Exception as e:\n",
    "                self.count_invalid_smiles += 1\n",
    "        if self.count_invalid_smiles > 0:\n",
    "            print(f\"{self.task}: Number of invalid smiles: {self.count_invalid_smiles}\")\n",
    "            print(\n",
    "                f\"{self.task}: Invalid smiles ratio: {self.count_invalid_smiles/len(self.input_list)}\"\n",
    "            )\n",
    "\n",
    "        self.input_list = input_list\n",
    "        self.label_list = label_list\n",
    "        self.input_mol_string_list = input_mol_string_list\n",
    "        self.graph_list = graph_list\n",
    "        self.instruction_list = instruction_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "\n",
    "    def get_necessary_data(self, index):\n",
    "        instruction = np.random.choice(self.instruction_templates)\n",
    "        input = self.input_list[index]  # if mol_string, representation is selfies\n",
    "        label = self.label_list[index]  # if mol_string, representation is selfies\n",
    "\n",
    "        if self.task in REACTION_BENCHMARKS:\n",
    "            # two smiles in input0\n",
    "            if self.task in [\"reagent_prediction\"]:\n",
    "                assert \">>\" in input\n",
    "                list_selfies = input.split(\n",
    "                    \">>\"\n",
    "                )  # reagent prediction has two selfies in input\n",
    "                input_mol_string = input.replace(\n",
    "                    \">>\",\n",
    "                    f\"{added_tokens.SELFIES[1]}{added_tokens.REACTION_DIRECTION[0]}{added_tokens.SELFIES[0]}\",\n",
    "                )\n",
    "                list_smiles = [sf.decoder(s) for s in list_selfies]\n",
    "                graph = [smiles2data(s) for s in list_smiles]\n",
    "            # one smiles in input and one smiles in output\n",
    "            else:\n",
    "                input_mol_string = input\n",
    "                smiles = sf.decoder(input_mol_string)\n",
    "                graph = smiles2data(smiles)\n",
    "        elif self.task in CLASSIFICATION_BENCHMARKS:\n",
    "            input_mol_string = input\n",
    "            smiles = sf.decoder(input_mol_string)\n",
    "            graph = smiles2data(smiles)\n",
    "        else:\n",
    "            # one selfies in input\n",
    "            input_mol_string = input\n",
    "            smiles = sf.decoder(input_mol_string)\n",
    "            graph = smiles2data(smiles)\n",
    "\n",
    "        label = wrap_label(label, self.task)\n",
    "        input_mol_string = (\n",
    "            added_tokens.SELFIES[0] + input_mol_string + added_tokens.SELFIES[1]\n",
    "        )\n",
    "\n",
    "        return graph, label, input_mol_string, instruction\n",
    "\n",
    "    # LLM input order: <instruction><qformer_output><smiles_tokens>\n",
    "    def __getitem__(self, index):\n",
    "        graph = self.graph_list[index]\n",
    "        label = self.label_list[index]\n",
    "        input_mol_string = self.input_mol_string_list[index]\n",
    "        instruction = self.instruction_list[index]\n",
    "\n",
    "        return graph, label, input_mol_string, self.task, instruction\n",
    "\n",
    "\n",
    "class ChEBIDataset(Dataset):\n",
    "    def __init__(self, data, task_subtask_pair, **kwargs):\n",
    "        self.data = data\n",
    "        self.task_subtask_pair = task_subtask_pair\n",
    "        self.task, self.subtask = task_subtask_pair.split(\"/\")\n",
    "\n",
    "        self.set_necesary_data()\n",
    "\n",
    "    def set_necesary_data(self):\n",
    "        self.description_list = self.data[\"description\"]\n",
    "        self.selfies_list = self.data[\"SELFIES\"]\n",
    "        if \"mol2text\" in self.task:\n",
    "            self.instruction_templates = getattr(\n",
    "                instructions_smol, \"molecule_captioning\"\n",
    "            )\n",
    "        elif \"text2mol\" in self.task:\n",
    "            self.instruction_templates = getattr(\n",
    "                instructions_smol, \"molecule_generation\"\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.input_mol_string_list = []\n",
    "        self.graph_list = []\n",
    "        self.instruction_list = []\n",
    "        self.label_list = []\n",
    "\n",
    "        self.count_invalid_smiles = 0\n",
    "        iter_bar = tqdm(\n",
    "            range(len(self.description_list)),\n",
    "            total=len(self.description_list),\n",
    "            desc=self.task,\n",
    "        )\n",
    "        for i in iter_bar:\n",
    "            try:\n",
    "                graph, label, input_mol_string, instruction = self.get_necessary_data(i)\n",
    "                self.label_list.append(label)\n",
    "                self.input_mol_string_list.append(input_mol_string)\n",
    "                self.graph_list.append(graph)\n",
    "                self.instruction_list.append(instruction)\n",
    "            except Exception as e:\n",
    "                self.count_invalid_smiles += 1\n",
    "        if self.count_invalid_smiles > 0:\n",
    "            print(f\"{self.task}: Number of invalid smiles: {self.count_invalid_smiles}\")\n",
    "            print(\n",
    "                f\"{self.task}: Invalid smiles ratio: {self.count_invalid_smiles/len(self.label_list)}\"\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "\n",
    "    def get_necessary_data(self, index):\n",
    "        instruction = np.random.choice(self.instruction_templates)\n",
    "        descriptiopn = self.description_list[index]\n",
    "        selfies = self.selfies_list[index]\n",
    "        smiles = sf.decoder(selfies)\n",
    "\n",
    "        if self.task in TEXT2MOL_BENCHMARKS:\n",
    "            label = selfies\n",
    "            description = (\n",
    "                added_tokens.DESCRIPTION[0] + descriptiopn + added_tokens.DESCRIPTION[1]\n",
    "            )\n",
    "            instruction = instruction.replace(\"<INPUT>\", description)\n",
    "            graph = smiles2data(\n",
    "                \"CC\"\n",
    "            )  # null smiles, just input dummy graph for batch processing\n",
    "            input_mol_string = \"<None>\"\n",
    "        elif self.task in MOL2TEXT_BENCHMARKS:\n",
    "            label = descriptiopn\n",
    "            input_mol_string = selfies\n",
    "            graph = smiles2data(smiles)\n",
    "\n",
    "        label = wrap_label(label, self.task)\n",
    "        input_mol_string = (\n",
    "            added_tokens.SELFIES[0] + input_mol_string + added_tokens.SELFIES[1]\n",
    "        )\n",
    "        return graph, label, input_mol_string, instruction\n",
    "\n",
    "    # LLM input order: <instruction><qformer_output><smiles_tokens>\n",
    "    def __getitem__(self, index):\n",
    "        graph = self.graph_list[index]\n",
    "        label = self.label_list[index]\n",
    "        input_mol_string = self.input_mol_string_list[index]\n",
    "        instruction = self.instruction_list[index]\n",
    "\n",
    "        return graph, label, input_mol_string, self.task_subtask_pair, instruction\n",
    "\n",
    "\n",
    "class SMolInstructDataset(Dataset):\n",
    "    def __init__(self, data, task_subtask_pair, **kwargs):\n",
    "        self.data = data\n",
    "        self.task_subtask_pair = task_subtask_pair\n",
    "        self.task, self.subtask = task_subtask_pair.split(\"/\")\n",
    "        if \"forward_synthesis\" in self.task:\n",
    "            self.instruction_templates = getattr(\n",
    "                instructions_smol, \"forward_reaction_prediction\"\n",
    "            )\n",
    "        else:\n",
    "            self.instruction_templates = getattr(\n",
    "                instructions_smol, self.task.replace(\"smol-\", \"\").replace(\"-\", \"_\")\n",
    "            )\n",
    "        self.set_necesary_data()\n",
    "\n",
    "    def set_necesary_data(self):\n",
    "        self.semi_colon_count_input = 0\n",
    "        self.semi_colon_count_label = 0\n",
    "\n",
    "        self.input_mol_string_list = []\n",
    "        self.graph_list = []\n",
    "        self.instruction_list = []\n",
    "        self.label_list = []\n",
    "\n",
    "        # pre-load data\n",
    "        raw_inputs = self.data[\"raw_input\"][:]\n",
    "        raw_outputs = self.data[\"raw_output\"][:]\n",
    "\n",
    "        iter_bar = tqdm(\n",
    "            range(len(self.data)),\n",
    "            total=len(self.data),\n",
    "            desc=self.task,\n",
    "        )\n",
    "        self.count_invalid_smiles = 0\n",
    "\n",
    "        for i in iter_bar:\n",
    "            try:\n",
    "                graph, label, input_mol_string, instruction = self.get_necessary_data(\n",
    "                    i, raw_inputs[i], raw_outputs[i]\n",
    "                )\n",
    "                self.graph_list.append(graph)\n",
    "                self.label_list.append(label)\n",
    "                self.input_mol_string_list.append(input_mol_string)\n",
    "                self.instruction_list.append(instruction)\n",
    "            except Exception as e:\n",
    "                self.count_invalid_smiles += 1\n",
    "        if self.count_invalid_smiles > 0:\n",
    "            print(f\"{self.task}: Number of invalid smiles: {self.count_invalid_smiles}\")\n",
    "            print(\n",
    "                f\"{self.task}: Invalid smiles ratio: {1.0 - len(self.label_list)/len(self.data)}\"\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "\n",
    "    def get_necessary_data(self, index, raw_input, raw_output):\n",
    "        raw_input = raw_input\n",
    "        label = raw_output\n",
    "\n",
    "        if \";\" in raw_input:\n",
    "            self.semi_colon_count_input += 1\n",
    "        if \";\" in raw_output:\n",
    "            self.semi_colon_count_label += 1\n",
    "\n",
    "        if self.task in TEXT2MOL_BENCHMARKS:\n",
    "            s_token, e_token = (\n",
    "                added_tokens.IUPAC\n",
    "                if self.task in [\"smol-name_conversion-i2s\", \"smol-name_conversion-i2f\"]\n",
    "                else added_tokens.DESCRIPTION\n",
    "            )\n",
    "            description = raw_input\n",
    "            description = s_token + description + e_token\n",
    "            instruction = np.random.choice(self.instruction_templates)\n",
    "            instruction = instruction.replace(\"<INPUT>\", description)\n",
    "            graph = smiles2data(\n",
    "                \"CC\"\n",
    "            )  # null smiles, just input dummy graph for batch processing\n",
    "            input_mol_string = \"<None>\"\n",
    "            label = re.sub(r\"\\s*;\\s*\", \".\", label)\n",
    "        elif self.task in REACTION_BENCHMARKS:\n",
    "            instruction = np.random.choice(self.instruction_templates)\n",
    "            input_mol_string = raw_input\n",
    "            smiles = sf.decoder(input_mol_string)\n",
    "            graph = smiles2data(smiles)\n",
    "        # multi labeled property prediction datasets\n",
    "        elif self.task in [\"smol-property_prediction-sider\"]:\n",
    "            instance_input = self.data[index][\"input\"]\n",
    "            assert re.search(r\"\\[.*\\]\", instance_input) is not None\n",
    "            instruction = re.sub(r\"\\[.*\\]\", \"<INPUT>\", instance_input)\n",
    "\n",
    "            # use re sub to replace \";\" with \".\"\n",
    "            input_mol_string = re.sub(r\"\\s*;\\s*\", \".\", raw_input)\n",
    "            smiles = sf.decoder(input_mol_string)\n",
    "            graph = smiles2data(smiles)\n",
    "        elif (\n",
    "            self.task\n",
    "            in MOL2TEXT_BENCHMARKS + CLASSIFICATION_BENCHMARKS + REGRESSION_BENCHMARKS\n",
    "        ):\n",
    "            instruction = np.random.choice(self.instruction_templates)\n",
    "            # use re sub to replace \";\" with \".\"\n",
    "            input_mol_string = re.sub(r\"\\s*;\\s*\", \".\", raw_input)\n",
    "            smiles = sf.decoder(input_mol_string)\n",
    "            graph = smiles2data(smiles)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Task: {self.task} is not supported\")\n",
    "\n",
    "        label = wrap_label(label, self.task)\n",
    "        input_mol_string = (\n",
    "            added_tokens.SELFIES[0] + input_mol_string + added_tokens.SELFIES[1]\n",
    "        )\n",
    "\n",
    "        return graph, label, input_mol_string, instruction\n",
    "\n",
    "    # LLM input order: <instruction><qformer_output><smiles_tokens>\n",
    "    def __getitem__(self, index):\n",
    "        graph = self.graph_list[index]\n",
    "        label = self.label_list[index]\n",
    "        input_mol_string = self.input_mol_string_list[index]\n",
    "        instruction = self.instruction_list[index]\n",
    "\n",
    "        return graph, label, input_mol_string, self.task_subtask_pair, instruction\n",
    "\n",
    "\n",
    "from ogb.utils import smiles2graph\n",
    "\n",
    "\n",
    "def smiles2data(smiles):\n",
    "    graph = smiles2graph(smiles)\n",
    "    x = torch.from_numpy(graph[\"node_feat\"])\n",
    "    edge_index = torch.from_numpy(\n",
    "        graph[\"edge_index\"],\n",
    "    )\n",
    "    edge_attr = torch.from_numpy(graph[\"edge_feat\"])\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    return data\n",
    "\n",
    "\n",
    "# torch_geometric.data.Data variants for paired graph data, i.e. reagent prediction\n",
    "class PairData(Data):\n",
    "    def __inc__(self, key: str, value: Any, *args, **kwargs) -> Any:\n",
    "        if key == \"edge_index\":\n",
    "            return self.x.size(0)\n",
    "        elif key == \"additional_edge_index\":\n",
    "            return self.additional_x.size(0)\n",
    "        return super().__inc__(key, value, *args, **kwargs)\n",
    "\n",
    "\n",
    "class PackedData(Data):\n",
    "    def __inc__(self, key: str, value: Any, *args, **kwargs) -> Any:\n",
    "        if \"edge_index\" in key:\n",
    "            prefix = key.split(\"edge_index\")[0]\n",
    "            return getattr(self, f\"{prefix}x.size\")(0)\n",
    "        return super().__inc__(key, value, *args, **kwargs)\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "\n",
    "def get_canonical_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is not None:\n",
    "        return Chem.MolToSmiles(mol)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "import yaml\n",
    "\n",
    "\n",
    "def get_task_subtask_info(target_benchmarks):\n",
    "    task_subtask_dict = {}\n",
    "    for task in target_benchmarks:\n",
    "        if isinstance(task, str):\n",
    "            task_subtask_dict[task] = [0]\n",
    "        else:\n",
    "            task_subtask_dict.update(task)\n",
    "\n",
    "    task_subtask_pairs = [\n",
    "        (task, subtask)\n",
    "        for task, subtasks in task_subtask_dict.items()\n",
    "        for subtask in subtasks\n",
    "    ]\n",
    "    return task_subtask_dict, task_subtask_pairs\n",
    "\n",
    "\n",
    "def get_dataset(task_name, raw_data_root):\n",
    "    # get dataset from deepchem\n",
    "    if \"smol\" in task_name:\n",
    "        smol_dataset = load_dataset(\n",
    "            \"osunlp/SMolInstruct\",\n",
    "            use_selfies=True,\n",
    "            insert_core_tags=False,  # loada data w/o core tags such as <SELFIES>, </SELFIES>\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        _task = re.sub(\"smol-\", \"\", task_name)  # remove smol- from smol-<task_name>\n",
    "\n",
    "        # DEBUG: to avoid lengthy processing time\n",
    "        train_dataset = smol_dataset[\"train\"].filter(lambda x: x[\"task\"] == _task)\n",
    "        valid_dataset = smol_dataset[\"validation\"].filter(lambda x: x[\"task\"] == _task)\n",
    "        test_dataset = smol_dataset[\"test\"].filter(lambda x: x[\"task\"] == _task)\n",
    "        tasks = [task_name]\n",
    "    elif task_name in [\n",
    "        \"toxcast\",\n",
    "        \"tox21\",\n",
    "        \"hopv\"\n",
    "    ]:\n",
    "        loading_fn = getattr(dc.molnet, f\"load_{task_name}\")\n",
    "    elif task_name in [\"qm9_additional_label\"]:\n",
    "        loading_fn = dc.molnet.load_qm9\n",
    "    # TODO: address biot5 datasets\n",
    "    elif task_name == \"bace\":\n",
    "        train_dataset = pd.read_csv(\n",
    "            os.path.join(raw_data_root, \"raw/BioT5_bace_train.csv\")\n",
    "        )\n",
    "        valid_dataset = pd.read_csv(\n",
    "            os.path.join(raw_data_root, \"raw/BioT5_bace_valid.csv\")\n",
    "        )\n",
    "        test_dataset = pd.read_csv(\n",
    "            os.path.join(raw_data_root, \"raw/BioT5_bace_test.csv\")\n",
    "        )\n",
    "        tasks = [task_name]\n",
    "    elif \"chebi-20\" in task_name:\n",
    "        # load data from csv\n",
    "        train_dataset = pd.read_csv(\n",
    "            os.path.join(raw_data_root, \"raw/BioT5_chebi20_train.csv\")\n",
    "        )\n",
    "        valid_dataset = pd.read_csv(\n",
    "            os.path.join(raw_data_root, \"raw/BioT5_chebi20_valid.csv\")\n",
    "        )\n",
    "        test_dataset = pd.read_csv(\n",
    "            os.path.join(raw_data_root, \"raw/BioT5_chebi20_test.csv\")\n",
    "        )\n",
    "\n",
    "        tasks = [task_name]\n",
    "\n",
    "    # mol-instruction datasets\n",
    "    elif task_name in [\n",
    "        \"reagent_prediction\",\n",
    "        \"forward_reaction_prediction\",\n",
    "        \"retrosynthesis\",\n",
    "        \"qm9_homo\",\n",
    "        \"qm9_lumo\",\n",
    "        \"qm9_homo_lumo_gap\",\n",
    "    ]:\n",
    "        mol_instruction_dataset = load_dataset(\n",
    "            \"zjunlp/Mol-Instructions\",\n",
    "            \"Molecule-oriented Instructions\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        if \"qm9_\" in task_name:\n",
    "            dataset = mol_instruction_dataset[\"property_prediction\"]\n",
    "            subtask_name = task_name.split(\"_\")[1]\n",
    "            subtask_instruction_templates = getattr(\n",
    "                instructions_smol, \"filtering_template_\" + subtask_name\n",
    "            )\n",
    "            dataset = dataset.filter(\n",
    "                lambda x: x[\"instruction\"] in subtask_instruction_templates\n",
    "            )\n",
    "            assert len(dataset) > 0, f\"len(dataset) = {len(dataset)}\"\n",
    "        else:\n",
    "            dataset = mol_instruction_dataset[task_name]\n",
    "\n",
    "        train_dataset = dataset.filter(lambda x: \"train\" in x[\"metadata\"])\n",
    "        split = train_dataset.train_test_split(test_size=0.02, shuffle=True)\n",
    "        train_dataset, valid_dataset = split[\"train\"], split[\"test\"]\n",
    "\n",
    "        test_dataset = dataset.filter(lambda x: \"test\" in x[\"metadata\"])\n",
    "        tasks = [task_name]\n",
    "    else:\n",
    "        print()\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # dataset from deepchem\n",
    "    if (\n",
    "        task_name in CLASSIFICATION_BENCHMARKS + REGRESSION_BENCHMARKS\n",
    "        and task_name not in [\"qm9_homo\", \"qm9_lumo\", \"qm9_homo_lumo_gap\", \"bace\"]\n",
    "        and \"smol\" not in task_name\n",
    "    ):\n",
    "        base_path = f\"dataset/{task_name}\"\n",
    "        os.makedirs(base_path, exist_ok=True)\n",
    "        tasks, datasets, transformers = loading_fn(\n",
    "            featurizer=\"Raw\",\n",
    "            splitter=\"scaffold\",\n",
    "            save_dir=base_path,\n",
    "            data_dir=base_path,\n",
    "            reload=True,\n",
    "        )\n",
    "        train_dataset, valid_dataset, test_dataset = datasets\n",
    "    else:\n",
    "        # dataset from mol-instruction is already loaded\n",
    "        pass\n",
    "    return tasks, train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "\n",
    "def from_dict(dict):\n",
    "    class Struct:\n",
    "        def __init__(self, **entries):\n",
    "            self.__dict__.update(entries)\n",
    "\n",
    "    return Struct(**dict)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    import os\n",
    "    import random\n",
    "\n",
    "    # get arg replace_ratio, dataset_path\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config_dir\", type=str, default=\"./configs/download/\")\n",
    "    parser.add_argument(\"--config\", type=str, default=\"default\")\n",
    "    parser.add_argument(\"--train_procs\", type=int, default=32)\n",
    "    parser.add_argument(\"--test_procs\", type=int, default=32)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    \n",
    "    arg_path = os.path.join(args.config_dir, args.config) + \".yaml\"\n",
    "    # read config file\n",
    "    with open(arg_path, \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    # convert args to be accessible its values by attributes\n",
    "    cfg = from_dict(cfg)\n",
    "\n",
    "    raw_data_root = cfg.raw_data_root\n",
    "\n",
    "    if not os.path.exists(raw_data_root):\n",
    "        os.makedirs(raw_data_root)\n",
    "\n",
    "    start, end = added_tokens.SELFIES\n",
    "    task_subtask_dict, task_subtask_pairs = get_task_subtask_info(\n",
    "        cfg.target_benchmarks\n",
    "    )\n",
    "    data_tag = cfg.data_tag\n",
    "\n",
    "    downloading_task_subtask_pairs = []\n",
    "    for task_subtask_pair in task_subtask_pairs:\n",
    "        task, subtask_idx = task_subtask_pair\n",
    "        if os.path.exists(\n",
    "            f\"{raw_data_root}/{task}_subtask-{subtask_idx}_train\"\n",
    "        ) and os.path.exists(f\"{raw_data_root}/{task}_subtask-{subtask_idx}_test\"):\n",
    "            print(f\"{task}_{subtask_idx} already exists\")\n",
    "        else:\n",
    "            downloading_task_subtask_pairs.append(task_subtask_pair)\n",
    "\n",
    "    for task_subtask_pair in tqdm(\n",
    "        downloading_task_subtask_pairs, desc=\"Downloading task_subtask_pairs\"\n",
    "    ):\n",
    "        task_name = task_subtask_pair[0]\n",
    "        new_dataset = get_dataset(task_name=task_name, raw_data_root=raw_data_root)\n",
    "\n",
    "        subtasks = new_dataset[0]\n",
    "        subtask_idx = task_subtask_pair[1]\n",
    "        if subtask_idx == \"multi_label_classification\":\n",
    "            task_subtask_pair = f\"{task_name}/multi_label_classification\"\n",
    "        elif task_name in [\"toxcast\", \"tox21\", \"qm9_additional_label\", \"hopv\"]:\n",
    "            task_subtask_pair = f\"{task_name}/{subtasks[subtask_idx]}\"\n",
    "        else:\n",
    "            task_subtask_pair = f\"{task_name}/0\"\n",
    "\n",
    "        data_split = new_dataset[1:]  # train_set, val_set, test_set\n",
    "        \n",
    "        def _task_arg_for(dataset_cls):\n",
    "            # MolInstructionDatset만은 'task' 단독 문자열이어야 함\n",
    "            return task_name if dataset_cls is MolInstructionDatset else task_subtask_pair\n",
    "\n",
    "        dataset_cls = (\n",
    "            SMolInstructDataset if \"smol\" in task_name else\n",
    "            MoleculeNetDatasetDeepChem if task_name in [\"toxcast\",\"tox21\",\"qm9_additional_label\",\"hopv\"] else\n",
    "            ChEBIDataset if task_name in [\"chebi-20-mol2text\",\"chebi-20-text2mol\"] else\n",
    "            MolInstructionDatset\n",
    "        )\n",
    "        \n",
    "        if \"smol\" in task_name:\n",
    "            dataset = SMolInstructDataset\n",
    "        elif task_name in [\n",
    "            \"toxcast\",\n",
    "            \"tox21\",\n",
    "            \"qm9_additional_label\",\n",
    "            \"hopv\"\n",
    "        ]:\n",
    "            dataset = MoleculeNetDatasetDeepChem\n",
    "        elif task_name in [\"chebi-20-mol2text\", \"chebi-20-text2mol\"]:\n",
    "            dataset = ChEBIDataset\n",
    "        # qm9 in regression benchmark is processed via MolInstructionDataset\n",
    "        elif task_name in [\n",
    "            \"chebi-20-text2mol\",\n",
    "            \"chebi-20-mol2text\",\n",
    "            \"reagent_prediction\",\n",
    "            \"forward_reaction_prediction\",\n",
    "            \"retrosynthesis\",\n",
    "            \"qm9_homo\",\n",
    "            \"qm9_lumo\",\n",
    "            \"qm9_homo_lumo_gap\",\n",
    "            \"bace\",\n",
    "        ]:\n",
    "            dataset = MolInstructionDatset\n",
    "\n",
    "        valid_dataset = dataset_cls(\n",
    "            data=data_split[1],\n",
    "            task_subtask_pair=_task_arg_for(dataset_cls),\n",
    "            subtask_idx=subtask_idx,\n",
    "        )\n",
    "        test_dataset = dataset_cls(\n",
    "            data=data_split[2],\n",
    "            task_subtask_pair=_task_arg_for(dataset_cls),\n",
    "            subtask_idx=subtask_idx,\n",
    "        )\n",
    "        train_dataset = dataset_cls(\n",
    "            data=data_split[0],\n",
    "            task_subtask_pair=_task_arg_for(dataset_cls),\n",
    "            subtask_idx=subtask_idx,\n",
    "        )\n",
    "        dataset_splits = {\n",
    "            \"val\": valid_dataset,\n",
    "            \"test\": test_dataset,\n",
    "            \"train\": train_dataset,\n",
    "        }\n",
    "\n",
    "        for split in dataset_splits.keys():\n",
    "            dataset = dataset_splits[split]\n",
    "            list_dict_data = []\n",
    "            for i in range(len(dataset)):\n",
    "                # 원래 튜플 해체\n",
    "                graph, label, input_mol_string, task_pair_or_name, instruction = dataset[i]\n",
    "\n",
    "                # 1) instruction 이 numpy.str_ 일 수도 있어서 문자열로 강제\n",
    "                if hasattr(instruction, \"item\"):\n",
    "                    instruction = instruction.item()\n",
    "                instruction = str(instruction)\n",
    "\n",
    "                # 2) reagent_prediction 등: graph 가 list[Data, Data]\n",
    "                #    -> 첫 번째를 main, 두 번째를 additional 로 넣어줍니다\n",
    "                if isinstance(graph, list):\n",
    "                    if len(graph) >= 2:\n",
    "                        g0, g1 = graph[0], graph[1]\n",
    "                    elif len(graph) == 1:\n",
    "                        g0 = g1 = graph[0]\n",
    "                    else:\n",
    "                        # 그래프가 비면 스킵\n",
    "                        continue\n",
    "                else:\n",
    "                    g0 = g1 = graph  # 단일 그래프인 대부분의 태스크\n",
    "\n",
    "                # 3) dict_data 생성\n",
    "                dict_data = {\n",
    "                    \"x\": g0.x,\n",
    "                    \"edge_index\": g0.edge_index,\n",
    "                    \"edge_attr\": g0.edge_attr,\n",
    "                    \"label\": label,\n",
    "                    \"input_mol_string\": input_mol_string,\n",
    "                    \"task_subtask_pair\": task_pair_or_name,\n",
    "                    \"instruction\": instruction,\n",
    "                    # reagent_prediction 처럼 2개짜리인 경우를 지원\n",
    "                    \"additional_x\": g1.x,\n",
    "                    \"additional_edge_index\": g1.edge_index,\n",
    "                    \"additional_edge_attr\": g1.edge_attr,\n",
    "                }\n",
    "                list_dict_data.append(dict_data)\n",
    "\n",
    "            # 만약 list_dict_data 가 비면 save_to_disk 가 또 실패하니 방어\n",
    "            if not list_dict_data:\n",
    "                print(f\"[warn] {task_name} split={split} produced no valid samples; skipping save.\")\n",
    "                continue\n",
    "\n",
    "            dataset = datasets.Dataset.from_list(list_dict_data)\n",
    "            dataset.save_to_disk(f\"{raw_data_root}/{task_name}_subtask-{subtask_idx}_{split}\")\n",
    "\n",
    "\n",
    "    trainsets = []\n",
    "    testsets = []\n",
    "    valsets = []\n",
    "    trainsets_dict = {}\n",
    "    testsets_dict = {}\n",
    "    valsets_dict = {}\n",
    "\n",
    "    for task_subtask_pair in task_subtask_pairs:\n",
    "        task, subtask_idx = task_subtask_pair\n",
    "        print(task, \"-task\")\n",
    "        trainset = datasets.Dataset.load_from_disk(\n",
    "            f\"{raw_data_root}/{task}_subtask-{subtask_idx}_train\"\n",
    "        )\n",
    "        trainsets.append(trainset)\n",
    "        trainsets_dict[task_subtask_pair] = trainset\n",
    "        valset = datasets.Dataset.load_from_disk(\n",
    "            f\"{raw_data_root}/{task}_subtask-{subtask_idx}_val\"\n",
    "        )\n",
    "        valsets.append(valset)\n",
    "        valsets_dict[task_subtask_pair] = valset\n",
    "        testset = datasets.Dataset.load_from_disk(\n",
    "            f\"{raw_data_root}/{task}_subtask-{subtask_idx}_test\"\n",
    "        )\n",
    "        testsets.append(testset)\n",
    "        testsets_dict[task_subtask_pair] = testset\n",
    "\n",
    "        print(f\"{task}_{subtask_idx} loaded\")\n",
    "\n",
    "    concat_trainset = datasets.concatenate_datasets(trainsets)\n",
    "    concat_testset = datasets.concatenate_datasets(testsets)\n",
    "    #concat_testset = datasets.concatenate_datasets(testsets + trainsets + valsets)\n",
    "        \n",
    "    from transformers import AutoTokenizer\n",
    "    system_prompt = \"You are a helpful assistant for molecular chemistry, to address tasks including molecular property classification, molecular property regression, chemical reaction prediction, molecule captioning, molecule generation.\"\n",
    "\n",
    "\n",
    "    def prepare_data_instance(\n",
    "            data_instance,\n",
    "            system_prompt,\n",
    "            mol_token=\"<mol>\",\n",
    "            num_query_tokens=32,\n",
    "    ):\n",
    "        input_mol_string = data_instance[\"input_mol_string\"]\n",
    "        input_mol_string = input_mol_string.replace(\"<SELFIES>\", \"<SELFIES> \").replace(\"</SELFIES>\", \" </SELFIES>\")\n",
    "        input_prompt = data_instance[\"instruction\"]\n",
    "\n",
    "\n",
    "        graph_sequence = \"<GRAPH>\" + mol_token * num_query_tokens + \"</GRAPH>\"\n",
    "        input_mol_string += graph_sequence\n",
    "        # assert \"<INPUT>\" in input_prompt, f\"llm_prompt should contain <INPUT_MOL>, {input_prompt}\"\n",
    "        if \"<INPUT>\" in input_prompt:\n",
    "            input_prompt = input_prompt.replace(\"<INPUT>\", input_mol_string)\n",
    "        else:\n",
    "            input_prompt = input_prompt\n",
    "\n",
    "        formatted_prompt_text = \"<s>[INST] \" + system_prompt + \" \\n\\n\" + input_prompt + \" [INST]\"\n",
    "        formatted_target_text = data_instance[\"label\"] + \" </s>\"\n",
    "\n",
    "        if \"additional\" in data_instance[\"task_subtask_pair\"]:\n",
    "            convert_dict = {\n",
    "                'qm9_additional_label/mu' : \"qm9_dipole_moment\",\n",
    "                'qm9_additional_label/alpha' : \"qm9_isotropic_polarizability\",\n",
    "                'qm9_additional_label/r2' : \"qm9_electronic_spatial_extent\",\n",
    "                'qm9_additional_label/zpve' : \"qm9_zero_point_vibrational_energy\",\n",
    "\n",
    "            }\n",
    "            task = convert_dict[data_instance[\"task_subtask_pair\"]]\n",
    "        else:\n",
    "            task = data_instance[\"task_subtask_pair\"]\n",
    "\n",
    "        data ={\n",
    "            \"task\": task,\n",
    "            \"x\": data_instance[\"x\"],\n",
    "            \"edge_index\": data_instance[\"edge_index\"],\n",
    "            \"edge_attr\": data_instance[\"edge_attr\"],\n",
    "            \"additional_x\": data_instance[\"additional_x\"],\n",
    "            \"additional_edge_index\": data_instance[\"additional_edge_index\"],\n",
    "            \"additional_edge_attr\": data_instance[\"additional_edge_attr\"],\n",
    "            \"prompt_text\": formatted_prompt_text,\n",
    "            \"target_text\": formatted_target_text,\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    \n",
    "    data_instance = concat_testset[0]\n",
    "\n",
    "    remove_keys = set(concat_testset.column_names)\n",
    "    remove_keys -= {\n",
    "        \"task\",\n",
    "        \"x\",\n",
    "        \"edge_index\",\n",
    "        \"edge_attr\",\n",
    "        \"additional_x\",\n",
    "        \"additional_edge_index\",\n",
    "        \"additional_edge_attr\",\n",
    "    }\n",
    "    mapped_trainset = concat_trainset.map(\n",
    "        lambda x: prepare_data_instance(\n",
    "            x, system_prompt=system_prompt,\n",
    "        ),\n",
    "            # use 36 processes\n",
    "            num_proc=args.train_procs\n",
    "    )\n",
    "    mapped_testset = concat_testset.map(\n",
    "        lambda x: prepare_data_instance(\n",
    "            x, system_prompt=system_prompt,\n",
    "        ),\n",
    "            # use 36 processes\n",
    "            num_proc=args.test_procs\n",
    "    )\n",
    "    llm_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    mol_representation = \"string+graph\"\n",
    "    num_query_token = 32\n",
    "    base_model = llm_model.replace(\"/\", \"-\")\n",
    "    tags = [base_model, mol_representation]\n",
    "    if \"graph\" in mol_representation:\n",
    "        tags += [f\"q{num_query_token}\"]\n",
    "    \n",
    "    processed_file_name = \"_\".join(tags)\n",
    "\n",
    "    mapped_trainset.save_to_disk(f\"{raw_data_root}/{processed_file_name}_train_{cfg.data_tag}\")\n",
    "    mapped_testset.save_to_disk(f\"{raw_data_root}/{processed_file_name}_test_{cfg.data_tag}\")\n",
    "    mapped_testset.save_to_disk(f\"{raw_data_root}/{processed_file_name}_validation_{cfg.data_tag}\")\n",
    "\n",
    "    a = 17\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
