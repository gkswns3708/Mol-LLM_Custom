{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a270ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "# from collections import Counter\n",
    "\n",
    "# test_ds = load_from_disk('Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_train_512_Truncation')\n",
    "# Counter(test_ds['task'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef5625a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n{\\n    'bace': o,\\n    'chebi-20-mol2text': o,\\n    'chebi-20-text2mol': o,\\n    'forward_reaction_prediction': o,\\n    'qm9_homo': o,\\n    'qm9_lumo': o,\\n    'qm9_homo_lumo_gap': o,\\n    'reagent_prediction': o,\\n    'retrosynthesis': o,\\n    'smol-property_prediction-clintox': o,\\n    'smol-property_prediction-bbbp': o,\\n    'smol-molecule_captioning': o,\\n    'smol-forward_synthesis': o,\\n    'smol-molecule_generation': o,\\n    'mol-name_conversion-i2s': o,\\n    'mol-name_conversion-s2i': o, \\n    'smol-property_prediction-esol': o,\\n    'smol-property_prediction-hiv': o,\\n    'smol-property_prediction-lipo': o,\\n    'smol-property_prediction-sider': o,\\n    'smol-retrosynthesis': o})\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "{\n",
    "    'bace': o,\n",
    "    'chebi-20-mol2text': o,\n",
    "    'chebi-20-text2mol': o,\n",
    "    'forward_reaction_prediction': o,\n",
    "    'qm9_homo': o,\n",
    "    'qm9_lumo': o,\n",
    "    'qm9_homo_lumo_gap': o,\n",
    "    'reagent_prediction': o,\n",
    "    'retrosynthesis': o,\n",
    "    'smol-property_prediction-clintox': o,\n",
    "    'smol-property_prediction-bbbp': o,\n",
    "    'smol-molecule_captioning': o,\n",
    "    'smol-forward_synthesis': o,\n",
    "    'smol-molecule_generation': o,\n",
    "    'mol-name_conversion-i2s': o,\n",
    "    'mol-name_conversion-s2i': o, \n",
    "    'smol-property_prediction-esol': o,\n",
    "    'smol-property_prediction-hiv': o,\n",
    "    'smol-property_prediction-lipo': o,\n",
    "    'smol-property_prediction-sider': o,\n",
    "    'smol-retrosynthesis': o})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e01b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "import os\n",
    "\n",
    "# 경로 설정\n",
    "BASE_DIR = \"/app/Mol-LLM_Custom/dataset/filtered_dataset\"\n",
    "OUTPUT_DIR = \"/app/Mol-LLM_Custom/dataset/merged_dataset\"\n",
    "\n",
    "# 데이터셋 이름과 경로\n",
    "DATASETS = {\n",
    "    \"bace\": f\"{BASE_DIR}/bace\",\n",
    "    \"chebi-20-mol2text\": f\"{BASE_DIR}/chebi-20-mol2text\",\n",
    "    \"chebi-20-text2mol\": f\"{BASE_DIR}/chebi-20-text2mol\",\n",
    "    \"qm9_homo\": f\"{BASE_DIR}/qm9_homo\",\n",
    "}\n",
    "\n",
    "# split 종류\n",
    "SPLITS = [\"train\", \"test\", \"val\"]\n",
    "\n",
    "# 파일명 패턴\n",
    "PREFIX = \"GSAI-ML-LLaDA-8B-Instruct_string+graph_q32\"\n",
    "SUFFIX = \"512_Truncation\"\n",
    "\n",
    "# 출력 파일명 suffix\n",
    "OUTPUT_SUFFIX = \"merged_bace_chebi_mol2text_chebi_text2mol_qm9_homo\"\n",
    "\n",
    "\n",
    "def merge_datasets_for_split(split: str):\n",
    "    \"\"\"특정 split에 대해 4개 데이터셋을 병합\"\"\"\n",
    "    datasets_to_merge = []\n",
    "\n",
    "    for dataset_name, dataset_path in DATASETS.items():\n",
    "        # 각 데이터셋의 split별 경로 구성\n",
    "        # 예: bace -> GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_train_512_Truncation_bace\n",
    "        split_dir = f\"{PREFIX}_{split}_{SUFFIX}_{dataset_name}\"\n",
    "        full_path = os.path.join(dataset_path, split_dir)\n",
    "\n",
    "        if os.path.exists(full_path):\n",
    "            print(f\"  로딩: {dataset_name} ({split})\")\n",
    "            ds = load_from_disk(full_path)\n",
    "            print(f\"    - 샘플 수: {len(ds)}\")\n",
    "            datasets_to_merge.append(ds)\n",
    "        else:\n",
    "            print(f\"  [경고] 경로 없음: {full_path}\")\n",
    "\n",
    "    if not datasets_to_merge:\n",
    "        print(f\"  [에러] {split}에 대해 병합할 데이터셋이 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    # 데이터셋 병합\n",
    "    merged = concatenate_datasets(datasets_to_merge)\n",
    "    print(f\"  병합 완료: 총 {len(merged)} 샘플\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 출력 디렉토리 생성\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"데이터셋 병합 시작\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for split in SPLITS:\n",
    "        print(f\"\\n[{split.upper()}] 병합 중...\")\n",
    "\n",
    "        merged_dataset = merge_datasets_for_split(split)\n",
    "\n",
    "        if merged_dataset is not None:\n",
    "            # 출력 경로 설정\n",
    "            # 예: GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_train_512_Truncation_merged_bace_chebi_mol2text_chebi_text2mol_qm9_homo\n",
    "            output_name = f\"{PREFIX}_{split}_{SUFFIX}_{OUTPUT_SUFFIX}\"\n",
    "            output_path = os.path.join(OUTPUT_DIR, output_name)\n",
    "\n",
    "            # 저장\n",
    "            print(f\"  저장 중: {output_path}\")\n",
    "            merged_dataset.save_to_disk(output_path)\n",
    "            print(f\"  저장 완료!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"모든 병합 완료!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 결과 확인\n",
    "    print(\"\\n[결과 확인]\")\n",
    "    for split in SPLITS:\n",
    "        output_name = f\"{PREFIX}_{split}_{SUFFIX}_{OUTPUT_SUFFIX}\"\n",
    "        output_path = os.path.join(OUTPUT_DIR, output_name)\n",
    "        if os.path.exists(output_path):\n",
    "            ds = load_from_disk(output_path)\n",
    "            print(f\"  {split}: {len(ds)} 샘플\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a840330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/MolDA_CHJ/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "usage: ipykernel_launcher.py [-h] [--dataset_path DATASET_PATH]\n",
      "                             [--selfies_dict SELFIES_DICT] [--top_k TOP_K]\n",
      "                             [--output OUTPUT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/root/.local/share/jupyter/runtime/kernel-v39bdc3123549b22d3e3491388678e219ef98e1ef4.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/MolDA_CHJ/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "SELFIES 토큰 등장 빈도 분석 스크립트\n",
    "\n",
    "Train dataset에서 SELFIES 토큰의 등장 빈도를 분석합니다:\n",
    "1. Input (Prompt) 부분의 SELFIES 토큰 등장 빈도\n",
    "2. Response (Output) 부분의 SELFIES 토큰 등장 빈도\n",
    "3. Total (Input + Response) 합산 등장 빈도\n",
    "\n",
    "Usage:\n",
    "    python utils/analyze_selfies_frequency.py --dataset_path <path> --selfies_dict <path> --top_k 100\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "def load_selfies_dict(selfies_dict_path: str) -> set:\n",
    "    \"\"\"SELFIES 토큰 사전 로드\"\"\"\n",
    "    with open(selfies_dict_path, 'r') as f:\n",
    "        tokens = {line.strip() for line in f.readlines() if line.strip()}\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def extract_selfies_tokens(text: str, selfies_dict: set) -> List[str]:\n",
    "    \"\"\"\n",
    "    텍스트에서 SELFIES 토큰 추출\n",
    "\n",
    "    SELFIES 토큰은 [...]  형태로 되어 있음\n",
    "    예: [C], [N], [=Branch1], [C@@H], [Ring1] 등\n",
    "    \"\"\"\n",
    "    # [...] 패턴 매칭 (SELFIES 토큰 형태)\n",
    "    pattern = r'\\[[^\\]]+\\]'\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    # selfies_dict에 있는 토큰만 필터링\n",
    "    selfies_tokens = [m for m in matches if m in selfies_dict]\n",
    "    return selfies_tokens\n",
    "\n",
    "\n",
    "def analyze_dataset(\n",
    "    dataset_path: str,\n",
    "    selfies_dict_path: str,\n",
    "    top_k: int = 100\n",
    ") -> Tuple[Counter, Counter, Counter]:\n",
    "    \"\"\"\n",
    "    데이터셋 분석\n",
    "\n",
    "    Returns:\n",
    "        input_counter: Input(Prompt)에서의 토큰 빈도\n",
    "        response_counter: Response(Output)에서의 토큰 빈도\n",
    "        total_counter: 합산 빈도\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset from: {dataset_path}\")\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "\n",
    "    print(f\"Loading SELFIES dictionary from: {selfies_dict_path}\")\n",
    "    selfies_dict = load_selfies_dict(selfies_dict_path)\n",
    "    print(f\"  - SELFIES dictionary size: {len(selfies_dict)}\")\n",
    "\n",
    "    input_counter = Counter()\n",
    "    response_counter = Counter()\n",
    "\n",
    "    print(f\"\\nAnalyzing {len(dataset)} samples...\")\n",
    "\n",
    "    for i, sample in enumerate(dataset):\n",
    "        # Input (prompt_text) 분석\n",
    "        prompt_text = sample.get('prompt_text', '')\n",
    "        input_tokens = extract_selfies_tokens(prompt_text, selfies_dict)\n",
    "        input_counter.update(input_tokens)\n",
    "\n",
    "        # Response (target_text) 분석\n",
    "        target_text = sample.get('target_text', '')\n",
    "        response_tokens = extract_selfies_tokens(target_text, selfies_dict)\n",
    "        response_counter.update(response_tokens)\n",
    "\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(f\"  Processed {i + 1}/{len(dataset)} samples...\")\n",
    "\n",
    "    # Total = Input + Response\n",
    "    total_counter = input_counter + response_counter\n",
    "\n",
    "    return input_counter, response_counter, total_counter\n",
    "\n",
    "\n",
    "def print_top_k_tokens(\n",
    "    counter: Counter,\n",
    "    title: str,\n",
    "    top_k: int = 100\n",
    ") -> None:\n",
    "    \"\"\"상위 K개 토큰 출력\"\"\"\n",
    "    total_count = sum(counter.values())\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{title}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total token occurrences: {total_count:,}\")\n",
    "    print(f\"Unique tokens: {len(counter):,}\")\n",
    "    print(f\"\\nTop {top_k} tokens:\")\n",
    "    print(f\"{'Rank':<6} {'Token':<20} {'Count':>12} {'Ratio':>10} {'Cumulative':>12}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "\n",
    "    cumulative = 0\n",
    "    for rank, (token, count) in enumerate(counter.most_common(top_k), 1):\n",
    "        ratio = count / total_count * 100 if total_count > 0 else 0\n",
    "        cumulative += ratio\n",
    "        print(f\"{rank:<6} {token:<20} {count:>12,} {ratio:>9.2f}% {cumulative:>11.2f}%\")\n",
    "\n",
    "\n",
    "def save_results_to_file(\n",
    "    input_counter: Counter,\n",
    "    response_counter: Counter,\n",
    "    total_counter: Counter,\n",
    "    output_path: str,\n",
    "    top_k: int = 100\n",
    ") -> None:\n",
    "    \"\"\"결과를 파일로 저장\"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        for counter, title in [\n",
    "            (total_counter, \"TOTAL (Input + Response)\"),\n",
    "            (input_counter, \"INPUT (Prompt) Only\"),\n",
    "            (response_counter, \"RESPONSE (Output) Only\"),\n",
    "        ]:\n",
    "            total_count = sum(counter.values())\n",
    "\n",
    "            f.write(f\"\\n{'='*80}\\n\")\n",
    "            f.write(f\"{title}\\n\")\n",
    "            f.write(f\"{'='*80}\\n\")\n",
    "            f.write(f\"Total token occurrences: {total_count:,}\\n\")\n",
    "            f.write(f\"Unique tokens: {len(counter):,}\\n\")\n",
    "            f.write(f\"\\nTop {top_k} tokens:\\n\")\n",
    "            f.write(f\"{'Rank':<6} {'Token':<20} {'Count':>12} {'Ratio':>10} {'Cumulative':>12}\\n\")\n",
    "            f.write(f\"{'-'*60}\\n\")\n",
    "\n",
    "            cumulative = 0\n",
    "            for rank, (token, count) in enumerate(counter.most_common(top_k), 1):\n",
    "                ratio = count / total_count * 100 if total_count > 0 else 0\n",
    "                cumulative += ratio\n",
    "                f.write(f\"{rank:<6} {token:<20} {count:>12,} {ratio:>9.2f}% {cumulative:>11.2f}%\\n\")\n",
    "\n",
    "    print(f\"\\nResults saved to: {output_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Analyze SELFIES token frequency in dataset\")\n",
    "    parser.add_argument(\n",
    "        \"--dataset_path\",\n",
    "        type=str,\n",
    "        default=\"Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_train_512_Truncation_merged_bace_chebi_mol2text_chebi_text2mol_qm9_homo\",\n",
    "        help=\"Path to the dataset\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--selfies_dict\",\n",
    "        type=str,\n",
    "        default=\"Mol-LLM_Custom/model/selfies_dict.txt\",\n",
    "        help=\"Path to SELFIES dictionary file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--top_k\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help=\"Number of top tokens to display\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Output file path (optional, prints to stdout if not specified)\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # 분석 실행\n",
    "    input_counter, response_counter, total_counter = analyze_dataset(\n",
    "        args.dataset_path,\n",
    "        args.selfies_dict,\n",
    "        args.top_k\n",
    "    )\n",
    "\n",
    "    # 결과 출력\n",
    "    print_top_k_tokens(total_counter, \"TOTAL (Input + Response)\", args.top_k)\n",
    "    print_top_k_tokens(input_counter, \"INPUT (Prompt) Only\", args.top_k)\n",
    "    print_top_k_tokens(response_counter, \"RESPONSE (Output) Only\", args.top_k)\n",
    "\n",
    "    # 파일로 저장 (옵션)\n",
    "    if args.output:\n",
    "        save_results_to_file(\n",
    "            input_counter, response_counter, total_counter,\n",
    "            args.output, args.top_k\n",
    "        )\n",
    "\n",
    "    # 추가 통계\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    input_total = sum(input_counter.values())\n",
    "    response_total = sum(response_counter.values())\n",
    "    total_total = sum(total_counter.values())\n",
    "\n",
    "    print(f\"Input tokens:    {input_total:>12,} ({input_total/total_total*100:.1f}%)\")\n",
    "    print(f\"Response tokens: {response_total:>12,} ({response_total/total_total*100:.1f}%)\")\n",
    "    print(f\"Total tokens:    {total_total:>12,}\")\n",
    "\n",
    "    # Input에만 있는 토큰 vs Response에만 있는 토큰\n",
    "    input_only = set(input_counter.keys()) - set(response_counter.keys())\n",
    "    response_only = set(response_counter.keys()) - set(input_counter.keys())\n",
    "    both = set(input_counter.keys()) & set(response_counter.keys())\n",
    "\n",
    "    print(f\"\\nUnique token distribution:\")\n",
    "    print(f\"  - Input only:    {len(input_only):>6} tokens\")\n",
    "    print(f\"  - Response only: {len(response_only):>6} tokens\")\n",
    "    print(f\"  - Both:          {len(both):>6} tokens\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10916ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/MolDA_CHJ/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from collections import Counter\n",
    "\n",
    "train_ds = load_from_disk('/app/Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_train_512_Truncation_chebi-20-mol2text') \n",
    "test_ds = load_from_disk('/app/Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_test_512_Truncation_chebi-20-mol2text') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c8f7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'chebi-20-mol2text': 26113})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_ds['task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973f1495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'chebi-20-mol2text': 327})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(test_ds['task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fce81c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MolDA_CHJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
