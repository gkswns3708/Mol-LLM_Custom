{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aca01d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1967084579.py, line 102)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 102\u001b[0;36m\u001b[0m\n\u001b[0;31m    check_all_csv_files():\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# [ì„¤ì •] ê²€ìƒ‰ ì‹œì‘ ë£¨íŠ¸ ê²½ë¡œ (ì´ ê²½ë¡œ í•˜ìœ„ì˜ ëª¨ë“  í´ë”ë¥¼ ë’¤ì§‘ë‹ˆë‹¤)\n",
    "# =============================================================================\n",
    "BASE_DIR = \"/home/jovyan/CHJ/Mol-LLM_Custom/Inference_log/Benchmark_inference_csv\"\n",
    "\n",
    "# ê²€ì‚¬í•  ì¢…ë£Œ í† í°\n",
    "TARGET_TOKEN = \"<|eot_id|>\"\n",
    "\n",
    "def check_all_csv_files():\n",
    "    # 1. ëª¨ë“  CSV íŒŒì¼ ì°¾ê¸° (ì¬ê·€ì  íƒìƒ‰)\n",
    "    print(f\"ğŸ” Searching for CSV files in: {BASE_DIR} ...\")\n",
    "    all_csv_files = glob.glob(os.path.join(BASE_DIR, \"**/*.csv\"), recursive=True)\n",
    "    \n",
    "    # ì´ë¯¸ ìƒì„±ëœ ì—ëŸ¬ ë¦¬í¬íŠ¸ íŒŒì¼ì€ ì œì™¸ (íŒŒì¼ëª…ì— '_error'ê°€ í¬í•¨ëœ ê²½ìš° ë“±)\n",
    "    target_files = [f for f in all_csv_files if \"_no_eot_error\" not in f and \"checkpoint\" not in f]\n",
    "    \n",
    "    print(f\"found {len(target_files)} CSV files to check.\\n\")\n",
    "\n",
    "    summary_report = []\n",
    "\n",
    "    # 2. íŒŒì¼ ìˆœíšŒí•˜ë©° ê²€ì‚¬\n",
    "    for file_path in tqdm(target_files, desc=\"Checking files\"):\n",
    "        try:\n",
    "            # ë¡œë”© ì†ë„ë¥¼ ìœ„í•´ 'label' ì»¬ëŸ¼ë§Œ ë¡œë“œ (ì—†ìœ¼ë©´ ì „ì²´ ë¡œë“œ ì‹œë„ í›„ ì—ëŸ¬ ì²˜ë¦¬)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, usecols=['label'])\n",
    "            except ValueError:\n",
    "                # label ì»¬ëŸ¼ì´ ì—†ëŠ” íŒŒì¼ì¼ ìˆ˜ ìˆìŒ (ì „ì²´ ì½ì–´ì„œ í™•ì¸)\n",
    "                df = pd.read_csv(file_path)\n",
    "                if 'label' not in df.columns:\n",
    "                    continue  # label ì»¬ëŸ¼ ì—†ìœ¼ë©´ ìŠ¤í‚µ\n",
    "\n",
    "            total_rows = len(df)\n",
    "            if total_rows == 0:\n",
    "                continue\n",
    "\n",
    "            # 3. ê²€ì‚¬ ë¡œì§ (ê³µë°± ì œê±° í›„ í† í° í™•ì¸)\n",
    "            # NaNì€ ë¬¸ìì—´ ë³€í™˜ -> ê³µë°± ì œê±° -> endswith í™•ì¸ -> ë¶€ì •(~)\n",
    "            mask = ~df['label'].astype(str).str.strip().str.endswith(TARGET_TOKEN)\n",
    "            error_count = mask.sum()\n",
    "\n",
    "            status = \"âœ… Pass\"\n",
    "            saved_path = \"-\"\n",
    "\n",
    "            # 4. ë¬¸ì œê°€ ìˆë‹¤ë©´ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥\n",
    "            if error_count > 0:\n",
    "                status = f\"âŒ {error_count} Errors\"\n",
    "                \n",
    "                # ì „ì²´ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ì½ì–´ì™€ì„œ(ì»¬ëŸ¼ ì „ì²´) ë¬¸ì œ í–‰ ì €ì¥\n",
    "                full_df = pd.read_csv(file_path)\n",
    "                problematic_df = full_df[mask]\n",
    "                \n",
    "                output_path = file_path.replace(\".csv\", \"_no_eot_error.csv\")\n",
    "                problematic_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "                saved_path = os.path.basename(output_path)\n",
    "\n",
    "            # ë¦¬í¬íŠ¸ì— ì¶”ê°€\n",
    "            summary_report.append({\n",
    "                \"File Name\": os.path.basename(file_path),\n",
    "                \"Total Rows\": total_rows,\n",
    "                \"Missing EOT\": error_count,\n",
    "                \"Status\": status,\n",
    "                \"Saved File\": saved_path,\n",
    "                \"Full Path\": file_path\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[Error] Failed to process {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "    # 5. ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "    if summary_report:\n",
    "        result_df = pd.DataFrame(summary_report)\n",
    "        \n",
    "        # ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥í•˜ê¸° ìœ„í•´ ì •ë ¬ (ì—ëŸ¬ ë§ì€ ìˆœ)\n",
    "        result_df = result_df.sort_values(by=\"Missing EOT\", ascending=False)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ“Š [Check Summary Report]\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # ì£¼ìš” ì»¬ëŸ¼ë§Œ ì¶œë ¥\n",
    "        print(result_df[['File Name', 'Total Rows', 'Missing EOT', 'Status']].to_string(index=False))\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # ì—ëŸ¬ê°€ ìˆëŠ” íŒŒì¼ë“¤ë§Œ ëª¨ì•„ì„œ ê²½ë¡œ ì¶œë ¥\n",
    "        error_files = result_df[result_df['Missing EOT'] > 0]\n",
    "        if not error_files.empty:\n",
    "            print(f\"\\nâš ï¸ Total {len(error_files)} files have missing '{TARGET_TOKEN}' issues.\")\n",
    "        else:\n",
    "            print(f\"\\nâœ… All checked files are clean! (All labels end with '{TARGET_TOKEN}')\")\n",
    "            \n",
    "    else:\n",
    "        print(\"No CSV files processed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_all_csv_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb230d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2071e9397840a8b88e01041fb308de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/32595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_ds = load_from_disk('/home/jovyan/CHJ/Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_test_512_Truncation')\n",
    "test_smol_generation_ds = test_ds.filter(lambda x:  '<SELFIES>[C][C][=Branch1][C][=O][N][C@H1][C@H1][Branch2][Ring1][#Branch2][O][C@H1][C@H1][Branch1][C][O][C@@H1][Branch1][#Branch1][N][C][Branch1][C][C][=O]' in x['target_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5902db93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': [[5, 0, 4, 5, 3, 0, 2, 0, 0], [5, 0, 4, 5, 3, 0, 2, 0, 0]],\n",
       " 'edge_index': [[0, 1], [1, 0]],\n",
       " 'edge_attr': [[0, 0, 0], [0, 0, 0]],\n",
       " 'label': '<SELFIES>[C][C][=Branch1][C][=O][N][C@H1][C@H1][Branch2][Ring1][#Branch2][O][C@H1][C@H1][Branch1][C][O][C@@H1][Branch1][#Branch1][N][C][Branch1][C][C][=O][C][Branch1][C][O][O][C@@H1][Ring1][N][C][O][O][C@H1][Branch1][Ring1][C][O][C@@H1][Branch2][Branch2][#Branch1][O][C@@H1][O][C@H1][Branch2][Branch1][#C][C][O][C@H1][O][C@H1][Branch2][Ring1][=Branch1][C][O][C@H1][O][C@H1][Branch1][Ring1][C][O][C@@H1][Branch1][C][O][C@H1][Branch1][C][O][C@@H1][Ring1][#Branch2][O][C@@H1][Branch1][C][O][C@H1][Branch2][Ring1][Branch1][O][C@H1][O][C@H1][Branch1][Ring1][C][O][C@@H1][Branch1][C][O][C@H1][Branch1][C][O][C@@H1][Ring1][#Branch2][O][C@@H1][Ring2][Ring1][S][O][C@@H1][Branch1][C][O][C@H1][Branch2][Ring1][Branch1][O][C@H1][O][C@H1][Branch1][Ring1][C][O][C@@H1][Branch1][C][O][C@H1][Branch1][C][O][C@@H1][Ring1][#Branch2][O][C@@H1][Ring2][Branch1][=Branch1][O][C@@H1][Ring2][=Branch1][#C][O]</SELFIES>',\n",
       " 'input_mol_string': '<SELFIES> <None> </SELFIES>',\n",
       " 'task_subtask_pair': 'smol-molecule_generation/0',\n",
       " 'instruction': 'Generate a molecule that fulfills the requirement: <DESCRIPTION>The molecule is an aminoheptasaccharide that is the MAN-3 glycan, alpha-D-Manp-(1->3)-[alpha-D-Manp-(1->6)]-beta-D-Manp-(1->4)-beta-D-GlcpNAc-(1->4)-D-GlcpNAc, in which the (1->6) linked mannosyl group has been glycosylated at positions 3 and 6 by alpha-D-mannopyranosyl groups. It is an amino heptasaccharide and a high-mannose oligosaccharide.</DESCRIPTION>',\n",
       " 'additional_x': [[5, 0, 4, 5, 3, 0, 2, 0, 0], [5, 0, 4, 5, 3, 0, 2, 0, 0]],\n",
       " 'additional_edge_index': [[0, 1], [1, 0]],\n",
       " 'additional_edge_attr': [[0, 0, 0], [0, 0, 0]],\n",
       " 'task': 'smol-molecule_generation',\n",
       " 'prompt_text': '<|startoftext|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant for molecular chemistry, to address tasks including molecular property classification, molecular property regression, chemical reaction prediction, molecule captioning, molecule generation.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nGenerate a molecule that fulfills the requirement: <DESCRIPTION>The molecule is an aminoheptasaccharide that is the MAN-3 glycan, alpha-D-Manp-(1->3)-[alpha-D-Manp-(1->6)]-beta-D-Manp-(1->4)-beta-D-GlcpNAc-(1->4)-D-GlcpNAc, in which the (1->6) linked mannosyl group has been glycosylated at positions 3 and 6 by alpha-D-mannopyranosyl groups. It is an amino heptasaccharide and a high-mannose oligosaccharide.</DESCRIPTION><GRAPH> <mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol> </GRAPH><|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
       " 'target_text': '<SELFIES>[C][C][=Branch1][C][=O][N][C@H1][C@H1][Branch2][Ring1][#Branch2][O][C@H1][C@H1][Branch1][C][O][C@@H1][Branch1][#Branch1][N][C][Branch1][C][C][=O][C][Branch1][C][O][O][C@@H1][Ring1][N][C][O][O][C@H1][Branch1][Ring1][C][O][C@@H1][Branch2][Branch2][#Branch1][O][C@@H1][O][C@H1][Branch2][Branch1][#C][C][O][C@H1][O][C@H1][Branch2][Ring1][=Branch1][C][O][C@H1][O][C@H1][Branch1][Ring1][C][O][C@@H1][Branch1][C][O][C@H1][Branch1][C][O][C@@H1][Ring1][#Branch2][O][C@@H1][Branch1][C][O][C@H1][Branch2][Ring1][Branch1][O][C@H1][O][C@H1][Branch1][Ring1][C][O][C@@H1][Branch1][C][O][C@H1][Branch1][C][O][C@@H1][Ring1][#Branch2][O][C@@H1][Ring2][Ring1][S][O][C@@H1][Branch1][C][O][C@H1][Branch2][Ring1][Branch1][O][C@H1][O][C@H1][Branch1][Ring1][C][O][C@@H1][Branch1][C][O][C@H1][Branch1][C][O][C@@H1][Ring1][#Branch2][O][C@@H1][Ring2][Branch1][=Branch1][O][C@@H1][Ring2][=Branch1][#C][O]</SELFIES><|eot_id|>'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_smol_generation_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf78b1a",
   "metadata": {},
   "source": [
    "# Benchmark ë°ì´í„°ì…‹ ê²€ì¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe59f7",
   "metadata": {},
   "source": [
    "## labelì˜ ëì´ `<|eot_id|>`ì´ ì•„ë‹Œ sample csv ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7000f5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Searching for CSV files in: /home/jovyan/CHJ/Mol-LLM_Custom/Inference_log/Benchmark_inference_csv ...\n",
      "found 20 CSV files to check.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 32.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸš¨ Found Total 229 error rows from 8 files.\n",
      "ğŸ’¾ Merged files saved to:\n",
      "  - JSON: /home/jovyan/CHJ/Mol-LLM_Custom/utils/all_missing_eot_errors.json\n",
      "  - CSV:  /home/jovyan/CHJ/Mol-LLM_Custom/utils/all_missing_eot_errors.csv\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# [ì„¤ì •] ê²½ë¡œ ì„¤ì •\n",
    "# =============================================================================\n",
    "SEARCH_DIR = \"/home/jovyan/CHJ/Mol-LLM_Custom/Inference_log/Benchmark_inference_csv\"\n",
    "SAVE_DIR = \"/home/jovyan/CHJ/Mol-LLM_Custom/utils\"\n",
    "\n",
    "TARGET_TOKEN = \"<|eot_id|>\"\n",
    "\n",
    "# ì €ì¥í•  íŒŒì¼ëª… ì •ì˜\n",
    "OUTPUT_JSON_NAME = \"all_missing_eot_errors.json\"\n",
    "OUTPUT_CSV_NAME = \"all_missing_eot_errors.csv\"\n",
    "\n",
    "def save_merged_error_samples():\n",
    "    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    \n",
    "    print(f\"ğŸ” Searching for CSV files in: {SEARCH_DIR} ...\")\n",
    "    \n",
    "    # 1. ëª¨ë“  CSV íŒŒì¼ ì¬ê·€ íƒìƒ‰\n",
    "    all_csv_files = glob.glob(os.path.join(SEARCH_DIR, \"**/*.csv\"), recursive=True)\n",
    "    \n",
    "    # ê²°ê³¼ íŒŒì¼, ì²´í¬í¬ì¸íŠ¸ ë“± ì œì™¸\n",
    "    target_files = [\n",
    "        f for f in all_csv_files \n",
    "        if \"_error\" not in f and \"checkpoint\" not in f\n",
    "    ]\n",
    "    \n",
    "    print(f\"found {len(target_files)} CSV files to check.\\n\")\n",
    "\n",
    "    # ëª¨ë“  ì—ëŸ¬ ë°ì´í„°ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸\n",
    "    all_error_dfs = []\n",
    "\n",
    "    # 2. íŒŒì¼ ìˆœíšŒí•˜ë©° ê²€ì‚¬\n",
    "    for file_path in tqdm(target_files, desc=\"Scanning files\"):\n",
    "        try:\n",
    "            # CSV ì½ê¸°\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "            except Exception:\n",
    "                continue \n",
    "\n",
    "            if 'label' not in df.columns:\n",
    "                continue\n",
    "\n",
    "            # 3. ì—ëŸ¬ ë°ì´í„° í•„í„°ë§ (ê³µë°± ì œê±° í›„ í† í° í™•ì¸)\n",
    "            mask = ~df['label'].astype(str).str.strip().str.endswith(TARGET_TOKEN)\n",
    "            \n",
    "            error_df = df[mask].copy()\n",
    "\n",
    "            # 4. ì—ëŸ¬ê°€ ìˆë‹¤ë©´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "            if not error_df.empty:\n",
    "                # [ì¤‘ìš”] ì–´ë–¤ íŒŒì¼ì—ì„œ ì˜¨ ì—ëŸ¬ì¸ì§€ ì¶œì²˜ ê¸°ë¡\n",
    "                error_df['source_file'] = os.path.basename(file_path)\n",
    "                \n",
    "                # ì›ë³¸ csvì˜ ì „ì²´ ê²½ë¡œë„ í•„ìš”í•˜ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ\n",
    "                # error_df['source_path'] = file_path \n",
    "                \n",
    "                all_error_dfs.append(error_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "    # 5. í•˜ë‚˜ë¡œ ë³‘í•© ë° ì €ì¥\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    if all_error_dfs:\n",
    "        # ëª¨ë“  ì—ëŸ¬ DataFrame í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°\n",
    "        merged_df = pd.concat(all_error_dfs, ignore_index=True)\n",
    "        \n",
    "        total_errors = len(merged_df)\n",
    "        unique_files = merged_df['source_file'].nunique()\n",
    "        \n",
    "        print(f\"ğŸš¨ Found Total {total_errors} error rows from {unique_files} files.\")\n",
    "\n",
    "        # ì €ì¥ ê²½ë¡œ ìƒì„±\n",
    "        json_path = os.path.join(SAVE_DIR, OUTPUT_JSON_NAME)\n",
    "        csv_path = os.path.join(SAVE_DIR, OUTPUT_CSV_NAME)\n",
    "\n",
    "        # (1) JSON ì €ì¥\n",
    "        merged_dict = merged_df.to_dict(orient='records')\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(merged_dict, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        # (2) CSV ì €ì¥\n",
    "        merged_df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "        print(f\"ğŸ’¾ Merged files saved to:\\n  - JSON: {json_path}\\n  - CSV:  {csv_path}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"âœ… No errors found! All labels properly end with '{TARGET_TOKEN}'.\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_merged_error_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c046c",
   "metadata": {},
   "source": [
    "# ì‹¤ì œ ë°ì´í„° verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e16004c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7b4c53521841a28c167673e06f5c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/32595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_ds = load_from_disk('/home/jovyan/CHJ/Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_test_512_Truncation')\n",
    "test_chebi_text2mol_ds = test_ds.filter(lambda x: x['task'] == 'chebi-20-text2mol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f485fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': [[5, 0, 4, 5, 3, 0, 2, 0, 0], [5, 0, 4, 5, 3, 0, 2, 0, 0]],\n",
       " 'edge_index': [[0, 1], [1, 0]],\n",
       " 'edge_attr': [[0, 0, 0], [0, 0, 0]],\n",
       " 'label': '<SELFIES>[C][O][C][=Branch1][C][=O][/C][=C][/C][C][C@H1][C@@H1][C][C][C][=C][C][=Branch1][C][=O][C][C][C@][Ring1][#Branch1][Branch1][C][C][C@H1][Ring1][N][C][=Branch1][C][=O][C][C@][Ring2][Ring1][Ring2][Ring1][P][C]</SELFIES>',\n",
       " 'input_mol_string': '<SELFIES> <None> </SELFIES>',\n",
       " 'task_subtask_pair': 'chebi-20-text2mol/0',\n",
       " 'instruction': 'Generate a molecule that fulfills the requirement: <DESCRIPTION>The molecule is a steroid ester that is methyl (17E)-pregna-4,17-dien-21-oate substituted by oxo groups at positions 3 and 11. It is a 3-oxo-Delta(4) steroid, an 11-oxo steroid, a steroid ester and a methyl ester. It derives from a hydride of a pregnane.</DESCRIPTION>',\n",
       " 'additional_x': [[5, 0, 4, 5, 3, 0, 2, 0, 0], [5, 0, 4, 5, 3, 0, 2, 0, 0]],\n",
       " 'additional_edge_index': [[0, 1], [1, 0]],\n",
       " 'additional_edge_attr': [[0, 0, 0], [0, 0, 0]],\n",
       " 'task': 'chebi-20-text2mol',\n",
       " 'prompt_text': '<|startoftext|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant for molecular chemistry, to address tasks including molecular property classification, molecular property regression, chemical reaction prediction, molecule captioning, molecule generation.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nGenerate a molecule that fulfills the requirement: <DESCRIPTION>The molecule is a steroid ester that is methyl (17E)-pregna-4,17-dien-21-oate substituted by oxo groups at positions 3 and 11. It is a 3-oxo-Delta(4) steroid, an 11-oxo steroid, a steroid ester and a methyl ester. It derives from a hydride of a pregnane.</DESCRIPTION><GRAPH> <mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol> </GRAPH><|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n',\n",
       " 'target_text': '<SELFIES>[C][O][C][=Branch1][C][=O][/C][=C][/C][C][C@H1][C@@H1][C][C][C][=C][C][=Branch1][C][=O][C][C][C@][Ring1][#Branch1][Branch1][C][C][C@H1][Ring1][N][C][=Branch1][C][=O][C][C@][Ring2][Ring1][Ring2][Ring1][P][C]</SELFIES><|eot_id|>'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_chebi_text2mol_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9a90fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MolDA_CHJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
