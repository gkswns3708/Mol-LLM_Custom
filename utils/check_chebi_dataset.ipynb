{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35cb86c",
   "metadata": {},
   "source": [
    "# Mol-LLM 데이터셋 검증 스크립트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dce496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== [Phase 1] Loading & Internal Deduplication (Pandas) ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cc0cb94433465cb2ab2e570a1c007d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import selfies as sf\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from datasets import load_from_disk, Dataset, enable_progress_bar\n",
    "from tabulate import tabulate\n",
    "import multiprocessing\n",
    "\n",
    "# HF Progress Bar 활성화\n",
    "enable_progress_bar()\n",
    "\n",
    "# =============================================================================\n",
    "# [Configuration]\n",
    "# =============================================================================\n",
    "SCAFFOLD_SPLIT_TASKS = {\"bace\", \"bbbp\", \"clintox\", \"tox21\", \"toxcast\", \"sider\", \"hiv\", \"muv\", \"esol\", \"freesolv\", \"lipo\", \"hopv\"}\n",
    "CATEGORY_ORDER = [\"Property Prediction (Regression)\", \"Property Prediction (Classification)\", \"Forward Reaction Prediction\", \"Retrosynthesis\", \"Reagent Prediction\", \"Molecule Captioning\", \"Description-Guided Molecule Generation\", \"Name Conversion\"]\n",
    "TASK_MAPPING = {\n",
    "    \"Property Prediction (Regression)\": [\"qm9_homo\", \"qm9_lumo\", \"qm9_homo_lumo_gap\", \"qm9_additional_label\", \"smol-property_prediction-esol\", \"smol-property_prediction-lipo\", \"smol-property_prediction-freesolv\", \"esol\", \"lipo\", \"freesolv\"],\n",
    "    \"Property Prediction (Classification)\": [\"bace\", \"tox21\", \"toxcast\", \"clintox\", \"bbbp\", \"hiv\", \"sider\", \"muv\", \"hopv\", \"smol-property_prediction-bbbp\", \"smol-property_prediction-clintox\", \"smol-property_prediction-hiv\", \"smol-property_prediction-sider\", \"smol-property_prediction-tox21\", \"smol-property_prediction-toxcast\", \"smol-property_prediction-muv\"],\n",
    "    \"Forward Reaction Prediction\": [\"forward_reaction_prediction\", \"smol-forward_synthesis\"],\n",
    "    \"Retrosynthesis\": [\"retrosynthesis\", \"smol-retrosynthesis\"],\n",
    "    \"Reagent Prediction\": [\"reagent_prediction\"],\n",
    "    \"Molecule Captioning\": [\"chebi-20-mol2text\", \"smol-molecule_captioning\"],\n",
    "    \"Description-Guided Molecule Generation\": [\"chebi-20-text2mol\", \"smol-molecule_generation\"],\n",
    "    \"Name Conversion\": [\"smol-name_conversion-i2s\", \"smol-name_conversion-s2i\", \"smol-name_conversion-i2f\", \"smol-name_conversion-s2f\"]\n",
    "}\n",
    "DATA_SOURCES = {\"Property Prediction (Regression)\": \"MoleculeNet\", \"Property Prediction (Classification)\": \"MoleculeNet\", \"Forward Reaction Prediction\": \"USPTO\", \"Retrosynthesis\": \"USPTO 500MT\", \"Reagent Prediction\": \"USPTO 500K\", \"Molecule Captioning\": \"ChEBI-20\", \"Description-Guided Molecule Generation\": \"ChEBI-20\", \"Name Conversion\": \"PubChem\"}\n",
    "\n",
    "def decode_and_get_info(batch):\n",
    "    input_mols = batch[\"input_mol_string\"]\n",
    "    canon_smiles_list, scaffold_list, valid_list = [], [], []\n",
    "    for input_mol in input_mols:\n",
    "        res_smiles, res_scaffold, is_valid = \"\", \"\", False\n",
    "        try:\n",
    "            if input_mol:\n",
    "                clean_str = re.sub(r\"<[^>]+>\", \"\", str(input_mol)).strip()\n",
    "                smiles = sf.decoder(clean_str)\n",
    "                if smiles:\n",
    "                    mol = Chem.MolFromSmiles(smiles)\n",
    "                    if mol:\n",
    "                        res_smiles = Chem.MolToSmiles(mol, canonical=True)\n",
    "                        res_scaffold = MurckoScaffold.MurckoScaffoldSmiles(mol=mol) or \"\"\n",
    "                        is_valid = True\n",
    "        except: pass\n",
    "        canon_smiles_list.append(res_smiles); scaffold_list.append(res_scaffold); valid_list.append(is_valid)\n",
    "    return {\"canon_smiles\": canon_smiles_list, \"scaffold\": scaffold_list, \"valid\": valid_list}\n",
    "\n",
    "def map_task_to_category(task_name):\n",
    "    for cat, tasks in TASK_MAPPING.items():\n",
    "        if task_name in tasks: return cat\n",
    "    return \"Others\"\n",
    "\n",
    "def final_cleanup_and_stats_fast(train_path, val_path, test_path, base_save_dir, num_cores=32):\n",
    "    print(f\"=== [Phase 1] Loading & Internal Deduplication (Pandas) ===\")\n",
    "    splits = {\"train\": train_path, \"val\": val_path, \"test\": test_path}\n",
    "    dfs = {}\n",
    "    \n",
    "    for name, path in splits.items():\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"[Skip] Path not found: {path}\")\n",
    "            continue\n",
    "        ds = load_from_disk(path)\n",
    "        df = ds.to_pandas()\n",
    "        \n",
    "        possible_output_cols = ['label', 'output_string', 'output', 'target', 'answers']\n",
    "        actual_out_col = next((c for c in possible_output_cols if c in df.columns), None)\n",
    "        if actual_out_col is None:\n",
    "            raise KeyError(f\"[{name}] 정답 컬럼을 찾을 수 없습니다. 목록: {df.columns.tolist()}\")\n",
    "        \n",
    "        initial_len = len(df)\n",
    "        df = df.drop_duplicates(subset=['task', 'input_mol_string', actual_out_col], keep='first').copy()\n",
    "        dfs[name] = df\n",
    "        print(f\" -> {name.upper()}: {initial_len:,} -> {len(df):,} (Removed: {initial_len - len(df):,})\")\n",
    "\n",
    "    print(f\"\\n=== [Phase 2] Molecular Parsing (Multiprocessing) ===\")\n",
    "    for name in [\"train\", \"val\", \"test\"]:\n",
    "        if name not in dfs: continue\n",
    "        print(f\" -> Parsing {name}...\")\n",
    "        temp_ds = Dataset.from_pandas(dfs[name][['task', 'input_mol_string']], preserve_index=False)\n",
    "        # num_proc 사용 시 Notebook 환경 안정성을 위해 batch_size 조절\n",
    "        parsed = temp_ds.map(decode_and_get_info, batched=True, batch_size=1000, num_proc=num_cores, desc=f\"Mapping {name}\")\n",
    "        parsed_df = parsed.to_pandas()\n",
    "        \n",
    "        info_only_df = parsed_df[['canon_smiles', 'scaffold', 'valid']].reset_index(drop=True)\n",
    "        dfs[name] = pd.concat([dfs[name].reset_index(drop=True), info_only_df], axis=1).copy()\n",
    "\n",
    "    print(f\"\\n=== [Phase 3] Fast Decontamination (Pandas Masking) ===\")\n",
    "    test_df = dfs[\"test\"]\n",
    "    test_black_smiles = test_df[test_df['valid']].groupby('task')['canon_smiles'].apply(set).to_dict()\n",
    "    test_black_scaf = test_df[test_df['valid']].groupby('task')['scaffold'].apply(set).to_dict()\n",
    "\n",
    "    final_dfs = {\"test\": dfs[\"test\"]}\n",
    "    for name in [\"train\", \"val\"]:\n",
    "        if name not in dfs: continue\n",
    "        df = dfs[name]\n",
    "        \n",
    "        def is_leak(row):\n",
    "            t, s, scaf, v = row['task'], row['canon_smiles'], row['scaffold'], row['valid']\n",
    "            if not v: return False\n",
    "            if s in test_black_smiles.get(t, set()): return True\n",
    "            if t in SCAFFOLD_SPLIT_TASKS:\n",
    "                if scaf in test_black_scaf.get(t, set()): return True\n",
    "            return False\n",
    "\n",
    "        tqdm.pandas(desc=f\"Filtering {name}\")\n",
    "        leak_mask = df.progress_apply(is_leak, axis=1)\n",
    "        final_dfs[name] = df[~leak_mask].copy()\n",
    "        print(f\" -> {name.upper()} Decontaminated: {len(df):,} -> {len(final_dfs[name]):,}\")\n",
    "\n",
    "    print(f\"\\n=== [Phase 4] Statistics & Final Saving ===\")\n",
    "    table_data = []\n",
    "    totals = {\"Train\": 0, \"Val\": 0, \"Test\": 0, \"All\": 0}\n",
    "\n",
    "    for cat in CATEGORY_ORDER:\n",
    "        cat_counts = {\"Train\": 0, \"Val\": 0, \"Test\": 0}\n",
    "        for split in [\"train\", \"val\", \"test\"]:\n",
    "            if split not in final_dfs: continue\n",
    "            split_df = final_dfs[split]\n",
    "            tasks_in_cat = TASK_MAPPING[cat]\n",
    "            count = split_df[split_df['task'].isin(tasks_in_cat)].shape[0]\n",
    "            cat_counts[split.capitalize()] = count\n",
    "        \n",
    "        n_all = cat_counts[\"Train\"] + cat_counts[\"Test\"]\n",
    "        table_data.append({\n",
    "            \"Task Category\": cat,\n",
    "            \"Data Sources\": DATA_SOURCES.get(cat, \"-\"),\n",
    "            \"# Train\": f\"{cat_counts['Train']:,}\",\n",
    "            \"# Val\": f\"{cat_counts['Val']:,}\",\n",
    "            \"# Test\": f\"{cat_counts['Test']:,}\",\n",
    "            \"# All\": f\"{n_all:,}\"\n",
    "        })\n",
    "        for k in [\"Train\", \"Val\", \"Test\"]: totals[k] += cat_counts[k]\n",
    "        totals[\"All\"] += n_all\n",
    "\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(tabulate(pd.DataFrame(table_data), headers=\"keys\", tablefmt=\"github\", stralign=\"right\"))\n",
    "    print(\"-\" * 120)\n",
    "    print(f\"{'Overall':<40} {'':<15} {totals['Train']:>10,} {totals['Val']:>10,} {totals['Test']:>10,} {totals['All']:>10,}\")\n",
    "    print(\"=\"*120)\n",
    "\n",
    "    for name in [\"train\", \"val\", \"test\"]:\n",
    "        if name not in final_dfs: continue\n",
    "        final_df = final_dfs[name].drop(columns=[\"canon_smiles\", \"scaffold\", \"valid\"])\n",
    "        save_path = os.path.join(base_save_dir, f\"GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_{name}_FINAL_CLEANED\")\n",
    "        Dataset.from_pandas(final_df, preserve_index=False).save_to_disk(save_path)\n",
    "        print(f\"[Saved] {name.upper()} -> {save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # OS 환경에 따른 멀티프로세싱 시작 방식 설정 (Unix 계열 권장)\n",
    "    try:\n",
    "        multiprocessing.set_start_method('spawn', force=True)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "    train_in = \"Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_train_3.3M_0415_raw\"\n",
    "    val_in = \"Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_validation_3.3M_0415_raw\"\n",
    "    test_in = \"Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_test_3.3M_0415_raw\"\n",
    "    save_dir = \"Mol-LLM_Custom/dataset/train_official/\"\n",
    "    \n",
    "    # 멈춤 현상 방지를 위해 num_cores를 시스템 물리 코어의 절반 정도로 설정하는 것을 권장합니다 (예: 16~32)\n",
    "    final_cleanup_and_stats_fast(train_in, val_in, test_in, save_dir, num_cores=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7aaecf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498c35f0c39f4f1280a60bb18e4bb3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(3464783, 32822, 36016)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "train_path = \"Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_train_3.3M_0415_deduplicate_CLEANED\"\n",
    "test_path = \"Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_test_3.3M_0415_deduplicate_CLEANED\"\n",
    "val_path = \"Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_validation_3.3M_0415_deduplicate_CLEANED\"\n",
    "\n",
    "train_ds = load_from_disk(train_path)\n",
    "test_ds = load_from_disk(test_path)\n",
    "val_ds = load_from_disk(val_path)\n",
    "\n",
    "len(train_ds), len(test_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe8fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "import selfies as sf\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from datasets import load_from_disk, Dataset, enable_progress_bar\n",
    "from tabulate import tabulate\n",
    "import multiprocessing\n",
    "\n",
    "# HF Progress Bar 활성화\n",
    "enable_progress_bar()\n",
    "\n",
    "# =============================================================================\n",
    "# [Configuration]\n",
    "# =============================================================================\n",
    "SCAFFOLD_SPLIT_TASKS = {\"bace\", \"bbbp\", \"clintox\", \"tox21\", \"toxcast\", \"sider\", \"hiv\", \"muv\", \"esol\", \"freesolv\", \"lipo\", \"hopv\"}\n",
    "CATEGORY_ORDER = [\n",
    "    \"Property Prediction (Regression)\", \"Property Prediction (Classification)\", \n",
    "    \"Forward Reaction Prediction\", \"Retrosynthesis\", \"Reagent Prediction\", \n",
    "    \"Molecule Captioning\", \"Description-Guided Molecule Generation\", \"Name Conversion\"\n",
    "]\n",
    "\n",
    "TASK_MAPPING = {\n",
    "    \"Property Prediction (Regression)\": [\"qm9_homo\", \"qm9_lumo\", \"qm9_homo_lumo_gap\", \"qm9_additional_label\", \"smol-property_prediction-esol\", \"smol-property_prediction-lipo\", \"smol-property_prediction-freesolv\", \"esol\", \"lipo\", \"freesolv\"],\n",
    "    \"Property Prediction (Classification)\": [\"bace\", \"tox21\", \"toxcast\", \"clintox\", \"bbbp\", \"hiv\", \"sider\", \"muv\", \"hopv\", \"smol-property_prediction-bbbp\", \"smol-property_prediction-clintox\", \"smol-property_prediction-hiv\", \"smol-property_prediction-sider\", \"smol-property_prediction-tox21\", \"smol-property_prediction-toxcast\", \"smol-property_prediction-muv\"],\n",
    "    \"Forward Reaction Prediction\": [\"forward_reaction_prediction\", \"smol-forward_synthesis\"],\n",
    "    \"Retrosynthesis\": [\"retrosynthesis\", \"smol-retrosynthesis\"],\n",
    "    \"Reagent Prediction\": [\"reagent_prediction\"],\n",
    "    \"Molecule Captioning\": [\"chebi-20-mol2text\", \"smol-molecule_captioning\"],\n",
    "    \"Description-Guided Molecule Generation\": [\"chebi-20-text2mol\", \"smol-molecule_generation\"],\n",
    "    \"Name Conversion\": [\"smol-name_conversion-i2s\", \"smol-name_conversion-s2i\", \"smol-name_conversion-i2f\", \"smol-name_conversion-s2f\"]\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# [Step 2: Molecular Parsing Logic]\n",
    "# =============================================================================\n",
    "def decode_and_get_info(batch):\n",
    "    \"\"\"HF map 전용: 멀티프로세싱으로 분자 정보 파싱\"\"\"\n",
    "    input_mols = batch[\"input_mol_string\"]\n",
    "    canon_smiles_list, scaffold_list, valid_list = [], [], []\n",
    "    for input_mol in input_mols:\n",
    "        res_smiles, res_scaffold, is_valid = \"\", \"\", False\n",
    "        try:\n",
    "            if input_mol:\n",
    "                clean_str = re.sub(r\"<[^>]+>\", \"\", str(input_mol)).strip()\n",
    "                smiles = sf.decoder(clean_str)\n",
    "                if smiles:\n",
    "                    mol = Chem.MolFromSmiles(smiles)\n",
    "                    if mol:\n",
    "                        res_smiles = Chem.MolToSmiles(mol, canonical=True)\n",
    "                        res_scaffold = MurckoScaffold.MurckoScaffoldSmiles(mol=mol) or \"\"\n",
    "                        is_valid = True\n",
    "        except: pass\n",
    "        canon_smiles_list.append(res_smiles); scaffold_list.append(res_scaffold); valid_list.append(is_valid)\n",
    "    return {\"canon_smiles\": canon_smiles_list, \"scaffold\": scaffold_list, \"valid\": valid_list}\n",
    "\n",
    "# =============================================================================\n",
    "# [Main Pipeline]\n",
    "# =============================================================================\n",
    "def final_integrated_cleanup(train_path, val_path, test_path, base_save_dir, num_cores=24):\n",
    "    print(f\"=== [Step 2] Molecular Parsing with HF Multiprocessing ===\")\n",
    "    splits = {\"train\": train_path, \"val\": val_path, \"test\": test_path}\n",
    "    dfs = {}\n",
    "    \n",
    "    for name, path in splits.items():\n",
    "        print(f\" -> Processing {name} split...\")\n",
    "        ds = load_from_disk(path)\n",
    "        \n",
    "        # 1. 필요한 컬럼만 추출하여 병렬 파싱 수행\n",
    "        # (병목을 줄이기 위해 batch_size를 키우고 num_proc 활용)\n",
    "        parsed_info = ds.select_columns(['task', 'input_mol_string']).map(\n",
    "            decode_and_get_info, batched=True, batch_size=2000, num_proc=num_cores, desc=f\"Parsing {name}\"\n",
    "        )\n",
    "        \n",
    "        # 2. 파싱 결과와 원본 데이터를 Pandas에서 결합 (가장 빠른 방식)\n",
    "        df_raw = ds.to_pandas()\n",
    "        df_info = parsed_info.to_pandas()[['canon_smiles', 'scaffold', 'valid']]\n",
    "        dfs[name] = pd.concat([df_raw.reset_index(drop=True), df_info.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    print(f\"\\n=== [Step 3] Fast Decontamination & Cross-Dataset Dedup ===\")\n",
    "    # 드랍 원인 추적을 위한 로그\n",
    "    drop_log = defaultdict(Counter)\n",
    "    \n",
    "    # Test 세트의 블랙리스트 구축 (Fast Lookup을 위해 set 사용)\n",
    "    test_df = dfs[\"test\"]\n",
    "    test_black_smiles = test_df[test_df['valid']].groupby('task')['canon_smiles'].apply(set).to_dict()\n",
    "    test_black_scaf = test_df[test_df['valid']].groupby('task')['scaffold'].apply(set).to_dict()\n",
    "\n",
    "    final_dfs = {\"test\": dfs[\"test\"]}\n",
    "    \n",
    "    for name in [\"train\", \"val\"]:\n",
    "        df = dfs[name]\n",
    "        out_col = next((c for c in ['label', 'output_string', 'output', 'target'] if c in df.columns), \"output\")\n",
    "        \n",
    "        # A. Leakage 체크 (Test셋 유출 제거)\n",
    "        def check_leak(row):\n",
    "            t, s, scaf, v = row['task'], row['canon_smiles'], row['scaffold'], row['valid']\n",
    "            if not v: return \"Keep\"\n",
    "            if s in test_black_smiles.get(t, set()): return \"Drop: Exact Match\"\n",
    "            if t in SCAFFOLD_SPLIT_TASKS and scaf in test_black_scaf.get(t, set()): return \"Drop: Scaffold Match\"\n",
    "            return \"Keep\"\n",
    "\n",
    "        tqdm.pandas(desc=f\"Checking leaks in {name}\")\n",
    "        df['status'] = df.progress_apply(check_leak, axis=1)\n",
    "        \n",
    "        # B. Cross-Dataset 중복 제거 (retrosynthesis vs smol-retrosynthesis 등)\n",
    "        # 내용이 같으면 중복으로 간주하고 하나만 남김\n",
    "        df_clean = df[df['status'] == \"Keep\"].copy()\n",
    "        initial_sub_count = len(df_clean)\n",
    "        \n",
    "        # Pandas의 drop_duplicates는 C 기반이라 수백만 행도 매우 빠름\n",
    "        df_unique = df_clean.drop_duplicates(subset=['input_mol_string', out_col], keep='first')\n",
    "        \n",
    "        # 기록 업데이트\n",
    "        for task, status in zip(df['task'], df['status']):\n",
    "            if status != \"Keep\": drop_log[task][f\"{name.upper()} - {status}\"] += 1\n",
    "        \n",
    "        # 중복으로 지워진 개수 기록\n",
    "        removed_dups = initial_sub_count - len(df_unique)\n",
    "        print(f\" -> {name.upper()}: Removed {removed_dups:,} cross-dataset duplicates.\")\n",
    "        \n",
    "        final_dfs[name] = df_unique\n",
    "\n",
    "    print(f\"\\n=== [Step 4] Final Report & Saving ===\")\n",
    "    # 1. 상세 드랍 원인 리포트\n",
    "    if drop_log:\n",
    "        print(\"\\n[Detailed Drop Reasons by Task]\")\n",
    "        print(tabulate(pd.DataFrame(drop_log).T.fillna(0), headers=\"keys\", tablefmt=\"grid\"))\n",
    "\n",
    "    # 2. 최종 통계 테이블 출력 및 저장\n",
    "    table_data = []\n",
    "    for cat in CATEGORY_ORDER:\n",
    "        counts = {s: final_dfs[s][final_dfs[s]['task'].isin(TASK_MAPPING[cat])].shape[0] for s in [\"train\", \"val\", \"test\"]}\n",
    "        table_data.append({\n",
    "            \"Task\": cat, \"# Train\": f\"{counts['train']:,}\", \"# Val\": f\"{counts['val']:,}\", \n",
    "            \"# Test\": f\"{counts['test']:,}\", \"# All\": f\"{counts['train']+counts['test']:,}\"\n",
    "        })\n",
    "    \n",
    "    print(\"\\n\" + tabulate(table_data, headers=\"keys\", tablefmt=\"github\", stralign=\"right\"))\n",
    "\n",
    "    for name, df in final_dfs.items():\n",
    "        save_path = os.path.join(base_save_dir, f\"GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_{name}_FINAL_CLEANED\")\n",
    "        # 분석용 컬럼 제거 후 저장\n",
    "        final_ds = Dataset.from_pandas(df.drop(columns=['canon_smiles', 'scaffold', 'valid', 'status'], errors='ignore'), preserve_index=False)\n",
    "        final_ds.save_to_disk(save_path)\n",
    "        print(f\"[Saved] {save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 데이터 경로 설정\n",
    "    train_in = \"Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_train_3.3M_0415_raw\"\n",
    "    val_in = \"Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_validation_3.3M_0415_raw\"\n",
    "    test_in = \"Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_test_3.3M_0415_raw\"\n",
    "    save_dir = \"Mol-LLM_Custom/dataset/train_official/\"\n",
    "    \n",
    "    final_integrated_cleanup(train_in, val_in, test_in, save_dir, num_cores=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859afde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "\n",
    "# =============================================================================\n",
    "# [1. 설정: Task Group 매핑 정의]\n",
    "# =============================================================================\n",
    "# 각 태스크 그룹에 속하는 개별 태스크 리스트입니다.\n",
    "TASK_GROUP_MAPPING = {\n",
    "    \"Property Prediction (Regression)\": [\n",
    "        \"qm9_homo\", \"qm9_lumo\", \"qm9_homo_lumo_gap\", \"qm9_additional_label\", \n",
    "        \"qm9_dipole_moment\", \"qm9_isotropic_polarizability\", \"qm9_electronic_spatial_extent\",\n",
    "        \"qm9_zero_point_vibrational_energy\", \"qm9_heat_capacity_298K\", \"qm9_internal_energy_298K\", \n",
    "        \"qm9_enthalpy_298K\", \"qm9_free_energy_298K\",\n",
    "        \"esol\", \"lipo\", \"freesolv\",\n",
    "        \"smol-property_prediction-esol\", \"smol-property_prediction-lipo\", \"smol-property_prediction-freesolv\"\n",
    "    ],\n",
    "    \"Property Prediction (Classification)\": [\n",
    "        \"bace\", \"bbbp\", \"clintox\", \"tox21\", \"toxcast\", \"sider\", \"hiv\", \"muv\", \"hopv\",\n",
    "        \"smol-property_prediction-bbbp\", \"smol-property_prediction-clintox\", \"smol-property_prediction-hiv\", \n",
    "        \"smol-property_prediction-sider\", \"smol-property_prediction-tox21\", \"smol-property_prediction-toxcast\", \n",
    "        \"smol-property_prediction-muv\"\n",
    "    ],\n",
    "    \"Forward Reaction Prediction\": [\n",
    "        \"forward_reaction_prediction\", \"smol-forward_synthesis\"\n",
    "    ],\n",
    "    \"Retrosynthesis\": [\n",
    "        \"retrosynthesis\", \"smol-retrosynthesis\"\n",
    "    ],\n",
    "    \"Reagent Prediction\": [\n",
    "        \"reagent_prediction\"\n",
    "    ],\n",
    "    \"Molecule Captioning\": [\n",
    "        \"chebi-20-mol2text\", \"smol-molecule_captioning\"\n",
    "    ],\n",
    "    \"Description-Guided Molecule Generation\": [\n",
    "        \"chebi-20-text2mol\", \"smol-molecule_generation\"\n",
    "    ],\n",
    "    \"Name Conversion\": [\n",
    "        \"smol-name_conversion-i2s\", \"smol-name_conversion-s2i\", \n",
    "        \"smol-name_conversion-i2f\", \"smol-name_conversion-s2f\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 태스크 이름을 입력하면 그룹 이름을 반환하는 역방향 매핑 생성\n",
    "TASK_TO_GROUP = {}\n",
    "for group, tasks in TASK_GROUP_MAPPING.items():\n",
    "    for task in tasks:\n",
    "        TASK_TO_GROUP[task] = group\n",
    "\n",
    "# =============================================================================\n",
    "# [2. 입력 데이터 (예시)]\n",
    "# =============================================================================\n",
    "# 사용자가 제공한 Train 카운터 데이터\n",
    "train_ds = load_from_disk('Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_train_3.3M_0415_raw')\n",
    "test_ds = load_from_disk('Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_test_3.3M_0415_raw')\n",
    "val_ds = load_from_disk('Mol-LLM_Custom/dataset/train_official/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_validation_3.3M_0415_raw')\n",
    "\n",
    "# Val/Test는 실제 데이터가 있다면 해당 Counter를 넣으시면 됩니다. (여기선 0으로 가정하거나 예시 값 사용)\n",
    "train_counts = Counter(train_ds['task'])\n",
    "val_counts = Counter(val_ds['task']) \n",
    "test_counts = Counter(test_ds['task'])\n",
    "\n",
    "# =============================================================================\n",
    "# [3. 집계 로직]\n",
    "# =============================================================================\n",
    "def generate_group_statistics(train_cnt, val_cnt, test_cnt):\n",
    "    table_data = []\n",
    "    \n",
    "    # 8개 그룹 순서대로 순회\n",
    "    for group_name in TASK_GROUP_MAPPING.keys():\n",
    "        # 해당 그룹에 속하는 태스크들 찾기\n",
    "        # (Counter에 있는 키들 중에서 현재 그룹에 속하는 것만 필터링)\n",
    "        \n",
    "        # 1. 현재 그룹에 매핑된 태스크 중 실제로 데이터에 존재하는 태스크 찾기\n",
    "        included_tasks = []\n",
    "        \n",
    "        # Train, Val, Test 모든 키를 합쳐서 검사\n",
    "        all_keys = set(train_cnt.keys()) | set(val_cnt.keys()) | set(test_cnt.keys())\n",
    "        \n",
    "        g_train = 0\n",
    "        g_val = 0\n",
    "        g_test = 0\n",
    "        \n",
    "        for task in all_keys:\n",
    "            # 해당 태스크가 현재 그룹에 속하는지 확인\n",
    "            if TASK_TO_GROUP.get(task) == group_name:\n",
    "                included_tasks.append(task)\n",
    "                g_train += train_cnt.get(task, 0)\n",
    "                g_val += val_cnt.get(task, 0)\n",
    "                g_test += test_cnt.get(task, 0)\n",
    "        \n",
    "        # 포함된 태스크 이름 정렬 및 문자열 변환\n",
    "        included_tasks_str = \", \".join(sorted(included_tasks)) if included_tasks else \"-\"\n",
    "        \n",
    "        # 결과 리스트 추가\n",
    "        table_data.append({\n",
    "            \"Task Group\": group_name,\n",
    "            \"Included Tasks\": included_tasks_str,\n",
    "            \"Train\": g_train,\n",
    "            \"Val\": g_val,\n",
    "            \"Test\": g_test,\n",
    "            \"Train + Test\": g_train + g_test\n",
    "        })\n",
    "\n",
    "    # DataFrame 생성\n",
    "    df = pd.DataFrame(table_data)\n",
    "    \n",
    "    # 숫자 포맷팅 (천 단위 콤마)\n",
    "    for col in [\"Train\", \"Val\", \"Test\", \"Train + Test\"]:\n",
    "        df[col] = df[col].apply(lambda x: f\"{x:,}\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# [4. 실행 및 출력]\n",
    "# =============================================================================\n",
    "result_df = generate_group_statistics(train_counts, val_counts, test_counts)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"Dataset Statistics by Task Group\")\n",
    "print(\"=\"*120)\n",
    "print(tabulate(result_df, headers=\"keys\", tablefmt=\"grid\", showindex=False))\n",
    "print(\"=\"*120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MolDA_CHJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
