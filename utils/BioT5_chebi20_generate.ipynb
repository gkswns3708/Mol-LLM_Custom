{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004118ce",
   "metadata": {},
   "source": [
    "# 새롭게 만든 실제 저자의 BioT5 FineTuning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67b13b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== 다운로드 & CSV 생성 시작 ====\n",
      "[OK] text2mol train: tasks/task1_chebi20_text2mol_train.json -> /root/.cache/huggingface/hub/datasets--QizhiPei--BioT5_finetune_dataset/snapshots/9f70da9e8f0df32e7e62846f7ad78a829b00c0fd/tasks/task1_chebi20_text2mol_train.json\n",
      "[OK] text2mol validation: tasks/task2_chebi20_text2mol_validation.json -> /root/.cache/huggingface/hub/datasets--QizhiPei--BioT5_finetune_dataset/snapshots/9f70da9e8f0df32e7e62846f7ad78a829b00c0fd/tasks/task2_chebi20_text2mol_validation.json\n",
      "[OK] text2mol test: tasks/task3_chebi20_text2mol_test.json -> /root/.cache/huggingface/hub/datasets--QizhiPei--BioT5_finetune_dataset/snapshots/9f70da9e8f0df32e7e62846f7ad78a829b00c0fd/tasks/task3_chebi20_text2mol_test.json\n",
      "[OK] mol2text train: tasks/task4_chebi20_mol2text_train.json -> /root/.cache/huggingface/hub/datasets--QizhiPei--BioT5_finetune_dataset/snapshots/9f70da9e8f0df32e7e62846f7ad78a829b00c0fd/tasks/task4_chebi20_mol2text_train.json\n",
      "[OK] mol2text validation: tasks/task5_chebi20_mol2text_validation.json -> /root/.cache/huggingface/hub/datasets--QizhiPei--BioT5_finetune_dataset/snapshots/9f70da9e8f0df32e7e62846f7ad78a829b00c0fd/tasks/task5_chebi20_mol2text_validation.json\n",
      "[OK] mol2text test: tasks/task6_chebi20_mol2text_test.json -> /root/.cache/huggingface/hub/datasets--QizhiPei--BioT5_finetune_dataset/snapshots/9f70da9e8f0df32e7e62846f7ad78a829b00c0fd/tasks/task6_chebi20_mol2text_test.json\n",
      "[INFO] mol2text train: 26407 instances 로드\n",
      "[OK] CSV 저장: /app/Mol-LLM_Custom/dataset/real_train/raw/chebi20_mol2text_train.csv (26407 rows)\n",
      "[INFO] mol2text validation: 3301 instances 로드\n",
      "[OK] CSV 저장: /app/Mol-LLM_Custom/dataset/real_train/raw/chebi20_mol2text_validation.csv (3301 rows)\n",
      "[INFO] mol2text test: 3300 instances 로드\n",
      "[OK] CSV 저장: /app/Mol-LLM_Custom/dataset/real_train/raw/chebi20_mol2text_test.csv (3300 rows)\n",
      "[INFO] text2mol train: 26407 instances 로드\n",
      "[OK] CSV 저장: /app/Mol-LLM_Custom/dataset/real_train/raw/chebi20_text2mol_train.csv (26407 rows)\n",
      "[INFO] text2mol validation: 3301 instances 로드\n",
      "[OK] CSV 저장: /app/Mol-LLM_Custom/dataset/real_train/raw/chebi20_text2mol_validation.csv (3301 rows)\n",
      "[INFO] text2mol test: 3300 instances 로드\n",
      "[OK] CSV 저장: /app/Mol-LLM_Custom/dataset/real_train/raw/chebi20_text2mol_test.csv (3300 rows)\n",
      "==== 완료 ====\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 기본 설정\n",
    "# --------------------------------------------------\n",
    "REPO_ID = \"QizhiPei/BioT5_finetune_dataset\"\n",
    "\n",
    "TASKS = {\n",
    "    \"text2mol\": {\n",
    "        \"train\": \"tasks/task1_chebi20_text2mol_train.json\",\n",
    "        \"validation\": \"tasks/task2_chebi20_text2mol_validation.json\",\n",
    "        \"test\": \"tasks/task3_chebi20_text2mol_test.json\",\n",
    "    },\n",
    "    \"mol2text\": {\n",
    "        \"train\": \"tasks/task4_chebi20_mol2text_train.json\",\n",
    "        \"validation\": \"tasks/task5_chebi20_mol2text_validation.json\",\n",
    "        \"test\": \"tasks/task6_chebi20_mol2text_test.json\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# ⚠️ 예전에 말한 경로로 다시 맞춰줌\n",
    "OUT_DIR = \"/app/Mol-LLM_Custom/dataset/real_train/raw\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. 모든 task JSON 다운로드\n",
    "# --------------------------------------------------\n",
    "def download_all_tasks():\n",
    "    local_paths = {}\n",
    "    for kind, splits in TASKS.items():\n",
    "        local_paths[kind] = {}\n",
    "        for split, remote in splits.items():\n",
    "            try:\n",
    "                local_path = hf_hub_download(\n",
    "                    repo_id=REPO_ID,\n",
    "                    repo_type=\"dataset\",\n",
    "                    filename=remote,\n",
    "                )\n",
    "                print(f\"[OK] {kind} {split}: {remote} -> {local_path}\")\n",
    "                local_paths[kind][split] = local_path\n",
    "            except Exception as e:\n",
    "                print(f\"[FAIL] {kind} {split}: {remote}\")\n",
    "                print(\"       \", e)\n",
    "    return local_paths\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. JSON에서 Instances 꺼내기\n",
    "# --------------------------------------------------\n",
    "def load_instances(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        if \"Instances\" in data:\n",
    "            return data[\"Instances\"]\n",
    "        elif \"instances\" in data:\n",
    "            return data[\"instances\"]\n",
    "        elif \"data\" in data:\n",
    "            return data[\"data\"]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"{path} : dict 구조인데 'Instances' 같은 리스트 키를 찾을 수 없음. \"\n",
    "                f\"keys={list(data.keys())[:10]}\"\n",
    "            )\n",
    "    elif isinstance(data, list):\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(f\"{path} : 알 수 없는 JSON 타입 {type(data)}\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. SELFIES 패턴 & 클리너\n",
    "# --------------------------------------------------\n",
    "def looks_like_selfies(s: str) -> bool:\n",
    "    # 아주 단순하지만 이 데이터에선 충분히 잘 동작할 패턴\n",
    "    return \"[\" in s and \"]\" in s\n",
    "\n",
    "def clean_selfies(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"<bom>\"):\n",
    "        s = s[len(\"<bom>\"):]\n",
    "    if s.endswith(\"<eom>\"):\n",
    "        s = s[:-len(\"<eom>\")]\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Instances -> DataFrame(SELFIES, description)\n",
    "#    (mol2text, text2mol 둘 다 여기로 처리)\n",
    "# --------------------------------------------------\n",
    "def instances_to_df_selfies_desc(instances):\n",
    "    rows = []\n",
    "    for inst in instances:\n",
    "        inp = inst.get(\"input\")\n",
    "        out_field = inst.get(\"output\", \"\")\n",
    "\n",
    "        # output이 리스트인 경우 첫 원소 사용\n",
    "        if isinstance(out_field, list):\n",
    "            out = out_field[0] if out_field else \"\"\n",
    "        else:\n",
    "            out = out_field\n",
    "\n",
    "        if not isinstance(inp, str) or not isinstance(out, str):\n",
    "            raise ValueError(f\"input/output이 문자열이 아님: {inst}\")\n",
    "\n",
    "        # SELFIES / description 자동 판별\n",
    "        if looks_like_selfies(inp) and not looks_like_selfies(out):\n",
    "            selfies = inp\n",
    "            desc = out\n",
    "        elif looks_like_selfies(out) and not looks_like_selfies(inp):\n",
    "            selfies = out\n",
    "            desc = inp\n",
    "        else:\n",
    "            # 둘 다 셀피처럼 보이거나 둘 다 텍스트처럼 보이면\n",
    "            # mol2text/text2mol 관계 생각 안 하고 그냥 input=SELFIES, output=desc로 둠\n",
    "            selfies = inp\n",
    "            desc = out\n",
    "\n",
    "        selfies = clean_selfies(selfies)\n",
    "\n",
    "        rows.append({\n",
    "            \"SELFIES\": selfies,\n",
    "            \"description\": desc,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 메인 실행\n",
    "# --------------------------------------------------\n",
    "def main():\n",
    "    print(\"==== 다운로드 & CSV 생성 시작 ====\")\n",
    "    local_paths = download_all_tasks()\n",
    "\n",
    "    # 1) mol2text → CSV\n",
    "    mol2text_paths = local_paths.get(\"mol2text\", {})\n",
    "    if not mol2text_paths:\n",
    "        print(\"[WARN] mol2text JSON을 하나도 못 받음\")\n",
    "    else:\n",
    "        for split, path in mol2text_paths.items():\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"[SKIP] mol2text {split}: 로컬 파일 없음 ({path})\")\n",
    "                continue\n",
    "\n",
    "            instances = load_instances(path)\n",
    "            print(f\"[INFO] mol2text {split}: {len(instances)} instances 로드\")\n",
    "\n",
    "            df = instances_to_df_selfies_desc(instances)\n",
    "            out_csv = os.path.join(OUT_DIR, f\"chebi20_mol2text_{split}.csv\")\n",
    "            df.to_csv(out_csv, index=False)\n",
    "            print(f\"[OK] CSV 저장: {out_csv} ({len(df)} rows)\")\n",
    "\n",
    "    # 2) text2mol → CSV\n",
    "    text2mol_paths = local_paths.get(\"text2mol\", {})\n",
    "    if not text2mol_paths:\n",
    "        print(\"[WARN] text2mol JSON을 하나도 못 받음\")\n",
    "    else:\n",
    "        for split, path in text2mol_paths.items():\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"[SKIP] text2mol {split}: 로컬 파일 없음 ({path})\")\n",
    "                continue\n",
    "\n",
    "            instances = load_instances(path)\n",
    "            print(f\"[INFO] text2mol {split}: {len(instances)} instances 로드\")\n",
    "\n",
    "            df = instances_to_df_selfies_desc(instances)\n",
    "            out_csv = os.path.join(OUT_DIR, f\"chebi20_text2mol_{split}.csv\")\n",
    "            df.to_csv(out_csv, index=False)\n",
    "            print(f\"[OK] CSV 저장: {out_csv} ({len(df)} rows)\")\n",
    "\n",
    "    print(\"==== 완료 ====\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce2c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: task4_chebi20_mol2text_train\n",
    "# valid: task5_chebi20_mol2text_validation\n",
    "# test: task6_chebi20_mol2text_test\n",
    "\n",
    "# train: task1_chebi20_text2mol_train\n",
    "# valid: task2_chebi20_text2mol_validation\n",
    "# test: task3_chebi20_text2mol_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff59ea",
   "metadata": {},
   "source": [
    "# 기존 Chebi-20 다운로드 받는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d212f8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved to: /app/Mol-LLM_Custom/dataset/real_train/raw\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from rdkit import Chem\n",
    "import selfies as sf\n",
    "\n",
    "# ===== 사용자 설정 =====\n",
    "DATASET_NAME = \"duongttr/chebi-20-new\"\n",
    "OUT_ROOT = \"/app/Mol-LLM_Custom/dataset/real_train/raw\"   # download_dataset.py의 raw_data_root 아래 raw/ 경로\n",
    "SPLIT_NAME = \"train\"                  # 보통 단일 split; 만약 이미 분할돼 있으면 적절히 바꾸세요.\n",
    "SEED = 42\n",
    "# ======================\n",
    "\n",
    "def pick(cols, cands):\n",
    "    for c in cands:\n",
    "        if c in cols: return c\n",
    "    return None\n",
    "\n",
    "def canon_smiles(s):\n",
    "    m = Chem.MolFromSmiles(s)\n",
    "    return Chem.MolToSmiles(m) if m else None\n",
    "\n",
    "ds = load_dataset(DATASET_NAME, split=SPLIT_NAME)\n",
    "\n",
    "cols = set(ds.column_names)\n",
    "cap_col    = pick(cols, [\"description\",\"caption\",\"text\",\"molecular_caption\",\"molecular_captions\"])\n",
    "selfies_col= pick(cols, [\"SELFIES\",\"selfies\"])\n",
    "smiles_col = pick(cols, [\"SMILES\",\"smiles\",\"smi\"])\n",
    "\n",
    "assert cap_col, \"캡션/설명 열을 찾지 못했습니다. (description/caption/text 등 후보 확인)\"\n",
    "# SELFIES 보장\n",
    "if selfies_col is None:\n",
    "    assert smiles_col, \"SELFIES가 없으므로 SMILES 열이 필요합니다.\"\n",
    "    ds = ds.map(lambda x: {\"_smi_canon\": canon_smiles(x[smiles_col])})\n",
    "    ds = ds.filter(lambda x: x[\"_smi_canon\"] is not None)\n",
    "    ds = ds.map(lambda x: {\"SELFIES\": sf.encoder(x[\"_smi_canon\"])})\n",
    "else:\n",
    "    if selfies_col != \"SELFIES\":\n",
    "        ds = ds.rename_column(selfies_col, \"SELFIES\")\n",
    "\n",
    "# description으로 표준화\n",
    "if cap_col != \"description\":\n",
    "    ds = ds.rename_column(cap_col, \"description\")\n",
    "\n",
    "# 필요한 열만 유지\n",
    "keep = [\"SELFIES\",\"description\"]\n",
    "ds = ds.remove_columns([c for c in ds.column_names if c not in keep])\n",
    "\n",
    "# 80/10/10 분할\n",
    "splits = ds.train_test_split(test_size=0.2, seed=SEED)\n",
    "tmp = splits[\"train\"].train_test_split(test_size=0.111111, seed=SEED)  # 0.111... of 0.9 ~= 0.1\n",
    "train, valid, test = tmp[\"train\"], tmp[\"test\"], splits[\"test\"]\n",
    "\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "train.to_pandas()[keep].to_csv(os.path.join(OUT_ROOT,\"BioT5_chebi20_train.csv\"), index=False)\n",
    "valid.to_pandas()[keep].to_csv(os.path.join(OUT_ROOT,\"BioT5_chebi20_valid.csv\"), index=False)\n",
    "test.to_pandas()[keep].to_csv(os.path.join(OUT_ROOT,\"BioT5_chebi20_test.csv\"), index=False)\n",
    "print(\"CSV saved to:\", OUT_ROOT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
