{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0884823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/miniconda3/envs/MolDA_CHJ/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_test_3.3M_0415_verified_filtered_512\n",
      "Warning: 'task' 컬럼을 대신 사용합니다.\n",
      "Original Dataset Size: 70,565\n",
      "Converting to Pandas DataFrame... (This might take a moment)\n",
      "Sampling 100 items per 'task'...\n",
      "Sampled DataFrame Size: 1,900\n",
      "Final Dataset Size: 1,900\n",
      "Saving sampled dataset to: /home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_test_3.3M_0415_verified_filtered_512_sampled_100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1900/1900 [00:00<00:00, 73347.91 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_from_disk, Dataset\n",
    "import os\n",
    "\n",
    "# 1. 데이터셋 경로 설정\n",
    "dataset_path = \"/home/jovyan/CHJ/Mol-LLM_Custom/dataset/train/GSAI-ML-LLaDA-8B-Instruct_string+graph_q32_test_3.3M_0415_verified_filtered_512\"\n",
    "save_path = dataset_path + \"_sampled_100\"\n",
    "\n",
    "def sample_dataset_fast_pandas(path, n_samples=100, task_col=\"Task\"):\n",
    "    print(f\"Loading dataset from: {path}\")\n",
    "    \n",
    "    try:\n",
    "        ds = load_from_disk(path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 해당 경로에서 데이터셋을 찾을 수 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    # 컬럼 이름 확인\n",
    "    if task_col not in ds.column_names:\n",
    "        if \"task\" in ds.column_names:\n",
    "            task_col = \"task\"\n",
    "            print(f\"Warning: '{task_col}' 컬럼을 대신 사용합니다.\")\n",
    "        else:\n",
    "            print(f\"Error: 데이터셋에 '{task_col}' 컬럼이 없습니다.\")\n",
    "            return None\n",
    "\n",
    "    print(f\"Original Dataset Size: {len(ds):,}\")\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # [핵심] Pandas로 변환하여 고속 처리\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"Converting to Pandas DataFrame... (This might take a moment)\")\n",
    "    df = ds.to_pandas()\n",
    "    \n",
    "    print(f\"Sampling {n_samples} items per '{task_col}'...\")\n",
    "    \n",
    "    # 1. 전체 데이터를 무작위로 섞습니다 (random_state로 재현성 확보)\n",
    "    #    이렇게 하면 뒤에서 그냥 위에서부터 잘라도(head) 랜덤 샘플링과 같은 효과입니다.\n",
    "    df_shuffled = df.sample(frac=1, random_state=42)\n",
    "    \n",
    "    # 2. Task별로 그룹화한 뒤, 상위 n_samples개만 남깁니다.\n",
    "    #    데이터가 n_samples보다 적으면 알아서 전부 가져오고, 많으면 n개만 자릅니다.\n",
    "    #    for loop 없이 내부적으로 최적화된 연산입니다.\n",
    "    sampled_df = df_shuffled.groupby(task_col).head(n_samples)\n",
    "    \n",
    "    print(f\"Sampled DataFrame Size: {len(sampled_df):,}\")\n",
    "    \n",
    "    # ---------------------------------------------------------\n",
    "    # 다시 Hugging Face Dataset으로 변환\n",
    "    # ---------------------------------------------------------\n",
    "    final_ds = Dataset.from_pandas(sampled_df)\n",
    "    \n",
    "    # to_pandas() -> from_pandas() 과정에서 생기는 인덱스 컬럼 제거\n",
    "    if \"__index_level_0__\" in final_ds.column_names:\n",
    "        final_ds = final_ds.remove_columns([\"__index_level_0__\"])\n",
    "        \n",
    "    print(f\"Final Dataset Size: {len(final_ds):,}\")\n",
    "    \n",
    "    return final_ds\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    sampled_dataset = sample_dataset_fast_pandas(dataset_path, n_samples=100)\n",
    "\n",
    "    if sampled_dataset:\n",
    "        print(f\"Saving sampled dataset to: {save_path}\")\n",
    "        sampled_dataset.save_to_disk(save_path)\n",
    "        print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7eff0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MolDA_CHJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
