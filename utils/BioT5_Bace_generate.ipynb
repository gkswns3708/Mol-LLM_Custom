{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d369bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"QizhiPei/BioT5_finetune_dataset\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5754baca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[경고] 유효하지 않은 SELFIES 1210개 제거\n",
      "train       tasks_plus/task31_bace_molnet_train.json  ->  /app/Mol-LLM/dataset/train/raw/BioT5_bace_train.csv  (rows=0)\n",
      "[경고] 유효하지 않은 SELFIES 151개 제거\n",
      "validation  tasks_plus/task32_bace_molnet_valid.json  ->  /app/Mol-LLM/dataset/train/raw/BioT5_bace_valid.csv  (rows=0)\n",
      "[경고] 유효하지 않은 SELFIES 152개 제거\n",
      "test        tasks_plus/task33_bace_molnet_test.json  ->  /app/Mol-LLM/dataset/train/raw/BioT5_bace_test.csv  (rows=0)\n",
      "완료! 이제 다음 분기가 그대로 동작합니다:\n",
      "pd.read_csv(os.path.join('<raw_data_root>', 'raw', 'BioT5_bace_train.csv'))\n",
      "pd.read_csv(os.path.join('<raw_data_root>', 'raw', 'BioT5_bace_valid.csv'))\n",
      "pd.read_csv(os.path.join('<raw_data_root>', 'raw', 'BioT5_bace_test.csv'))\n"
     ]
    }
   ],
   "source": [
    "# pip install -U huggingface_hub pandas selfies\n",
    "\n",
    "from huggingface_hub import list_repo_files, hf_hub_download\n",
    "import pandas as pd\n",
    "import json, os, re\n",
    "\n",
    "# ===== 설정 =====\n",
    "REPO = \"QizhiPei/BioT5_finetune_dataset\"\n",
    "OUT_DIR = \"/app/Mol-LLM/dataset/train/raw\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ===== 레포 파일 나열 =====\n",
    "files = list_repo_files(REPO, repo_type=\"dataset\")\n",
    "\n",
    "# 우선순위: tasks_plus > tasks > splits_plus > splits\n",
    "PREFS = [\"tasks_plus/\", \"tasks/\", \"splits_plus/\", \"splits/\"]\n",
    "\n",
    "def pick_bace_file(split: str) -> str:\n",
    "    split_alias = [\"validation\", \"valid\"] if split == \"validation\" else [split]\n",
    "    best = None\n",
    "    best_key = (999, 10**9)\n",
    "    for f in files:\n",
    "        fl = f.lower()\n",
    "        if not fl.endswith(\".json\"):\n",
    "            continue\n",
    "        if \"bace\" not in fl:\n",
    "            continue\n",
    "        if not any(sa in fl for sa in split_alias):\n",
    "            continue\n",
    "        pref_idx = next((i for i, p in enumerate(PREFS) if p in f), 999)\n",
    "        key = (pref_idx, len(f))\n",
    "        if key < best_key:\n",
    "            best, best_key = f, key\n",
    "    if best is None:\n",
    "        raise FileNotFoundError(f\"BACE {split} JSON을 찾지 못했습니다. repo={REPO}\")\n",
    "    return best\n",
    "\n",
    "paths = {\n",
    "    \"train\"     : pick_bace_file(\"train\"),\n",
    "    \"validation\": pick_bace_file(\"validation\"),\n",
    "    \"test\"      : pick_bace_file(\"test\"),\n",
    "}\n",
    "\n",
    "# ===== 유틸 =====\n",
    "def load_json_flex(local_path: str):\n",
    "    \"\"\"무슨 형태든 list[dict](샘플 리스트)로 반환. Instances가 있으면 평탄화.\"\"\"\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    # 1) JSON Lines 시도\n",
    "    try:\n",
    "        lines = [ln for ln in raw.splitlines() if ln.strip()]\n",
    "        objs = [json.loads(ln) for ln in lines]\n",
    "        if objs and isinstance(objs[0], dict):\n",
    "            # 혹시 각 줄이 dataset-level dict(Instances 포함)인 경우도 있음 → 아래에서 다시 평탄화\n",
    "            data = objs\n",
    "        else:\n",
    "            data = json.loads(raw.decode(\"utf-8\"))\n",
    "    except Exception:\n",
    "        data = json.loads(raw.decode(\"utf-8\"))\n",
    "\n",
    "    # ---- 평탄화 로직 ----\n",
    "    def flatten_instances(x):\n",
    "        # x가 dict이면\n",
    "        if isinstance(x, dict):\n",
    "            if \"Instances\" in x and isinstance(x[\"Instances\"], list):\n",
    "                return x[\"Instances\"]\n",
    "            # dict 내부에서 list[dict] 탐색\n",
    "            for v in x.values():\n",
    "                if isinstance(v, list) and (not v or isinstance(v[0], dict)):\n",
    "                    return v\n",
    "            return [x]\n",
    "        # x가 list면\n",
    "        if isinstance(x, list):\n",
    "            # 리스트 요소가 dataset-level dict(=Instances를 가진)라면 전부 모아 평탄화\n",
    "            if x and isinstance(x[0], dict) and \"Instances\" in x[0]:\n",
    "                out = []\n",
    "                for d in x:\n",
    "                    if \"Instances\" in d and isinstance(d[\"Instances\"], list):\n",
    "                        out.extend(d[\"Instances\"])\n",
    "                return out\n",
    "            # 이미 list[dict](샘플)인 경우\n",
    "            return x\n",
    "        # 그 외 타입은 실패\n",
    "        raise ValueError(\"지원하지 않는 JSON 구조\")\n",
    "\n",
    "    return flatten_instances(data)\n",
    "\n",
    "# SELFIES만 뽑기\n",
    "def extract_selfies(raw: str) -> str | None:\n",
    "    if pd.isna(raw):\n",
    "        return None\n",
    "    s = str(raw)\n",
    "\n",
    "    m = re.search(r\"SELFIES:\\s*<bom>(.*)</eom>\", s, flags=re.DOTALL)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    m = re.search(r\"<bom>(.*)</eom>\", s, flags=re.DOTALL)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    for line in s.splitlines():\n",
    "        if \"SELFIES:\" in line:\n",
    "            line = line.split(\"SELFIES:\", 1)[1].strip()\n",
    "            line = re.sub(r\"^<bom>\\s*|\\s*</eom>$\", \"\", line).strip()\n",
    "            return line\n",
    "    return s.strip()\n",
    "\n",
    "def normalize_label(v) -> str:\n",
    "    if isinstance(v, str):\n",
    "        t = v.strip().lower()\n",
    "        if t in (\"true\",\"yes\",\"y\",\"1\"): return \"True\"\n",
    "        if t in (\"false\",\"no\",\"n\",\"0\"): return \"False\"\n",
    "    try:\n",
    "        return \"True\" if float(v) > 0 else \"False\"\n",
    "    except Exception:\n",
    "        return \"True\" if str(v).strip().lower() == \"true\" else \"False\"\n",
    "\n",
    "# 선택: SELFIES 유효성 검사\n",
    "try:\n",
    "    import selfies as sf\n",
    "    def is_valid_selfies(s: str) -> bool:\n",
    "        try:\n",
    "            _ = sf.decoder(s)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "except Exception:\n",
    "    def is_valid_selfies(s: str) -> bool:\n",
    "        return isinstance(s, str) and len(s) > 0\n",
    "\n",
    "def to_biot5_df(examples: list[dict]) -> pd.DataFrame:\n",
    "    \"\"\"Instances 항목 안의 input/output(대소문자 가리지 않음) → SELFIES/label로 정규화\"\"\"\n",
    "    rows = []\n",
    "    for ex in examples:\n",
    "        if not isinstance(ex, dict):\n",
    "            continue\n",
    "        # 입력/출력 키 탐색(대소문자 불문 + 다양한 별칭)\n",
    "        kl = {k.lower(): k for k in ex.keys()}\n",
    "        in_key = next((kl[k] for k in (\"input\",\"raw_input\",\"selfies\",\"x\",\"mol\") if k in kl), None)\n",
    "        out_key = next((kl[k] for k in (\"output\",\"label\",\"labels\",\"y\",\"target\",\"answer\") if k in kl), None)\n",
    "\n",
    "        # Natural-Instructions 스타일로 'input'/'output'이 또 내부 dict에 들어가 있는 경우 처리\n",
    "        cand = ex.get(in_key) if in_key else None\n",
    "        if isinstance(cand, dict):\n",
    "            # 흔한 후보\n",
    "            in_key2 = next((k for k in cand.keys() if k.lower() in (\"input\",\"raw_input\",\"selfies\",\"x\",\"mol\",\"text\")), None)\n",
    "            input_val = cand.get(in_key2)\n",
    "        else:\n",
    "            input_val = cand\n",
    "\n",
    "        cand = ex.get(out_key) if out_key else None\n",
    "        if isinstance(cand, dict):\n",
    "            out_key2 = next((k for k in cand.keys() if k.lower() in (\"output\",\"label\",\"labels\",\"y\",\"target\",\"answer\",\"text\")), None)\n",
    "            output_val = cand.get(out_key2)\n",
    "        else:\n",
    "            output_val = cand\n",
    "\n",
    "        if input_val is None or output_val is None:\n",
    "            # 못 찾았으면 스킵\n",
    "            continue\n",
    "\n",
    "        # output이 리스트/딕트일 수 있음 → 문자열로 축약\n",
    "        if isinstance(output_val, list):\n",
    "            output_val = output_val[0] if output_val else \"\"\n",
    "        if isinstance(output_val, dict):\n",
    "            output_val = next((v for v in output_val.values() if isinstance(v, (str,int,float,bool))), str(output_val))\n",
    "\n",
    "        rows.append({\"SELFIES\": extract_selfies(str(input_val)), \"label\": normalize_label(output_val)})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    # 무효 SELFIES 제거\n",
    "    mask = df[\"SELFIES\"].notna() & df[\"SELFIES\"].map(is_valid_selfies)\n",
    "    if (~mask).sum():\n",
    "        print(f\"[경고] 유효하지 않은 SELFIES {(~mask).sum()}개 제거\")\n",
    "    df = df[mask].reset_index(drop=True)\n",
    "    return df[[\"SELFIES\",\"label\"]]\n",
    "\n",
    "# ===== 다운로드 → 파싱 → CSV 저장 =====\n",
    "for split, rel in paths.items():\n",
    "    local_path = hf_hub_download(REPO, rel, repo_type=\"dataset\")\n",
    "    ex = load_json_flex(local_path)\n",
    "    df = to_biot5_df(ex)\n",
    "    fname = f\"BioT5_bace_{'valid' if split=='validation' else split}.csv\"\n",
    "    out_path = os.path.join(OUT_DIR, fname)\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"{split:11s} {rel}  ->  {out_path}  (rows={len(df)})\")\n",
    "\n",
    "print(\"완료! 이제 다음 분기가 그대로 동작합니다:\")\n",
    "print(f\"pd.read_csv(os.path.join('<raw_data_root>', 'raw', 'BioT5_bace_train.csv'))\")\n",
    "print(f\"pd.read_csv(os.path.join('<raw_data_root>', 'raw', 'BioT5_bace_valid.csv'))\")\n",
    "print(f\"pd.read_csv(os.path.join('<raw_data_root>', 'raw', 'BioT5_bace_test.csv'))\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14f5b55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] /app/Mol-LLM/dataset/train/raw/BioT5_bace_train.csv (rows=1210)\n",
      "[saved] /app/Mol-LLM/dataset/train/raw/BioT5_bace_valid.csv (rows=151)\n",
      "[saved] /app/Mol-LLM/dataset/train/raw/BioT5_bace_test.csv (rows=152)\n",
      "\n",
      "[train] preview\n",
      "                                             SELFIES  label\n",
      "0  [C][N][C][=Branch1][C][=O][C@@][Branch1][P][C]...   True\n",
      "1  [C][N][C][=Branch1][C][=O][C@@][Branch1][P][C]...   True\n",
      "2  [C][N][C][=Branch1][C][=O][C@@][Branch1][P][C]...   True\n",
      "\n",
      "[valid] preview\n",
      "                                             SELFIES  label\n",
      "0  [C][C][Branch1][C][C][Branch1][C][C][C][=C][C]...  False\n",
      "1  [C][C][=Branch1][C][=O][N][C][Branch1][S][C][C...  False\n",
      "2  [C][C][Branch2][Ring1][O][C][=C][C][=C][C][Bra...  False\n",
      "\n",
      "[test] preview\n",
      "                                             SELFIES  label\n",
      "0  [C][C][=C][C][=C][C][=C][Ring1][=Branch1][C][=...   True\n",
      "1  [C][C][Branch1][C][C][Branch1][C][C][C][C][=C]...   True\n",
      "2  [C][O][C][C][=Branch1][C][=O][N][C][Branch2][R...   True\n"
     ]
    }
   ],
   "source": [
    "# 필요 패키지 (이미 있으면 생략 OK)\n",
    "# pip install -U deepchem selfies rdkit-pypi pandas\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "import selfies as sf\n",
    "import deepchem as dc\n",
    "\n",
    "RAW_DATA_ROOT = \"/app/Mol-LLM/dataset/train\"   # <- 네 config의 raw_data_root\n",
    "RAW_DIR = os.path.join(RAW_DATA_ROOT, \"raw\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "def get_bace_loader():\n",
    "    \"\"\"\n",
    "    DeepChem 버전별로 서로 다른 이름/경로를 순차 탐색해서\n",
    "    BACE 로더 함수(호출 가능한 함수)를 반환.\n",
    "    \"\"\"\n",
    "    # 1) 가장 직관적 이름\n",
    "    if hasattr(dc.molnet, \"load_bace\"):\n",
    "        return dc.molnet.load_bace\n",
    "    # 2) 과거/다른 이름 추정치\n",
    "    for name in (\"load_bace_classification\", \"load_bace_clf\"):\n",
    "        if hasattr(dc.molnet, name):\n",
    "            return getattr(dc.molnet, name)\n",
    "    # 3) 범용 로더 팩토리\n",
    "    try:\n",
    "        from deepchem.molnet import load_function\n",
    "        fn = load_function(\"bace\")\n",
    "        if callable(fn):\n",
    "            return fn\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 4) 실패 시 에러\n",
    "    raise RuntimeError(\n",
    "        \"이 DeepChem 버전에서 BACE 로더를 찾지 못했습니다. \"\n",
    "        \"가능하면 deepchem>=2.6 를 권장합니다.\"\n",
    "    )\n",
    "\n",
    "def to_bool_str(y):\n",
    "    # 1/True → \"True\", 0/False → \"False\"\n",
    "    try:\n",
    "        v = float(y)\n",
    "        return \"True\" if v > 0.0 else \"False\"\n",
    "    except Exception:\n",
    "        s = str(y).strip().lower()\n",
    "        if s in (\"1\",\"true\",\"yes\"): return \"True\"\n",
    "        return \"False\"\n",
    "\n",
    "def dc_dataset_to_csv(dc_dataset, out_csv_path):\n",
    "    rows = []\n",
    "    ys = np.array(dc_dataset.y)\n",
    "    ys = ys.reshape(-1)  # (N,1) → (N,)\n",
    "    for mol, y in zip(dc_dataset.X, ys):\n",
    "        if mol is None:\n",
    "            continue\n",
    "        smi = Chem.MolToSmiles(mol)\n",
    "        if not smi:\n",
    "            continue\n",
    "        try:\n",
    "            selfies_str = sf.encoder(smi)\n",
    "        except Exception:\n",
    "            continue\n",
    "        rows.append({\"SELFIES\": selfies_str, \"label\": to_bool_str(y)})\n",
    "    df = pd.DataFrame(rows, columns=[\"SELFIES\", \"label\"])\n",
    "    df.to_csv(out_csv_path, index=False)\n",
    "    print(f\"[saved] {out_csv_path} (rows={len(df)})\")\n",
    "\n",
    "# ── 1) DeepChem에서 BACE 로더 확보 ───────────────────────────────────────────\n",
    "loader = get_bace_loader()\n",
    "\n",
    "# ── 2) MoleculeNet BACE 불러오기 (Raw + scaffold split) ─────────────────────\n",
    "#    DeepChem 버전에 따라 파라미터 지원이 조금 다를 수 있어 kwargs로 안전 호출\n",
    "kwargs = dict(featurizer=\"Raw\", splitter=\"scaffold\", reload=True)\n",
    "try:\n",
    "    tasks, datasets, transformers = loader(**kwargs)\n",
    "except TypeError:\n",
    "    # 어떤 버전에선 data_dir/save_dir 필요하거나 reload 미지원일 수 있음 → 최소 인자 재시도\n",
    "    tasks, datasets, transformers = loader(featurizer=\"Raw\", splitter=\"scaffold\")\n",
    "\n",
    "# datasets 순서는 보통 (train, valid, test)\n",
    "train_dc, valid_dc, test_dc = datasets\n",
    "\n",
    "# ── 3) CSV 저장 ─────────────────────────────────────────────────────────────\n",
    "dc_dataset_to_csv(train_dc, os.path.join(RAW_DIR, \"BioT5_bace_train.csv\"))\n",
    "dc_dataset_to_csv(valid_dc, os.path.join(RAW_DIR, \"BioT5_bace_valid.csv\"))\n",
    "dc_dataset_to_csv(test_dc,  os.path.join(RAW_DIR, \"BioT5_bace_test.csv\"))\n",
    "\n",
    "# ── 4) 프리뷰 ────────────────────────────────────────────────────────────────\n",
    "for name in [\"train\",\"valid\",\"test\"]:\n",
    "    p = os.path.join(RAW_DIR, f\"BioT5_bace_{name}.csv\")\n",
    "    if os.path.exists(p):\n",
    "        print(f\"\\n[{name}] preview\")\n",
    "        print(pd.read_csv(p).head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a66c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
