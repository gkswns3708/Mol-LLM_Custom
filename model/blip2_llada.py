import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer, logging
from model.blip2_opt import Blip2OPT, split_batch_by_components
from torch_geometric.data import Batch
from torch.nn.utils.rnn import pad_sequence
import model.added_tokens as added_tokens
import numpy as np
import json
import os
from datetime import datetime, timezone, timedelta

# í•œêµ­ ì‹œê°„ëŒ€ (KST = UTC+9)
KST = timezone(timedelta(hours=9))
# [ì¤‘ìš”] Python ê¸°ë³¸ logging ëŒ€ì‹  transformersì˜ loggingì„ ì‚¬ìš©
logger = logging.get_logger(__name__)

# LLaDA MASK Token ID
MASK_TOKEN_ID = 126336 # <|mdm_mask|> id

class Blip2LLaDA(Blip2OPT):
    def __init__(self, args):
        super().__init__(args=args)
        self.mask_token_id = MASK_TOKEN_ID

        # [ìˆ˜ì •] LLaDAëŠ” ê¸°ì¡´ Attention Logging ë¡œì§ê³¼ í˜¸í™˜ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ,
        # Stage 3 í‰ê°€ ì‹œ ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ê°•ì œë¡œ Falseë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        if hasattr(self.args, 'log_attn_score'):
            self.args.log_attn_score = False
            logger.info("LLaDA model detected: Disabled 'log_attn_score' to avoid incompatibility.")

        # ==================== Debug Logging Configuration ====================
        # configì—ì„œ ë¡œê¹… ì„¤ì • ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        self._log_embedding_status = getattr(args, 'log_embedding_status', False)
        self._embedding_log_interval = getattr(args, 'embedding_log_interval', 500)
        self._log_model_init_details = getattr(args, 'log_model_init_details', False)
        self._log_nan_details = getattr(args, 'log_nan_details', True)
        self._nan_log_dir = getattr(args, 'nan_log_dir', './nan_logs')

        # Special token embedding ë¡œê¹…ì„ ìœ„í•œ ì¹´ìš´í„°
        self._log_step_counter = 0
        self._initial_embedding_norms = None  # ì´ˆê¸° embedding norm ì €ì¥ìš©

        # ìƒˆ í† í° ë””ë²„ê¹… ë¡œê¹… ì„¤ì •
        self._log_new_token_debug = getattr(args, 'log_new_token_debug', False)
        self._new_token_debug_interval = getattr(args, 'new_token_debug_interval', 100)
        # =====================================================================

    def get_lora_target_modules(self):
        return ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

    def set_llm_model(self, llm_model):
        self.llm_model = AutoModelForCausalLM.from_pretrained(
            llm_model, 
            trust_remote_code=True, 
            torch_dtype=torch.bfloat16
        )
        self.llm_tokenizer = AutoTokenizer.from_pretrained(
            llm_model, 
            trust_remote_code=True
        )
        
        if self.llm_tokenizer.pad_token is None:
            self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token
        
        self.llm_tokenizer.padding_side = 'left' 

        # Embedding Layer ì°¸ì¡° (ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°)
        try:
            self.llm_embed_tokens = self.llm_model.get_input_embeddings()
        except AttributeError:
            if hasattr(self.llm_model, 'embed_tokens'):
                self.llm_embed_tokens = self.llm_model.embed_tokens
            elif hasattr(self.llm_model, 'model') and hasattr(self.llm_model.model, 'embed_tokens'):
                self.llm_embed_tokens = self.llm_model.model.embed_tokens
            else:
                raise AttributeError("Cannot find embedding layer.")
        
        self.check_and_add_special_tokens()
        
    def check_and_add_special_tokens(self):
        # 1. ê¸°ë³¸ íŠ¹ìˆ˜ í† í° ë¦¬ìŠ¤íŠ¸ ì·¨í•©
        special_tokens_list = (
            added_tokens.BOOL + 
            added_tokens.FLOAT + 
            added_tokens.DESCRIPTION + 
            added_tokens.SELFIES +
            added_tokens.MOL_2D + 
            added_tokens.MOL_3D + 
            added_tokens.MOL_EMBEDDING +
            added_tokens.NUMBER + 
            added_tokens.INSTRUCTION + 
            added_tokens.REACTION_DIRECTION +
            added_tokens.IUPAC + 
            added_tokens.MOLFORMULA
        )

        # 2. SELFIES Dictionary íŒŒì¼ì—ì„œ í† í° ì½ì–´ì˜¤ê¸°
        if getattr(self.args, "add_selfies_tokens", False):
            if hasattr(self.args, "selfies_token_path") and self.args.selfies_token_path:
                try:
                    with open(self.args.selfies_token_path, "r") as f:
                        selfies_tokens = f.readlines()
                        selfies_tokens = [token.strip() for token in selfies_tokens]
                    
                    special_tokens_list.extend(selfies_tokens)
                    # configì—ì„œ ë¡œê¹… ì„¤ì • ê°€ì ¸ì˜¤ê¸° (ì´ˆê¸°í™” ìˆœì„œ ë•Œë¬¸ì— getattr ì‚¬ìš©)
                    if getattr(self.args, 'log_model_init_details', False):
                        logger.info(f"[Token Check] Loaded {len(selfies_tokens)} SELFIES tokens from {self.args.selfies_token_path}")
                    self.llm_tokenizer.added_selfies_tokens = selfies_tokens

                except Exception as e:
                    logger.error(f"[Token Check] Failed to load SELFIES tokens from file: {e}")

        # configì—ì„œ ë¡œê¹… ì„¤ì • ê°€ì ¸ì˜¤ê¸° (ì´ˆê¸°í™” ìˆœì„œ ë•Œë¬¸ì— getattr ì‚¬ìš©)
        _log_details = getattr(self.args, 'log_model_init_details', False)

        # 3. í† í¬ë‚˜ì´ì €ì— ëª¨ë“  í† í° ì¶”ê°€
        num_added_toks = self.llm_tokenizer.add_tokens(special_tokens_list)
        if _log_details:
            logger.info(f"[Token Check] Added {num_added_toks} special tokens to tokenizer.")

        # 4. ëª¨ë¸ ì„ë² ë”© í¬ê¸° ì¡°ì • (Input Embedding)
        new_vocab_size = len(self.llm_tokenizer)
        if _log_details:
            logger.info(f"[DEBUG] Resizing Input Embeddings to {new_vocab_size}")
        self.llm_model.resize_token_embeddings(new_vocab_size)

        # ==============================================================================
        # [ì¤‘ìš”] 5. ì¶œë ¥ ë ˆì´ì–´(LM Head) ê°•ì œ ë¦¬ì‚¬ì´ì§• (ì´ ë¶€ë¶„ì´ í•µì‹¬ í•´ê²°ì±…ì…ë‹ˆë‹¤)
        # ==============================================================================
        output_embeddings = self.llm_model.get_output_embeddings()

        if output_embeddings is not None and output_embeddings.weight.shape[0] != new_vocab_size:
            if _log_details:
                logger.info(f"[DEBUG] Output embedding size mismatch! Input: {new_vocab_size}, Output: {output_embeddings.weight.shape[0]}")
                logger.info("[DEBUG] Forcing resize of output embeddings (lm_head)...")

            # ìƒˆë¡œìš´ ì¶œë ¥ í—¤ë“œ ìƒì„± (ê¸°ì¡´ ê°€ì¤‘ì¹˜ ë³µì‚¬)
            new_lm_head = nn.Linear(
                output_embeddings.in_features,
                new_vocab_size,
                bias=output_embeddings.bias is not None
            ).to(self.llm_model.device).to(output_embeddings.weight.dtype)

            # ê¸°ì¡´ ê°€ì¤‘ì¹˜ ë³µì‚¬
            n_orig = output_embeddings.weight.shape[0]
            with torch.no_grad():
                new_lm_head.weight[:n_orig, :] = output_embeddings.weight
                if output_embeddings.bias is not None:
                    new_lm_head.bias[:n_orig] = output_embeddings.bias

            # ëª¨ë¸ì— ìƒˆë¡œìš´ í—¤ë“œ ì„¤ì •
            self.llm_model.set_output_embeddings(new_lm_head)

            # NOTE: requires_grad ì„¤ì •ì€ PEFT ì ìš© í›„ì— blip2_opt.pyì—ì„œ ì²˜ë¦¬ë¨
            # ì—¬ê¸°ì„œ ì„¤ì •í•´ë„ get_peft_model() í˜¸ì¶œ ì‹œ ë˜í•‘ë˜ë©´ì„œ ë¬´ì‹œë  ìˆ˜ ìˆìŒ
            if _log_details:
                logger.info(f"[DEBUG] Output embedding force resize complete. New shape: {new_lm_head.weight.shape}")
                logger.info(f"[DEBUG] Set lm_head.weight.requires_grad = True")
        else:
            if _log_details:
                logger.info(f"[DEBUG] Output embedding size is correct: {output_embeddings.weight.shape[0]}")
        # ==============================================================================

        # 6. <mol> í† í° ID ì €ì¥
        self.mol_token_id = self.llm_tokenizer.convert_tokens_to_ids(added_tokens.MOL_EMBEDDING)[0]

        # 7. SELFIES Token ID ì €ì¥
        if getattr(self.args, "add_selfies_tokens", False) and hasattr(self.llm_tokenizer, "added_selfies_tokens"):
            self.llm_tokenizer.selfies_token_ids = [
                self.llm_tokenizer.convert_tokens_to_ids(token)
                for token in self.llm_tokenizer.added_selfies_tokens
            ]
    
    # [Stage 1 ì—ëŸ¬ ìˆ˜ì •] ê·¸ë˜í”„ê°€ Noneì¼ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€ëœ ë²„ì „
    def inject_graph_embeds2input_embeds(self, input_embeds, is_mol_token, graphs):
        # graphs íŠœí”Œ ì–¸íŒ¨í‚¹ (Main Graph, Additional Graph)
        mol_graphs, mol2_graphs = graphs

        mol_token_sequence = []

        for graphs in [mol_graphs, mol2_graphs]:
            # ê·¸ë˜í”„ê°€ Noneì´ë©´ ê±´ë„ˆë›°ê¸°
            if graphs is None:
                continue

            mol_x = graphs["x"]
            mol_edge_index = graphs["edge_index"]
            mol_edge_attr = graphs["edge_attr"]
            mol_batch = graphs["batch"]

            if self.args.process_disjoint:
                num_graph_list = []
                graph_list = []
                for graph in graphs.to_data_list():
                    tmp_batch = Batch.from_data_list([graph])
                    tmp_batch = split_batch_by_components(tmp_batch)
                    graph_list.extend(tmp_batch)
                    num_graph_list.append(len(tmp_batch))
                
                graph_batch = Batch.from_data_list(graph_list)
                graph_embeds, graph_masks = self.graph_encoder(
                    graph_batch.x,
                    graph_batch.edge_index,
                    graph_batch.edge_attr,
                    graph_batch.batch,
                )
                mol_embeds_list = []
                mol_mask_list = []

                graph_embeds = torch.split(graph_embeds, num_graph_list, dim=0)
                graph_masks = torch.split(graph_masks, num_graph_list, dim=0)
                for graph_embed, graph_mask in zip(graph_embeds, graph_masks):
                    mol_embeds_list.append(graph_embed[graph_mask])
                    mol_mask_list.append(graph_mask[graph_mask])
                mol_embeds = pad_sequence(mol_embeds_list, batch_first=True)
                mol_masks = pad_sequence(mol_mask_list, batch_first=True)
            else:
                mol_embeds, mol_masks = self.graph_encoder(
                    mol_x, mol_edge_index, mol_edge_attr, mol_batch
                )

            if not self.tune_gnn:
                mol_embeds = mol_embeds.detach()
            mol_embeds = self.ln_graph(mol_embeds, mol_masks)

            if self.args.projector_type == "qformer":
                query_tokens = self.query_tokens.expand(mol_embeds.shape[0], -1, -1)
                query_output = self.Qformer.bert(
                    query_embeds=query_tokens,
                    encoder_hidden_states=mol_embeds,
                    encoder_attention_mask=mol_masks,
                    return_dict=True,
                )
                mol_tokens = self.opt_proj(query_output.last_hidden_state)
            else:
                mol_tokens = self.opt_proj(mol_embeds)
            mol_token_sequence.append(mol_tokens)

        # [í•µì‹¬] ê·¸ë˜í”„ í† í°ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° ì²˜ë¦¬
        if len(mol_token_sequence) == 0:
            batch_size = input_embeds.shape[0]
            device = input_embeds.device
            dummy_norm = torch.zeros(batch_size, device=device)
            return input_embeds, dummy_norm, dummy_norm

        # ê²°ê³¼ ì—°ê²°
        mol_tokens = torch.cat(mol_token_sequence, dim=1)
        moltoken_avg_norm = torch.norm(mol_tokens, p=1, dim=-1).mean(1)
        
        graph_avg_norm = torch.zeros(input_embeds.shape[0], device=input_embeds.device)
        
        num_mol_tokens_per_sample = is_mol_token.sum(dim=1)
        if (num_mol_tokens_per_sample > 0).any():
            mol_token_indices_full = (is_mol_token.cumsum(dim=1) - 1)
            batch_indices, token_indices = is_mol_token.nonzero(as_tuple=True)
            mol_token_indices = mol_token_indices_full[batch_indices, token_indices]
            
            input_embeds[batch_indices, token_indices, :] = mol_tokens[
                batch_indices, mol_token_indices, :
            ]

        return input_embeds, graph_avg_norm, moltoken_avg_norm

    def forward(self, samples):
        input_ids = samples['input_ids']
        attention_mask = samples['attention_mask']
        labels = samples['labels']

        batch_size, seq_len = input_ids.shape

        # ========================================================================
        # LLaDA Forward Process (SMDM ì›ë³¸ êµ¬í˜„ ì°¸ì¡°)
        #
        # í•µì‹¬: response ì˜ì—­ì—ì„œë§Œ ë§ˆìŠ¤í‚¹í•˜ê³ , ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì˜ ì›ë³¸ í† í°ì„ ì˜ˆì¸¡
        #
        # 1. labels != -100 ì¸ ìœ„ì¹˜ê°€ response ì˜ì—­ (is_answer)
        #    - DataLoaderì—ì„œ target text ëì— EOS í† í°ì´ ì¶”ê°€ë¨ (data_utils.py)
        #    - ë”°ë¼ì„œ EOS í† í°ë„ labelsì— í¬í•¨ë˜ì–´ response ì˜ì—­ì— í¬í•¨ë¨
        #    - LLaDA ë…¼ë¬¸ Section 2.3: "We treat |EOS| as a normal token during training"
        # 2. response ì˜ì—­ ë‚´ì—ì„œ ëœë¤í•˜ê²Œ ë§ˆìŠ¤í‚¹ (masked_indices)
        # 3. ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì—ì„œë§Œ cross-entropy loss ê³„ì‚°
        # 4. 1/p_maskë¡œ importance weighting
        # ========================================================================

        eps = 1e-6
        t = torch.rand(batch_size, device=self.device)
        p_mask = (1 - eps) * t + eps
        p_mask = p_mask[:, None].repeat(1, seq_len)

        # is_answer: response ì˜ì—­ ë§ˆìŠ¤í¬ (labels != -100ì¸ ìœ„ì¹˜)
        is_answer = (labels != -100)

        mask_prob = torch.rand((batch_size, seq_len), device=self.device)
        # ì˜¤ì§ response ì˜ì—­(is_answer)ì—ì„œë§Œ ë§ˆìŠ¤í‚¹
        masked_indices = (mask_prob < p_mask) & is_answer

        # ì›ë³¸ í† í° ì €ì¥ (loss ê³„ì‚°ìš©)
        original_tokens = input_ids.clone()

        # Noisy input ìƒì„±: ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ë¥¼ mask_token_idë¡œ ëŒ€ì²´
        noisy_input_ids = input_ids.clone()
        noisy_input_ids[masked_indices] = self.mask_token_id

        noisy_text_embeds = self.llm_embed_tokens(noisy_input_ids)
        inputs_embeds = noisy_text_embeds.clone()

        graph_avg_norm = torch.zeros(batch_size, device=self.device)
        moltoken_avg_norm = torch.zeros(batch_size, device=self.device)

        if "graphs" in samples:
             inputs_embeds, graph_avg_norm, moltoken_avg_norm = self.inject_graph_embeds2input_embeds(
                input_embeds=inputs_embeds,
                is_mol_token=samples['is_mol_token'],
                graphs=(samples['graphs'], samples['additional_graphs'])
            )

        outputs = self.llm_model(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            return_dict=True
        )
        logits = outputs.logits

        loss_fct = nn.CrossEntropyLoss(reduction='none')
        token_loss = None  # NaN ë¡œê¹…ì„ ìœ„í•´ ë¯¸ë¦¬ ì´ˆê¸°í™”

        if masked_indices.sum() == 0:
            loss = torch.tensor(0.0, device=self.device, requires_grad=True)
            instance_loss = torch.zeros(batch_size, device=self.device)
        else:
            # ================================================================
            # [í•µì‹¬ ìˆ˜ì •] SMDM ì›ë³¸ ë°©ì‹ìœ¼ë¡œ loss ê³„ì‚°
            #
            # ì›ë³¸: loss = CE(logits[mask], target[mask]) / p_mask[mask]
            #       loss = loss.sum() / (batch_size * seq_len)
            #
            # ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì—ì„œë§Œ lossë¥¼ ê³„ì‚°í•˜ê³ , 1/p_maskë¡œ weighting
            # ================================================================

            # logits: [batch, seq_len, vocab_size]
            # masked_indices: [batch, seq_len] boolean

            # ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì˜ logitsì™€ targets ì¶”ì¶œ
            masked_logits = logits[masked_indices]  # [num_masked, vocab_size]
            masked_targets = original_tokens[masked_indices]  # [num_masked]
            masked_p = p_mask[masked_indices]  # [num_masked]

            # Cross-entropy loss (ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ë§Œ)
            token_loss = loss_fct(masked_logits, masked_targets)  # [num_masked]

            # Importance weighting: 1/p_mask
            weighted_loss = token_loss / masked_p  # [num_masked]

            # Normalization: response ê¸¸ì´ì˜ í•©ìœ¼ë¡œ ë‚˜ëˆ” (ì›ë³¸ì€ ì „ì²´ seq_lenì´ì§€ë§Œ,
            # conditional generationì´ë¯€ë¡œ response ê¸¸ì´ ì‚¬ìš©)
            answer_lengths = is_answer.sum(dim=1).float()  # [batch]
            total_answer_length = answer_lengths.sum()

            loss = weighted_loss.sum() / (total_answer_length + 1e-8)

            # Instance-level loss ê³„ì‚° (per-sample)
            # ê° ìƒ˜í”Œë³„ë¡œ ë§ˆìŠ¤í‚¹ëœ í† í°ì˜ weighted loss í•©ê³„
            instance_loss = torch.zeros(batch_size, device=self.device)
            batch_indices = torch.where(masked_indices)[0]  # ê° ë§ˆìŠ¤í‚¹ëœ í† í°ì´ ì–´ë–¤ ë°°ì¹˜ì— ì†í•˜ëŠ”ì§€
            instance_loss.scatter_add_(0, batch_indices, weighted_loss)
            instance_loss = instance_loss / (answer_lengths + 1e-8)
        if torch.isnan(loss) or torch.isinf(loss):
            # NaN ë¡œê¹… (configë¡œ ì œì–´)
            if self._log_nan_details:
                self._log_nan_samples(
                    samples=samples,
                    logits=logits,
                    input_ids=input_ids,
                    labels=labels,
                    masked_indices=masked_indices,
                    p_mask=p_mask,
                    token_loss=token_loss,
                    graph_avg_norm=graph_avg_norm,
                    loss_value=loss
                )

            # [ì„ì‹œ ì¡°ì¹˜] í•™ìŠµì´ í„°ì§€ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ Lossë¥¼ 0ìœ¼ë¡œ ê°•ì œ ë³€í™˜
            loss = torch.tensor(0.0, device=self.device, requires_grad=True)

        # ==============================================================================
        # [ë””ë²„ê¹…] Special Token Embedding ìƒíƒœ ë¡œê¹… (configë¡œ ì œì–´)
        # ==============================================================================
        if self.training and self._log_embedding_status and self._embedding_log_interval > 0:
            self._log_step_counter += 1
            if self._log_step_counter % self._embedding_log_interval == 0:
                self._log_special_token_embedding_status()

        # ==============================================================================
        # [ë””ë²„ê¹…] ìƒˆ í† í° Loss ê¸°ì—¬ë„ ë° Gradient ë¶„ì„
        # ==============================================================================
        new_token_debug_info = {}
        if self.training and self._log_new_token_debug:
            self._log_step_counter += 1
            if self._log_step_counter % self._new_token_debug_interval == 0:
                new_token_debug_info = self._analyze_new_token_contribution(
                    original_tokens=original_tokens,
                    masked_indices=masked_indices,
                    masked_targets=masked_targets if masked_indices.sum() > 0 else None,
                    token_loss=token_loss,
                    masked_logits=masked_logits if masked_indices.sum() > 0 else None,
                )

        return {
            "loss": loss,
            "instance_loss": instance_loss,
            "logits": logits,
            "graph_avg_norm": graph_avg_norm,
            "moltoken_avg_norm": moltoken_avg_norm,
            "new_token_debug": new_token_debug_info,
        }

    def forward_stepwise_teacher_forcing(self, samples, steps=32):
        """
        Step-wise Teacher Forcing ë°©ì‹ì˜ Generation Loss ê³„ì‚°

        LLaDAì˜ iterative denoising ê³¼ì • ì „ì²´ë¥¼ ì‹œë®¬ë ˆì´ì…˜:
        - ê° stepë³„ë¡œ í•´ë‹¹ ë§ˆìŠ¤í‚¹ ë¹„ìœ¨ì— ë§ëŠ” ì…ë ¥ì„ ìƒì„±
        - ì •ë‹µ í† í°ì„ Teacher Forcingìœ¼ë¡œ ì œê³µ (ì‹¤ì œ ìƒì„± ëŒ€ì‹ )
        - ê° stepì˜ lossì— importance weighting (1/p) ì ìš©
        - ëª¨ë“  stepì˜ weighted lossë¥¼ í‰ê· 

        ì´ëŠ” val_total_lossì™€ ìœ ì‚¬í•˜ì§€ë§Œ, ì „ì²´ denoising trajectoryë¥¼
        deterministicí•˜ê²Œ ì‹œë®¬ë ˆì´ì…˜í•œë‹¤ëŠ” ì ì´ ë‹¤ë¦„.

        Args:
            samples: ë°°ì¹˜ ë°ì´í„° (input_ids, attention_mask, labels, graphs ë“±)
            steps: denoising step ìˆ˜ (ê¸°ë³¸ê°’: 32)

        Returns:
            dict: {
                "loss": ì „ì²´ í‰ê·  loss,
                "instance_loss": ìƒ˜í”Œë³„ loss [batch_size]
            }
        """
        input_ids = samples['input_ids']
        attention_mask = samples['attention_mask']
        labels = samples['labels']

        batch_size, seq_len = input_ids.shape

        # response ì˜ì—­ (labels != -100ì¸ ìœ„ì¹˜)
        is_answer = (labels != -100)
        answer_lengths = is_answer.sum(dim=1).float()  # [batch_size]

        # ì›ë³¸ í† í° (ì •ë‹µ) - Teacher Forcingì— ì‚¬ìš©
        original_tokens = input_ids.clone()

        loss_fct = nn.CrossEntropyLoss(reduction='none')

        # Step-wise loss ëˆ„ì 
        instance_weighted_loss_sum = torch.zeros(batch_size, device=self.device)

        for step in range(steps):
            # í˜„ì¬ stepì˜ ë§ˆìŠ¤í‚¹ ë¹„ìœ¨ ê³„ì‚° (1.0 â†’ 0.0 ì„ í˜• ê°ì†Œ)
            # t = 1.0 - step / steps
            # ë§ˆìŠ¤í‚¹ ë¹„ìœ¨ p = t (step 0ì—ì„œ 100%, step N-1ì—ì„œ ~0%)
            t = 1.0 - step / steps
            p_mask = max(t, 1e-6)  # 0 ë°©ì§€

            # ê° ìƒ˜í”Œì˜ response ì˜ì—­ì—ì„œ p_mask ë¹„ìœ¨ë§Œí¼ ë§ˆìŠ¤í‚¹
            noisy_input_ids = input_ids.clone()

            # ìƒ˜í”Œë³„ë¡œ ë§ˆìŠ¤í‚¹ ì ìš©
            for b in range(batch_size):
                # í•´ë‹¹ ìƒ˜í”Œì˜ response ìœ„ì¹˜ë“¤
                answer_positions = torch.where(is_answer[b])[0]
                num_answer_tokens = len(answer_positions)

                if num_answer_tokens == 0:
                    continue

                # p_mask ë¹„ìœ¨ë§Œí¼ ë§ˆìŠ¤í‚¹í•  í† í° ìˆ˜
                num_to_mask = int(num_answer_tokens * p_mask)
                num_to_mask = max(1, num_to_mask)  # ìµœì†Œ 1ê°œëŠ” ë§ˆìŠ¤í‚¹

                # ëœë¤í•˜ê²Œ ë§ˆìŠ¤í‚¹í•  ìœ„ì¹˜ ì„ íƒ
                perm = torch.randperm(num_answer_tokens, device=self.device)
                mask_indices = answer_positions[perm[:num_to_mask]]

                # ë§ˆìŠ¤í‚¹ ì ìš©
                noisy_input_ids[b, mask_indices] = self.mask_token_id

            # Embedding ìƒì„±
            noisy_text_embeds = self.llm_embed_tokens(noisy_input_ids)
            inputs_embeds = noisy_text_embeds.clone()

            # Graph embedding ì£¼ì… (ìˆëŠ” ê²½ìš°)
            if "graphs" in samples:
                inputs_embeds, _, _ = self.inject_graph_embeds2input_embeds(
                    input_embeds=inputs_embeds,
                    is_mol_token=samples['is_mol_token'],
                    graphs=(samples['graphs'], samples['additional_graphs'])
                )

            # Forward pass
            outputs = self.llm_model(
                inputs_embeds=inputs_embeds,
                attention_mask=attention_mask,
                return_dict=True
            )
            logits = outputs.logits

            # ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì—ì„œë§Œ loss ê³„ì‚°
            masked_indices = (noisy_input_ids == self.mask_token_id) & is_answer

            if masked_indices.sum() == 0:
                continue

            # ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì˜ logitsì™€ targets
            masked_logits = logits[masked_indices]
            masked_targets = original_tokens[masked_indices]

            # CE Loss
            token_loss = loss_fct(masked_logits, masked_targets)

            # Importance weighting: 1/p_mask
            weighted_token_loss = token_loss / p_mask

            # ìƒ˜í”Œë³„ë¡œ weighted loss ëˆ„ì 
            batch_indices = torch.where(masked_indices)[0]
            for i, (b_idx, w_loss) in enumerate(zip(batch_indices, weighted_token_loss)):
                instance_weighted_loss_sum[b_idx] += w_loss

        # ê° ìƒ˜í”Œì˜ ìµœì¢… loss: weighted_loss_sum / (answer_length * steps)
        # stepsë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ : ê° í† í°ì´ ì—¬ëŸ¬ stepì—ì„œ ë§ˆìŠ¤í‚¹ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ
        instance_loss = instance_weighted_loss_sum / ((answer_lengths + 1e-8) * steps)

        # ì „ì²´ í‰ê·  loss
        loss = instance_loss.mean()

        return {
            "loss": loss,
            "instance_loss": instance_loss,
        }

    def _log_special_token_embedding_status(self):
        """
        Special token embedding ìƒíƒœ ë° gradient ë¡œê¹…
        - ìƒˆë¡œ ì¶”ê°€ëœ í† í°ë“¤ì˜ embeddingì´ í•™ìŠµë˜ê³  ìˆëŠ”ì§€ í™•ì¸
        - requires_grad ìƒíƒœ, embedding norm, gradient norm ì¶œë ¥
        """
        special_tokens = [
            "<BOOLEAN>", "</BOOLEAN>",
            "<DESCRIPTION>", "</DESCRIPTION>",
            "<SELFIES>", "</SELFIES>",
            "<FLOAT>", "</FLOAT>",
            "<mol>",
        ]

        try:
            embed_layer = self.llm_model.get_input_embeddings()
            output_layer = self.llm_model.get_output_embeddings()

            logger.info("=" * 70)
            logger.info(f"[Step {self._log_step_counter}] Special Token Embedding Status")
            logger.info("=" * 70)

            # Embedding layer ìƒíƒœ
            logger.info(f"Input Embedding (embed_tokens):")
            logger.info(f"  - Shape: {embed_layer.weight.shape}")
            logger.info(f"  - requires_grad: {embed_layer.weight.requires_grad}")
            logger.info(f"  - has gradient: {embed_layer.weight.grad is not None}")

            if output_layer is not None:
                logger.info(f"Output Layer (lm_head):")
                logger.info(f"  - Shape: {output_layer.weight.shape}")
                logger.info(f"  - requires_grad: {output_layer.weight.requires_grad}")
                logger.info(f"  - has gradient: {output_layer.weight.grad is not None}")

            logger.info("-" * 70)
            logger.info("Special Token Details:")

            token_data = []
            for token in special_tokens:
                token_id = self.llm_tokenizer.convert_tokens_to_ids(token)

                # í† í°ì´ ì œëŒ€ë¡œ ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if token_id is None or token_id == self.llm_tokenizer.unk_token_id:
                    logger.warning(f"  {token}: NOT FOUND in tokenizer (id={token_id})")
                    continue

                if token_id >= embed_layer.weight.shape[0]:
                    logger.warning(f"  {token}: id={token_id} OUT OF RANGE (vocab_size={embed_layer.weight.shape[0]})")
                    continue

                # Embedding norm
                embed_norm = embed_layer.weight[token_id].detach().norm().item()

                # Gradient norm (ìˆìœ¼ë©´)
                embed_grad_norm = "N/A"
                if embed_layer.weight.grad is not None:
                    embed_grad_norm = f"{embed_layer.weight.grad[token_id].norm().item():.6f}"

                # Output layer norm & grad
                output_norm = "N/A"
                output_grad_norm = "N/A"
                if output_layer is not None and token_id < output_layer.weight.shape[0]:
                    output_norm = f"{output_layer.weight[token_id].detach().norm().item():.4f}"
                    if output_layer.weight.grad is not None:
                        output_grad_norm = f"{output_layer.weight.grad[token_id].norm().item():.6f}"

                logger.info(f"  {token:15} (id={token_id:6}): "
                           f"embed_norm={embed_norm:.4f}, embed_grad={embed_grad_norm}, "
                           f"output_norm={output_norm}, output_grad={output_grad_norm}")

                token_data.append({
                    'token': token,
                    'id': token_id,
                    'embed_norm': embed_norm
                })

            # ì´ˆê¸° embedding norm ì €ì¥ (ì²« ë¡œê¹… ì‹œ)
            if self._initial_embedding_norms is None and token_data:
                self._initial_embedding_norms = {d['token']: d['embed_norm'] for d in token_data}
                logger.info("-" * 70)
                logger.info("Initial embedding norms saved for comparison.")
            elif self._initial_embedding_norms:
                # ë³€í™”ëŸ‰ ì¶œë ¥
                logger.info("-" * 70)
                logger.info("Embedding Norm Changes (from initial):")
                for d in token_data:
                    if d['token'] in self._initial_embedding_norms:
                        initial = self._initial_embedding_norms[d['token']]
                        current = d['embed_norm']
                        change = current - initial
                        pct_change = (change / initial * 100) if initial != 0 else 0
                        logger.info(f"  {d['token']:15}: {initial:.4f} -> {current:.4f} "
                                   f"(delta={change:+.4f}, {pct_change:+.2f}%)")

            # ìƒˆë¡œ ì¶”ê°€ëœ í† í° ì „ì²´ì˜ gradient í†µê³„
            if embed_layer.weight.grad is not None:
                # LLaDA ì›ë˜ vocab size (ëŒ€ëµì ì¸ ê°’, ì‹¤ì œë¡œëŠ” configì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
                orig_vocab_size = 128256
                if embed_layer.weight.shape[0] > orig_vocab_size:
                    new_token_grads = embed_layer.weight.grad[orig_vocab_size:]
                    grad_norms = new_token_grads.norm(dim=1)
                    logger.info("-" * 70)
                    logger.info(f"New Token Gradients (id >= {orig_vocab_size}):")
                    logger.info(f"  - Count: {new_token_grads.shape[0]}")
                    logger.info(f"  - Grad norm mean: {grad_norms.mean().item():.6f}")
                    logger.info(f"  - Grad norm max: {grad_norms.max().item():.6f}")
                    logger.info(f"  - Grad norm min: {grad_norms.min().item():.6f}")
                    logger.info(f"  - Non-zero grads: {(grad_norms > 1e-8).sum().item()}/{new_token_grads.shape[0]}")

            logger.info("=" * 70)

        except Exception as e:
            logger.error(f"Error logging special token status: {e}")

    def _log_nan_samples(
        self,
        samples,
        logits,
        input_ids,
        labels,
        masked_indices,
        p_mask,
        token_loss,
        graph_avg_norm,
        loss_value
    ):
        """
        NaN/Inf loss ë°œìƒ ì‹œ ìƒì„¸ ì •ë³´ë¥¼ íŒŒì¼ë¡œ ë¡œê¹…í•©ë‹ˆë‹¤.
        - ì–´ë–¤ ìƒ˜í”Œì—ì„œ ë°œìƒí–ˆëŠ”ì§€
        - ì˜ˆì¸¡ê°’ (argmax of logits)
        - ë¼ë²¨ê°’
        - ê°ì¢… í†µê³„ ì •ë³´
        """
        timestamp = datetime.now(KST).strftime("%Y%m%d_%H%M%S_%f")
        log_dir = getattr(self.args, 'nan_log_dir', './nan_logs')
        os.makedirs(log_dir, exist_ok=True)
        log_file = os.path.join(log_dir, f"nan_sample_{timestamp}.json")

        batch_size = input_ids.shape[0]

        # Task ì •ë³´
        task_info = samples.get("task", samples.get("dataset_name", ["Unknown"] * batch_size))
        if isinstance(task_info, str):
            task_info = [task_info] * batch_size

        # ì½˜ì†” ë¡œê·¸ ì¶œë ¥
        logger.error("\n" + "="*60)
        logger.error(f"ğŸš¨ [NaN/Inf DETECTED] Logging to: {log_file}")
        logger.error(f"Loss Value: {loss_value.item() if torch.is_tensor(loss_value) else loss_value}")
        logger.error(f"Batch Size: {batch_size}")
        logger.error(f"Tasks: {task_info}")

        # Logit í†µê³„
        logger.error(f"Logits Stats - Max: {logits.max().item():.6f}, Min: {logits.min().item():.6f}, Mean: {logits.mean().item():.6f}")
        logger.error(f"Logits has NaN: {torch.isnan(logits).any().item()}, has Inf: {torch.isinf(logits).any().item()}")

        # Token Loss í†µê³„
        if token_loss is not None:
            logger.error(f"Token Loss Stats - Max: {token_loss.max().item():.6f}, Min: {token_loss.min().item():.6f}")
            logger.error(f"Token Loss has NaN: {torch.isnan(token_loss).any().item()}, has Inf: {torch.isinf(token_loss).any().item()}")

        # p_mask í†µê³„
        logger.error(f"p_mask Stats - Max: {p_mask.max().item():.6f}, Min: {p_mask.min().item():.6f}")

        # Graph Norm
        logger.error(f"Graph Avg Norm: {graph_avg_norm.mean().item():.6f}")
        logger.error("="*60)

        # ìƒì„¸ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥
        nan_log_data = {
            "timestamp": timestamp,
            "loss_value": float(loss_value.item()) if torch.is_tensor(loss_value) and not (torch.isnan(loss_value) or torch.isinf(loss_value)) else str(loss_value.item() if torch.is_tensor(loss_value) else loss_value),
            "batch_size": batch_size,
            "logits_stats": {
                "max": float(logits.max().item()),
                "min": float(logits.min().item()),
                "mean": float(logits.mean().item()),
                "has_nan": bool(torch.isnan(logits).any().item()),
                "has_inf": bool(torch.isinf(logits).any().item()),
                "nan_count": int(torch.isnan(logits).sum().item()),
                "inf_count": int(torch.isinf(logits).sum().item()),
            },
            "p_mask_stats": {
                "max": float(p_mask.max().item()),
                "min": float(p_mask.min().item()),
                "mean": float(p_mask.mean().item()),
            },
            "graph_avg_norm": float(graph_avg_norm.mean().item()),
            "samples": []
        }

        if token_loss is not None:
            nan_log_data["token_loss_stats"] = {
                "max": float(token_loss.max().item()),
                "min": float(token_loss.min().item()),
                "has_nan": bool(torch.isnan(token_loss).any().item()),
                "has_inf": bool(torch.isinf(token_loss).any().item()),
            }

        # ê° ìƒ˜í”Œë³„ ìƒì„¸ ì •ë³´
        predictions = torch.argmax(logits, dim=-1)  # [batch_size, seq_len]

        for i in range(batch_size):
            sample_info = {
                "sample_idx": i,
                "task": task_info[i] if i < len(task_info) else "Unknown",
            }

            # ì…ë ¥ í…ìŠ¤íŠ¸ (prompt)
            if "prompt" in samples:
                prompt = samples["prompt"]
                sample_info["prompt"] = prompt[i] if isinstance(prompt, list) else prompt

            # íƒ€ê²Ÿ í…ìŠ¤íŠ¸ (ì •ë‹µ)
            if "target" in samples:
                target = samples["target"]
                sample_info["target_text"] = target[i] if isinstance(target, list) else target

            # ë¶„ì ì •ë³´
            if "smiles" in samples:
                smiles = samples["smiles"]
                sample_info["smiles"] = smiles[i] if isinstance(smiles, list) else smiles

            # ë¼ë²¨ í† í° (ë§ˆìŠ¤í‚¹ ë˜ì§€ ì•Šì€ ì˜ì—­, labels != -100)
            label_mask = labels[i] != -100
            label_tokens = labels[i][label_mask].cpu().tolist()
            sample_info["label_token_ids"] = label_tokens
            sample_info["label_text"] = self.llm_tokenizer.decode(label_tokens, skip_special_tokens=False)

            # ì˜ˆì¸¡ í† í° (ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì—ì„œì˜ ì˜ˆì¸¡)
            masked_positions = masked_indices[i]
            if masked_positions.any():
                pred_at_masked = predictions[i][masked_positions].cpu().tolist()
                label_at_masked = input_ids[i][masked_positions].cpu().tolist()
                sample_info["masked_positions_count"] = int(masked_positions.sum().item())
                sample_info["pred_token_ids_at_masked"] = pred_at_masked
                sample_info["label_token_ids_at_masked"] = label_at_masked
                sample_info["pred_text_at_masked"] = self.llm_tokenizer.decode(pred_at_masked, skip_special_tokens=False)
                sample_info["label_text_at_masked"] = self.llm_tokenizer.decode(label_at_masked, skip_special_tokens=False)

            # í•´ë‹¹ ìƒ˜í”Œì˜ logit í†µê³„
            sample_logits = logits[i]
            sample_info["sample_logits_stats"] = {
                "max": float(sample_logits.max().item()),
                "min": float(sample_logits.min().item()),
                "has_nan": bool(torch.isnan(sample_logits).any().item()),
                "has_inf": bool(torch.isinf(sample_logits).any().item()),
            }

            # í•´ë‹¹ ìƒ˜í”Œì˜ token_loss í†µê³„
            if token_loss is not None:
                sample_token_loss = token_loss[i]
                sample_info["sample_token_loss_stats"] = {
                    "max": float(sample_token_loss.max().item()),
                    "min": float(sample_token_loss.min().item()),
                    "has_nan": bool(torch.isnan(sample_token_loss).any().item()),
                    "has_inf": bool(torch.isinf(sample_token_loss).any().item()),
                }

                # NaN/Infê°€ ë°œìƒí•œ ìœ„ì¹˜ ì°¾ê¸°
                nan_positions = torch.where(torch.isnan(sample_token_loss))[0].cpu().tolist()
                inf_positions = torch.where(torch.isinf(sample_token_loss))[0].cpu().tolist()
                if nan_positions:
                    sample_info["nan_token_positions"] = nan_positions[:20]  # ìµœëŒ€ 20ê°œë§Œ
                    sample_info["nan_token_ids"] = input_ids[i][nan_positions[:20]].cpu().tolist()
                    sample_info["nan_tokens_text"] = self.llm_tokenizer.decode(input_ids[i][nan_positions[:20]].cpu().tolist())
                if inf_positions:
                    sample_info["inf_token_positions"] = inf_positions[:20]
                    sample_info["inf_token_ids"] = input_ids[i][inf_positions[:20]].cpu().tolist()
                    sample_info["inf_tokens_text"] = self.llm_tokenizer.decode(input_ids[i][inf_positions[:20]].cpu().tolist())

            nan_log_data["samples"].append(sample_info)

        # JSON íŒŒì¼ë¡œ ì €ì¥
        try:
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(nan_log_data, f, ensure_ascii=False, indent=2)
            logger.error(f"âœ… NaN log saved to: {log_file}")
        except Exception as e:
            logger.error(f"âŒ Failed to save NaN log: {e}")

    def _analyze_new_token_contribution(
        self,
        original_tokens,
        masked_indices,
        masked_targets,
        token_loss,
        masked_logits,
    ):
        """
        ìƒˆë¡œ ì¶”ê°€ëœ í† í°(SELFIES ë“±)ì˜ Loss ê¸°ì—¬ë„ ë° í•™ìŠµ ìƒíƒœ ë¶„ì„

        ë¶„ì„ í•­ëª©:
        1. ì „ì²´ í† í° ì¤‘ ìƒˆ í† í° ë¹„ìœ¨ (input, masked)
        2. ìƒˆ í† í° vs ê¸°ì¡´ í† í°ì˜ Loss ë¹„êµ
        3. ìƒˆ í† í°ì— ëŒ€í•œ ëª¨ë¸ ì˜ˆì¸¡ ì •í™•ë„
        4. Embedding gradient ë¶„ì„
        """
        debug_info = {}

        try:
            # ê¸°ì¡´ vocab size (LLaDA ê¸°ë³¸)
            orig_vocab_size = getattr(self.args, 'original_vocab_size', 128256)

            batch_size, seq_len = original_tokens.shape

            # ================================================================
            # 1. í† í° ë¹„ìœ¨ ë¶„ì„
            # ================================================================
            # ì „ì²´ inputì—ì„œ ìƒˆ í† í° ë¹„ìœ¨
            is_new_token_input = (original_tokens >= orig_vocab_size)
            total_tokens = original_tokens.numel()
            new_token_count_input = is_new_token_input.sum().item()
            new_token_ratio_input = new_token_count_input / total_tokens * 100

            debug_info['token_ratio/input_new_token_count'] = new_token_count_input
            debug_info['token_ratio/input_new_token_pct'] = new_token_ratio_input
            debug_info['token_ratio/input_total_tokens'] = total_tokens

            # Masked í† í° ì¤‘ ìƒˆ í† í° ë¹„ìœ¨
            if masked_targets is not None and len(masked_targets) > 0:
                is_new_token_masked = (masked_targets >= orig_vocab_size)
                masked_total = len(masked_targets)
                new_token_count_masked = is_new_token_masked.sum().item()
                new_token_ratio_masked = new_token_count_masked / masked_total * 100

                debug_info['token_ratio/masked_new_token_count'] = new_token_count_masked
                debug_info['token_ratio/masked_new_token_pct'] = new_token_ratio_masked
                debug_info['token_ratio/masked_total'] = masked_total

                # ================================================================
                # 2. Loss ë¶„ì„ (ìƒˆ í† í° vs ê¸°ì¡´ í† í°)
                # ================================================================
                if token_loss is not None and len(token_loss) > 0:
                    # ìƒˆ í† í°ì— ëŒ€í•œ loss
                    if is_new_token_masked.sum() > 0:
                        new_token_loss = token_loss[is_new_token_masked]
                        debug_info['loss/new_token_mean'] = new_token_loss.mean().item()
                        debug_info['loss/new_token_max'] = new_token_loss.max().item()
                        debug_info['loss/new_token_min'] = new_token_loss.min().item()
                        debug_info['loss/new_token_sum'] = new_token_loss.sum().item()
                    else:
                        debug_info['loss/new_token_mean'] = 0.0
                        debug_info['loss/new_token_sum'] = 0.0

                    # ê¸°ì¡´ í† í°ì— ëŒ€í•œ loss
                    is_orig_token_masked = ~is_new_token_masked
                    if is_orig_token_masked.sum() > 0:
                        orig_token_loss = token_loss[is_orig_token_masked]
                        debug_info['loss/orig_token_mean'] = orig_token_loss.mean().item()
                        debug_info['loss/orig_token_max'] = orig_token_loss.max().item()
                        debug_info['loss/orig_token_min'] = orig_token_loss.min().item()
                        debug_info['loss/orig_token_sum'] = orig_token_loss.sum().item()
                    else:
                        debug_info['loss/orig_token_mean'] = 0.0
                        debug_info['loss/orig_token_sum'] = 0.0

                    # Loss ê¸°ì—¬ë„ ë¹„ìœ¨
                    total_loss = token_loss.sum().item()
                    if total_loss > 0:
                        debug_info['loss/new_token_contribution_pct'] = debug_info.get('loss/new_token_sum', 0) / total_loss * 100
                        debug_info['loss/orig_token_contribution_pct'] = debug_info.get('loss/orig_token_sum', 0) / total_loss * 100

                # ================================================================
                # 3. ì˜ˆì¸¡ ì •í™•ë„ ë¶„ì„
                # ================================================================
                if masked_logits is not None and len(masked_logits) > 0:
                    predictions = masked_logits.argmax(dim=-1)

                    # ìƒˆ í† í°ì— ëŒ€í•œ ì •í™•ë„
                    if is_new_token_masked.sum() > 0:
                        new_token_preds = predictions[is_new_token_masked]
                        new_token_targets = masked_targets[is_new_token_masked]
                        new_token_correct = (new_token_preds == new_token_targets).float().mean().item()
                        debug_info['accuracy/new_token'] = new_token_correct * 100

                        # ìƒˆ í† í°ì´ ìƒˆ í† í°ìœ¼ë¡œ ì˜ˆì¸¡ë˜ì—ˆëŠ”ì§€ (vocab ë²”ìœ„ ì²´í¬)
                        pred_is_new = (new_token_preds >= orig_vocab_size).float().mean().item()
                        debug_info['accuracy/new_token_pred_in_new_vocab_pct'] = pred_is_new * 100
                    else:
                        debug_info['accuracy/new_token'] = 0.0

                    # ê¸°ì¡´ í† í°ì— ëŒ€í•œ ì •í™•ë„
                    if is_orig_token_masked.sum() > 0:
                        orig_token_preds = predictions[is_orig_token_masked]
                        orig_token_targets = masked_targets[is_orig_token_masked]
                        orig_token_correct = (orig_token_preds == orig_token_targets).float().mean().item()
                        debug_info['accuracy/orig_token'] = orig_token_correct * 100
                    else:
                        debug_info['accuracy/orig_token'] = 0.0

            # ================================================================
            # 4. Embedding Gradient ë¶„ì„
            # ================================================================
            embed_layer = self.llm_model.get_input_embeddings()
            output_layer = self.llm_model.get_output_embeddings()

            # Input Embedding gradient
            if embed_layer is not None:
                # ì‹¤ì œ weight ê°€ì ¸ì˜¤ê¸° (PEFT wrapper ì²˜ë¦¬)
                if hasattr(embed_layer, 'modules_to_save'):
                    # PEFT ModulesToSaveWrapperì¸ ê²½ìš°
                    actual_embed = embed_layer.modules_to_save.get('default', embed_layer)
                    if hasattr(actual_embed, 'weight'):
                        embed_weight = actual_embed.weight
                    else:
                        embed_weight = embed_layer.weight
                elif hasattr(embed_layer, 'weight'):
                    embed_weight = embed_layer.weight
                else:
                    embed_weight = None

                if embed_weight is not None and embed_weight.grad is not None:
                    vocab_size = embed_weight.shape[0]

                    if vocab_size > orig_vocab_size:
                        # ìƒˆ í† í° gradient
                        new_token_grad = embed_weight.grad[orig_vocab_size:]
                        new_grad_norms = new_token_grad.norm(dim=1)
                        debug_info['grad/embed_new_mean'] = new_grad_norms.mean().item()
                        debug_info['grad/embed_new_max'] = new_grad_norms.max().item()
                        debug_info['grad/embed_new_nonzero_count'] = (new_grad_norms > 1e-10).sum().item()
                        debug_info['grad/embed_new_nonzero_pct'] = (new_grad_norms > 1e-10).sum().item() / len(new_grad_norms) * 100

                        # ê¸°ì¡´ í† í° gradient (ë¹„êµìš©)
                        orig_token_grad = embed_weight.grad[:orig_vocab_size]
                        orig_grad_norms = orig_token_grad.norm(dim=1)
                        debug_info['grad/embed_orig_mean'] = orig_grad_norms.mean().item()
                        debug_info['grad/embed_orig_max'] = orig_grad_norms.max().item()

                        # Gradient ë¹„ìœ¨
                        if debug_info['grad/embed_orig_mean'] > 0:
                            debug_info['grad/embed_new_vs_orig_ratio'] = debug_info['grad/embed_new_mean'] / debug_info['grad/embed_orig_mean']

            # Output (LM Head) gradient
            if output_layer is not None:
                if hasattr(output_layer, 'modules_to_save'):
                    actual_output = output_layer.modules_to_save.get('default', output_layer)
                    if hasattr(actual_output, 'weight'):
                        output_weight = actual_output.weight
                    else:
                        output_weight = output_layer.weight
                elif hasattr(output_layer, 'weight'):
                    output_weight = output_layer.weight
                else:
                    output_weight = None

                if output_weight is not None and output_weight.grad is not None:
                    vocab_size = output_weight.shape[0]

                    if vocab_size > orig_vocab_size:
                        # ìƒˆ í† í° gradient
                        new_head_grad = output_weight.grad[orig_vocab_size:]
                        new_head_grad_norms = new_head_grad.norm(dim=1)
                        debug_info['grad/head_new_mean'] = new_head_grad_norms.mean().item()
                        debug_info['grad/head_new_max'] = new_head_grad_norms.max().item()
                        debug_info['grad/head_new_nonzero_count'] = (new_head_grad_norms > 1e-10).sum().item()

                        # ê¸°ì¡´ í† í° gradient
                        orig_head_grad = output_weight.grad[:orig_vocab_size]
                        orig_head_grad_norms = orig_head_grad.norm(dim=1)
                        debug_info['grad/head_orig_mean'] = orig_head_grad_norms.mean().item()

            # ================================================================
            # 5. ì½˜ì†” ë¡œê¹… (ìš”ì•½)
            # ================================================================
            logger.info("\n" + "=" * 70)
            logger.info(f"[Step {self._log_step_counter}] NEW TOKEN DEBUG ANALYSIS")
            logger.info("=" * 70)
            logger.info(f"[Token Ratio]")
            logger.info(f"  Input:  {new_token_count_input}/{total_tokens} ({new_token_ratio_input:.2f}%)")
            if 'token_ratio/masked_total' in debug_info:
                logger.info(f"  Masked: {debug_info.get('token_ratio/masked_new_token_count', 0)}/{debug_info['token_ratio/masked_total']} ({debug_info.get('token_ratio/masked_new_token_pct', 0):.2f}%)")

            logger.info(f"\n[Loss Analysis]")
            logger.info(f"  New Token Loss Mean:  {debug_info.get('loss/new_token_mean', 'N/A')}")
            logger.info(f"  Orig Token Loss Mean: {debug_info.get('loss/orig_token_mean', 'N/A')}")
            logger.info(f"  New Token Loss Contribution: {debug_info.get('loss/new_token_contribution_pct', 'N/A'):.2f}%")

            logger.info(f"\n[Prediction Accuracy]")
            logger.info(f"  New Token Accuracy:  {debug_info.get('accuracy/new_token', 'N/A'):.2f}%")
            logger.info(f"  Orig Token Accuracy: {debug_info.get('accuracy/orig_token', 'N/A'):.2f}%")

            logger.info(f"\n[Gradient Analysis]")
            logger.info(f"  Embed New Grad Mean:  {debug_info.get('grad/embed_new_mean', 'N/A')}")
            logger.info(f"  Embed Orig Grad Mean: {debug_info.get('grad/embed_orig_mean', 'N/A')}")
            logger.info(f"  Embed New/Orig Ratio: {debug_info.get('grad/embed_new_vs_orig_ratio', 'N/A')}")
            logger.info(f"  Embed New Nonzero:    {debug_info.get('grad/embed_new_nonzero_count', 'N/A')}/{vocab_size - orig_vocab_size if 'grad/embed_new_mean' in debug_info else 'N/A'}")
            logger.info("=" * 70 + "\n")

        except Exception as e:
            logger.error(f"Error in _analyze_new_token_contribution: {e}")
            import traceback
            logger.error(traceback.format_exc())

        return debug_info

    @staticmethod
    def add_gumbel_noise(logits, temperature):
        if temperature <= 1e-6:
            return logits
        logits = logits.to(torch.float64)
        noise = torch.rand_like(logits, dtype=torch.float64)
        gumbel_noise = (- torch.log(noise)) ** temperature
        return logits.exp() / gumbel_noise

    @staticmethod
    def get_num_transfer_tokens(mask_index, steps):
        mask_num = mask_index.sum(dim=1, keepdim=True)
        base = mask_num // steps
        remainder = mask_num % steps
        num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base
        for i in range(mask_num.size(0)):
            num_transfer_tokens[i, :remainder[i]] += 1
        return num_transfer_tokens

    @torch.no_grad()
    def generate(
        self,
        graphs,
        input_ids,
        attention_mask,
        is_mol_token=None,
        max_length=128,
        steps=64,
        temperature=0.0,
        remasking_strategy='low_confidence',
        use_semi_ar=False,
        semi_ar_block_size=32,
        task_name=None,
        **kwargs
    ):
        """
        LLaDA Generation with configurable remasking strategy

        ========================================================================
        LLaDA ë…¼ë¬¸ Algorithm 4 & 5 êµ¬í˜„ (Appendix A.3)
        ========================================================================

        remasking_strategy ì˜µì…˜:
        - 'low_confidence': Algorithm 5 - ë‚®ì€ confidence í† í°ì„ ë‹¤ì‹œ mask
        - 'random': Algorithm 4 - ëœë¤í•˜ê²Œ s/t ë¹„ìœ¨ë§Œí¼ ë‹¤ì‹œ mask
        - 'none': Remasking ì—†ì´ ë§¤ stepë§ˆë‹¤ top-k í† í°ë§Œ unmask (ê¸°ì¡´ ë°©ì‹)

        use_semi_ar ì˜µì…˜:
        - True: Semi-Autoregressive ëª¨ë“œ (ë¸”ë¡ ë‹¨ìœ„ë¡œ ìˆœì°¨ ìƒì„±)
        - False: ì „ì²´ ì˜ì—­ ë™ì‹œ ìƒì„±

        Args:
            graphs: ë¶„ì ê·¸ë˜í”„ (tuple of main_graph, additional_graph)
            input_ids: ì…ë ¥ í† í° ID [batch, prompt_len]
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬ [batch, prompt_len]
            is_mol_token: mol token ìœ„ì¹˜ ë§ˆìŠ¤í¬ [batch, prompt_len]
            max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´
            steps: Diffusion steps
            temperature: Gumbel noise temperature (0=greedy, >0=stochastic)
            remasking_strategy: 'low_confidence' | 'random' | 'none'
            use_semi_ar: Semi-Autoregressive ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€
            semi_ar_block_size: Semi-AR ë¸”ë¡ í¬ê¸°
            task_name: Task ì´ë¦„ (Semi-AR ëª¨ë“œì—ì„œ format token ê²°ì •ìš©)
        """
        # Semi-AR ëª¨ë“œ ë¶„ê¸°
        if use_semi_ar:
            return self._generate_semi_ar(
                graphs=graphs,
                input_ids=input_ids,
                attention_mask=attention_mask,
                is_mol_token=is_mol_token,
                max_length=max_length,
                steps=steps,
                temperature=temperature,
                remasking_strategy=remasking_strategy,
                block_size=semi_ar_block_size,
                task_name=task_name,
                **kwargs
            )

        batch_size = input_ids.shape[0]
        prompt_len = input_ids.shape[1]
        gen_len = max_length

        # ì´ˆê¸°í™”: ìƒì„± ì˜ì—­ì„ ëª¨ë‘ MASKë¡œ ì„¤ì • (t=1 ìƒíƒœ)
        gen_tokens = torch.full((batch_size, gen_len), self.mask_token_id, device=self.device, dtype=torch.long)
        full_ids = torch.cat([input_ids, gen_tokens], dim=1)

        gen_mask = torch.ones((batch_size, gen_len), device=self.device, dtype=attention_mask.dtype)
        full_attention_mask = torch.cat([attention_mask, gen_mask], dim=1)

        if is_mol_token is not None:
            is_mol_token_gen = torch.zeros((batch_size, gen_len), device=self.device, dtype=torch.bool)
            full_is_mol_token = torch.cat([is_mol_token, is_mol_token_gen], dim=1)
        else:
            full_is_mol_token = None

        # ================================================================
        # Remasking ì „ëµì— ë”°ë¥¸ ìƒì„± ë¡œì§
        # ================================================================

        if remasking_strategy == 'none':
            # ê¸°ì¡´ ë°©ì‹: ë§¤ stepë§ˆë‹¤ top-k í† í°ë§Œ unmask (remasking ì—†ìŒ)
            return self._generate_no_remask(
                full_ids=full_ids,
                full_attention_mask=full_attention_mask,
                full_is_mol_token=full_is_mol_token,
                graphs=graphs,
                prompt_len=prompt_len,
                gen_len=gen_len,
                steps=steps,
                temperature=temperature,
            )
        else:
            # Algorithm 4 (random) ë˜ëŠ” Algorithm 5 (low_confidence)
            return self._generate_with_remask(
                full_ids=full_ids,
                full_attention_mask=full_attention_mask,
                full_is_mol_token=full_is_mol_token,
                graphs=graphs,
                prompt_len=prompt_len,
                gen_len=gen_len,
                steps=steps,
                temperature=temperature,
                remasking_strategy=remasking_strategy,
            )

    def _generate_no_remask(
        self,
        full_ids,
        full_attention_mask,
        full_is_mol_token,
        graphs,
        prompt_len,
        gen_len,
        steps,
        temperature,
    ):
        """
        Remasking ì—†ëŠ” ìƒì„± (ê¸°ì¡´ ë°©ì‹)

        ë§¤ stepë§ˆë‹¤ ê°€ì¥ ë†’ì€ confidenceì˜ kê°œ í† í°ë§Œ unmask.
        í•œë²ˆ unmaskëœ í† í°ì€ ë³€ê²½ë˜ì§€ ì•ŠìŒ.
        """
        batch_size = full_ids.shape[0]

        mask_index = (full_ids[:, prompt_len:] == self.mask_token_id)
        num_transfer_tokens = self.get_num_transfer_tokens(mask_index, steps)

        for step in range(steps):
            cur_mask_index = (full_ids == self.mask_token_id)

            current_embeds = self.llm_embed_tokens(full_ids)

            if graphs is not None:
                current_embeds, _, _ = self.inject_graph_embeds2input_embeds(
                    input_embeds=current_embeds,
                    is_mol_token=full_is_mol_token,
                    graphs=graphs
                )

            outputs = self.llm_model(
                inputs_embeds=current_embeds,
                attention_mask=full_attention_mask,
                return_dict=True
            )
            logits = outputs.logits

            logits_with_noise = self.add_gumbel_noise(logits, temperature)
            x0_pred = torch.argmax(logits_with_noise, dim=-1)

            # Confidence ê³„ì‚° (low_confidence ë°©ì‹ ì‚¬ìš©)
            p = F.softmax(logits, dim=-1)
            x0_p = torch.squeeze(
                torch.gather(p, dim=-1, index=torch.unsqueeze(x0_pred, -1)), -1
            )

            x0_p[:, :prompt_len] = -np.inf
            confidence = torch.where(cur_mask_index, x0_p, torch.tensor(-np.inf, device=self.device))

            # Top-k ì„ íƒí•˜ì—¬ unmask
            transfer_mask = torch.zeros_like(full_ids, dtype=torch.bool)

            for b in range(batch_size):
                k = num_transfer_tokens[b, step]
                if k > 0:
                    _, select_indices = torch.topk(confidence[b], k=k)
                    transfer_mask[b, select_indices] = True

            full_ids[transfer_mask] = x0_pred[transfer_mask]

        generated_tokens = full_ids[:, prompt_len:]
        generated_text = self.llm_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

        return AttrDict(
            predictions=generated_text,
            sequences=full_ids,
            logits=logits,
            attentions=None
        )

    def _generate_with_remask(
        self,
        full_ids,
        full_attention_mask,
        full_is_mol_token,
        graphs,
        prompt_len,
        gen_len,
        steps,
        temperature,
        remasking_strategy,
    ):
        """
        LLaDA ë…¼ë¬¸ Algorithm 4 (random) & Algorithm 5 (low_confidence) êµ¬í˜„

        í•µì‹¬ ì°¨ì´ì  (ê¸°ì¡´ ë°©ì‹ vs ë…¼ë¬¸ ë°©ì‹):
        - ê¸°ì¡´: ë§¤ stepë§ˆë‹¤ kê°œ í† í°ë§Œ unmask, ë‚˜ë¨¸ì§€ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        - ë…¼ë¬¸: ëª¨ë“  mask í† í°ì„ ì˜ˆì¸¡ í›„, ë‚®ì€ confidence í† í°ì„ ë‹¤ì‹œ mask

        Algorithm 5 (Low-Confidence Remasking):
        1. ëª¨ë“  masked í† í°ì„ ì˜ˆì¸¡ (r0 = argmax)
        2. ì´ë¯¸ unmaskedëœ í† í°ì€ confidence = 1ë¡œ ì„¤ì •
        3. nun = âŒŠL(1-s)âŒ‹ ê°œì˜ ê°€ì¥ ë†’ì€ confidence í† í°ë§Œ unmask ìƒíƒœ ìœ ì§€
        4. ë‚˜ë¨¸ì§€ëŠ” ë‹¤ì‹œ MASKë¡œ ë˜ëŒë¦¼ (remasking)

        Algorithm 4 (Random Remasking):
        - Step 2ì—ì„œ confidence ëŒ€ì‹  random ê°’ ì‚¬ìš©
        """
        batch_size = full_ids.shape[0]

        for step in range(steps):
            # í˜„ì¬ timestep tì™€ ë‹¤ìŒ timestep s ê³„ì‚°
            t = 1.0 - step / steps
            s = max(0.0, t - 1.0 / steps)

            # Forward pass
            current_embeds = self.llm_embed_tokens(full_ids)

            if graphs is not None:
                current_embeds, _, _ = self.inject_graph_embeds2input_embeds(
                    input_embeds=current_embeds,
                    is_mol_token=full_is_mol_token,
                    graphs=graphs
                )

            outputs = self.llm_model(
                inputs_embeds=current_embeds,
                attention_mask=full_attention_mask,
                return_dict=True
            )
            logits = outputs.logits

            # Gumbel noiseë¡œ ì˜ˆì¸¡
            logits_with_noise = self.add_gumbel_noise(logits, temperature)
            x0_pred = torch.argmax(logits_with_noise, dim=-1)

            # ============================================================
            # Confidence ê³„ì‚° (Algorithm 5 line 8-9)
            # ============================================================
            cur_mask_index = (full_ids == self.mask_token_id)

            if remasking_strategy == 'low_confidence':
                # Algorithm 5: ì˜ˆì¸¡ í† í°ì˜ í™•ë¥ ì„ confidenceë¡œ ì‚¬ìš©
                p = F.softmax(logits, dim=-1)
                x0_p = torch.squeeze(
                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0_pred, -1)), -1
                )
            elif remasking_strategy == 'random':
                # Algorithm 4: ëœë¤ ê°’ì„ confidenceë¡œ ì‚¬ìš©
                x0_p = torch.rand_like(logits[:, :, 0])
            else:
                raise NotImplementedError(f"Unknown remasking strategy: {remasking_strategy}")

            # Confidence ì´ˆê¸°í™”:
            # - í˜„ì¬ maskedì¸ ìœ„ì¹˜: ì˜ˆì¸¡ confidence
            # - í˜„ì¬ unmaskedì¸ ìœ„ì¹˜: 1.0 (ì´ë¯¸ í™•ì •ëœ í† í°ì€ ìœ ì§€)
            confidence = torch.zeros_like(x0_p)
            confidence[:, :prompt_len] = np.inf  # promptëŠ” ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
            confidence[:, prompt_len:] = torch.where(
                cur_mask_index[:, prompt_len:],
                x0_p[:, prompt_len:],  # masked -> ì˜ˆì¸¡ confidence
                torch.ones_like(x0_p[:, prompt_len:])  # unmasked -> 1.0 (ìœ ì§€)
            )

            # ============================================================
            # Step 1: ëª¨ë“  masked ìœ„ì¹˜ë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì±„ì›€ (Algorithm 5 line 6-8)
            # ============================================================
            full_ids = torch.where(cur_mask_index, x0_pred, full_ids)

            # ============================================================
            # Step 2: Remasking (Algorithm 5 line 12-16)
            # nun = âŒŠL(1-s)âŒ‹ ê°œì˜ ê°€ì¥ ë†’ì€ confidence í† í°ë§Œ ìœ ì§€
            # ============================================================
            if s > 0:  # ë§ˆì§€ë§‰ stepì´ ì•„ë‹ˆë©´ remasking ìˆ˜í–‰
                # nun: unmask ìƒíƒœë¡œ ìœ ì§€í•  í† í° ìˆ˜
                nun = int(gen_len * (1 - s))

                for b in range(batch_size):
                    # ìƒì„± ì˜ì—­ì˜ confidenceë§Œ ê³ ë ¤
                    gen_confidence = confidence[b, prompt_len:]

                    if nun < gen_len:
                        # ê°€ì¥ ë†’ì€ confidenceì˜ nunê°œë§Œ ìœ ì§€, ë‚˜ë¨¸ì§€ëŠ” ë‹¤ì‹œ mask
                        _, keep_indices = torch.topk(gen_confidence, k=nun, largest=True)

                        # ëª¨ë“  ìƒì„± ì˜ì—­ì„ MASKë¡œ ì„¤ì •
                        full_ids[b, prompt_len:] = self.mask_token_id

                        # ê°€ì¥ ë†’ì€ confidenceì˜ í† í°ë§Œ ë³µì›
                        full_ids[b, prompt_len + keep_indices] = x0_pred[b, prompt_len + keep_indices]

        generated_tokens = full_ids[:, prompt_len:]
        generated_text = self.llm_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

        return AttrDict(
            predictions=generated_text,
            sequences=full_ids,
            logits=logits,
            attentions=None
        )

    def _generate_semi_ar(
        self,
        graphs,
        input_ids,
        attention_mask,
        is_mol_token,
        max_length,
        steps,
        temperature,
        remasking_strategy,
        block_size,
        task_name=None,
        **kwargs
    ):
        """
        Semi-Autoregressive Generation (ë…¼ë¬¸ Appendix A.3, Figure 4)

        ì‹œí€€ìŠ¤ë¥¼ ì—¬ëŸ¬ ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ìˆœì°¨ ìƒì„±.
        ê° ë¸”ë¡ ë‚´ì—ì„œëŠ” ì§€ì •ëœ remasking_strategy ì‚¬ìš©.

        Args:
            block_size: ê° ë¸”ë¡ì˜ í¬ê¸° (í† í° ìˆ˜)
            remasking_strategy: ë¸”ë¡ ë‚´ remasking ì „ëµ
        """
        batch_size = input_ids.shape[0]
        prompt_len = input_ids.shape[1]
        gen_len = max_length

        # ì´ˆê¸°í™”
        gen_tokens = torch.full((batch_size, gen_len), self.mask_token_id, device=self.device, dtype=torch.long)
        full_ids = torch.cat([input_ids, gen_tokens], dim=1)

        gen_mask = torch.ones((batch_size, gen_len), device=self.device, dtype=attention_mask.dtype)
        full_attention_mask = torch.cat([attention_mask, gen_mask], dim=1)

        if is_mol_token is not None:
            is_mol_token_gen = torch.zeros((batch_size, gen_len), device=self.device, dtype=torch.bool)
            full_is_mol_token = torch.cat([is_mol_token, is_mol_token_gen], dim=1)
        else:
            full_is_mol_token = None

        # ë¸”ë¡ ìˆ˜ ê³„ì‚°
        num_blocks = (gen_len + block_size - 1) // block_size

        # ê° ë¸”ë¡ì— í• ë‹¹í•  step ìˆ˜
        steps_per_block = max(1, steps // num_blocks)

        for block_idx in range(num_blocks):
            block_start = prompt_len + block_idx * block_size
            block_end = min(prompt_len + (block_idx + 1) * block_size, prompt_len + gen_len)
            current_block_size = block_end - block_start

            if current_block_size <= 0:
                break

            # í˜„ì¬ ë¸”ë¡ì— ëŒ€í•´ diffusion ìˆ˜í–‰
            for step in range(steps_per_block):
                t = 1.0 - step / steps_per_block
                s = max(0.0, t - 1.0 / steps_per_block)

                # Forward pass (ì „ì²´ ì‹œí€€ìŠ¤)
                current_embeds = self.llm_embed_tokens(full_ids)

                if graphs is not None:
                    current_embeds, _, _ = self.inject_graph_embeds2input_embeds(
                        input_embeds=current_embeds,
                        is_mol_token=full_is_mol_token,
                        graphs=graphs
                    )

                outputs = self.llm_model(
                    inputs_embeds=current_embeds,
                    attention_mask=full_attention_mask,
                    return_dict=True
                )
                logits = outputs.logits

                logits_with_noise = self.add_gumbel_noise(logits, temperature)
                x0_pred = torch.argmax(logits_with_noise, dim=-1)

                # í˜„ì¬ ë¸”ë¡ì˜ mask ì—¬ë¶€ í™•ì¸
                cur_block_mask = (full_ids[:, block_start:block_end] == self.mask_token_id)

                # Confidence ê³„ì‚°
                if remasking_strategy == 'low_confidence':
                    p = F.softmax(logits[:, block_start:block_end, :], dim=-1)
                    block_pred = x0_pred[:, block_start:block_end]
                    x0_p = torch.squeeze(
                        torch.gather(p, dim=-1, index=torch.unsqueeze(block_pred, -1)), -1
                    )
                elif remasking_strategy == 'random':
                    x0_p = torch.rand((batch_size, current_block_size), device=self.device)
                else:
                    # 'none': confidence ê¸°ë°˜ ì„ íƒ
                    p = F.softmax(logits[:, block_start:block_end, :], dim=-1)
                    block_pred = x0_pred[:, block_start:block_end]
                    x0_p = torch.squeeze(
                        torch.gather(p, dim=-1, index=torch.unsqueeze(block_pred, -1)), -1
                    )

                # Confidence ì„¤ì •
                confidence = torch.where(
                    cur_block_mask,
                    x0_p,
                    torch.ones_like(x0_p)  # ì´ë¯¸ unmaskëœ í† í°ì€ ìœ ì§€
                )

                # ëª¨ë“  masked ìœ„ì¹˜ë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì±„ì›€
                full_ids[:, block_start:block_end] = torch.where(
                    cur_block_mask,
                    x0_pred[:, block_start:block_end],
                    full_ids[:, block_start:block_end]
                )

                # Remasking (ë§ˆì§€ë§‰ stepì´ ì•„ë‹Œ ê²½ìš°)
                if s > 0 and remasking_strategy != 'none':
                    nun = int(current_block_size * (1 - s))

                    for b in range(batch_size):
                        block_confidence = confidence[b]

                        if nun < current_block_size:
                            _, keep_indices = torch.topk(block_confidence, k=nun, largest=True)

                            # ë¸”ë¡ ì „ì²´ë¥¼ MASKë¡œ ì„¤ì •
                            full_ids[b, block_start:block_end] = self.mask_token_id

                            # ë†’ì€ confidence í† í°ë§Œ ë³µì›
                            for idx in keep_indices:
                                full_ids[b, block_start + idx] = x0_pred[b, block_start + idx]

        generated_tokens = full_ids[:, prompt_len:]
        generated_text = self.llm_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

        return AttrDict(
            predictions=generated_text,
            sequences=full_ids,
            logits=logits,
            attentions=None
        )

    def extract_graph_feature(self, samples):
        return None, None, None

    # ==========================================================================
    # Semi-Autoregressive Generation
    # ==========================================================================
    # í•µì‹¬ ì•„ì´ë””ì–´:
    # 1. Task ìœ í˜•ì— ë”°ë¼ format tokens (<BOOLEAN>, <SELFIES> ë“±)ì„ ë¨¼ì € ê²°ì •
    # 2. Format tokensë¥¼ ìƒì„± ì‹œí€€ìŠ¤ì˜ ì‹œì‘/ë ìœ„ì¹˜ì— ë°°ì¹˜
    # 3. ë‚˜ë¨¸ì§€ content ìœ„ì¹˜ì—ì„œë§Œ diffusion ìˆ˜í–‰
    # ==========================================================================

    # Task -> Format Token ë§¤í•‘
    TASK_FORMAT_MAPPING = {
        # Classification tasks -> BOOLEAN
        'bace': ('BOOLEAN', 'BOOLEAN'),
        'bbbp': ('BOOLEAN', 'BOOLEAN'),
        'hiv': ('BOOLEAN', 'BOOLEAN'),
        'clintox': ('BOOLEAN', 'BOOLEAN'),
        'tox21': ('BOOLEAN', 'BOOLEAN'),
        'sider': ('BOOLEAN', 'BOOLEAN'),
        'pcba': ('BOOLEAN', 'BOOLEAN'),
        'muv': ('BOOLEAN', 'BOOLEAN'),

        # Regression tasks -> FLOAT
        'esol': ('FLOAT', 'FLOAT'),
        'freesolv': ('FLOAT', 'FLOAT'),
        'lipo': ('FLOAT', 'FLOAT'),
        'qm7': ('FLOAT', 'FLOAT'),
        'qm8': ('FLOAT', 'FLOAT'),
        'qm9': ('FLOAT', 'FLOAT'),

        # Description tasks -> DESCRIPTION
        'chebi-20': ('DESCRIPTION', 'DESCRIPTION'),
        'mol2text': ('DESCRIPTION', 'DESCRIPTION'),
        'description': ('DESCRIPTION', 'DESCRIPTION'),

        # Molecule generation tasks -> SELFIES
        'text2mol': ('SELFIES', 'SELFIES'),
        'forward_synthesis': ('SELFIES', 'SELFIES'),
        'retrosynthesis': ('SELFIES', 'SELFIES'),
        'molecule_generation': ('SELFIES', 'SELFIES'),
        'selfies': ('SELFIES', 'SELFIES'),

        # IUPAC tasks
        'iupac': ('IUPAC', 'IUPAC'),
        'smiles2iupac': ('IUPAC', 'IUPAC'),

        # Molecular Formula
        'molformula': ('MOLFORMULA', 'MOLFORMULA'),
    }

    def _get_format_tokens_for_task(self, task_name):
        """
        Task ì´ë¦„ì— ë”°ë¼ ì ì ˆí•œ format token pairë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            task_name: Task ì´ë¦„ (ì˜ˆ: 'bace', 'esol', 'chebi-20')

        Returns:
            Tuple[str, str]: (open_tag, close_tag) ì˜ˆ: ('<BOOLEAN>', '</BOOLEAN>')
        """
        if task_name is None:
            return None, None

        task_lower = task_name.lower().strip()

        # ì •í™•í•œ ë§¤ì¹­ ë¨¼ì € ì‹œë„
        if task_lower in self.TASK_FORMAT_MAPPING:
            format_type = self.TASK_FORMAT_MAPPING[task_lower][0]
            return f'<{format_type}>', f'</{format_type}>'

        # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„ (task ì´ë¦„ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°)
        for key, (format_type, _) in self.TASK_FORMAT_MAPPING.items():
            if key in task_lower or task_lower in key:
                return f'<{format_type}>', f'</{format_type}>'

        # í‚¤ì›Œë“œ ê¸°ë°˜ fallback
        if any(kw in task_lower for kw in ['class', 'binary', 'bool']):
            return '<BOOLEAN>', '</BOOLEAN>'
        elif any(kw in task_lower for kw in ['regress', 'predict', 'value', 'float']):
            return '<FLOAT>', '</FLOAT>'
        elif any(kw in task_lower for kw in ['desc', 'caption', 'text', 'explain']):
            return '<DESCRIPTION>', '</DESCRIPTION>'
        elif any(kw in task_lower for kw in ['mol', 'smiles', 'selfies', 'synth', 'retro']):
            return '<SELFIES>', '</SELFIES>'

        return None, None

    def _estimate_content_length(self, task_name, max_length):
        """
        Taskì— ë”°ë¥¸ ì˜ˆìƒ content ê¸¸ì´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        Format tokensì„ ì œì™¸í•œ ì‹¤ì œ content ì˜ì—­ì˜ ê¸¸ì´ì…ë‹ˆë‹¤.

        Args:
            task_name: Task ì´ë¦„
            max_length: ì „ì²´ ìµœëŒ€ ìƒì„± ê¸¸ì´

        Returns:
            int: ì˜ˆìƒ content ê¸¸ì´
        """
        task_lower = task_name.lower().strip() if task_name else ''

        # Classification: "True" or "False" -> ë§¤ìš° ì§§ìŒ
        if task_lower in ['bace', 'bbbp', 'hiv', 'clintox', 'tox21', 'sider', 'pcba', 'muv']:
            return min(8, max_length - 4)  # "True"/"False" + ì—¬ìœ 

        # Regression: ìˆ«ì -> ì§§ìŒ
        if task_lower in ['esol', 'freesolv', 'lipo', 'qm7', 'qm8', 'qm9']:
            return min(16, max_length - 4)  # "-3.456" ì •ë„

        # Description: ê¸´ í…ìŠ¤íŠ¸
        if task_lower in ['chebi-20', 'mol2text', 'description']:
            return max_length - 4

        # Molecule generation: SELFIES ì‹œí€€ìŠ¤
        if task_lower in ['text2mol', 'forward_synthesis', 'retrosynthesis', 'molecule_generation', 'selfies']:
            return max_length - 4

        # Default
        return max_length - 4

    @torch.no_grad()
    def generate_semi_ar(
        self,
        graphs,
        input_ids,
        attention_mask,
        is_mol_token=None,
        task_name=None,  # Task ì´ë¦„ (format ê²°ì •ìš©)
        max_length=128,
        steps=64,
        temperature=0.0,
        remasking_strategy='low_confidence',
        **kwargs
    ):
        """
        Semi-Autoregressive Generation for LLaDA

        Format tokensë¥¼ ë¨¼ì € ê³ ì •í•œ í›„, contentë§Œ diffusionìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        ì´ë¥¼ í†µí•´ multi-task í™˜ê²½ì—ì„œ format í˜¼ë€ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.

        Args:
            graphs: ë¶„ì ê·¸ë˜í”„ (tuple of main_graph, additional_graph)
            input_ids: ì…ë ¥ í† í° ID [batch, prompt_len]
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬ [batch, prompt_len]
            is_mol_token: mol token ìœ„ì¹˜ ë§ˆìŠ¤í¬ [batch, prompt_len]
            task_name: Task ì´ë¦„ ë˜ëŠ” Task ë¦¬ìŠ¤íŠ¸ (batchë³„ë¡œ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
            max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´
            steps: Diffusion steps
            temperature: Gumbel noise temperature
            remasking_strategy: 'low_confidence' ë˜ëŠ” 'random'

        Returns:
            AttrDict with predictions, sequences, logits, attentions
        """
        batch_size = input_ids.shape[0]
        prompt_len = input_ids.shape[1]

        # Task nameì„ batch í˜•íƒœë¡œ ì •ê·œí™”
        if task_name is None:
            task_names = [None] * batch_size
        elif isinstance(task_name, str):
            task_names = [task_name] * batch_size
        else:
            task_names = list(task_name)
            if len(task_names) < batch_size:
                task_names.extend([task_names[-1]] * (batch_size - len(task_names)))

        # ============================================================
        # Step 1: Format tokens ê²°ì • ë° ë°°ì¹˜
        # ============================================================

        # ê° ìƒ˜í”Œë³„ format token ì •ë³´ ìˆ˜ì§‘
        format_info = []
        for i, tn in enumerate(task_names):
            open_tag, close_tag = self._get_format_tokens_for_task(tn)

            if open_tag is not None:
                open_id = self.llm_tokenizer.convert_tokens_to_ids(open_tag)
                close_id = self.llm_tokenizer.convert_tokens_to_ids(close_tag)

                # í† í°ì´ ì œëŒ€ë¡œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if open_id == self.llm_tokenizer.unk_token_id or close_id == self.llm_tokenizer.unk_token_id:
                    logger.warning(f"[Semi-AR] Format tokens not found in vocab: {open_tag}, {close_tag}")
                    format_info.append({'has_format': False})
                else:
                    content_len = self._estimate_content_length(tn, max_length)
                    format_info.append({
                        'has_format': True,
                        'open_tag': open_tag,
                        'close_tag': close_tag,
                        'open_id': open_id,
                        'close_id': close_id,
                        'content_len': content_len
                    })
            else:
                format_info.append({'has_format': False})

        # ============================================================
        # Step 2: ìƒì„± ì‹œí€€ìŠ¤ ì´ˆê¸°í™” (format tokens í¬í•¨)
        # ============================================================

        gen_len = max_length
        gen_tokens = torch.full((batch_size, gen_len), self.mask_token_id, device=self.device, dtype=torch.long)

        # Format tokens ë°°ì¹˜
        for i, info in enumerate(format_info):
            if info['has_format']:
                # ì‹œì‘ ìœ„ì¹˜: 0ë²ˆ ì¸ë±ìŠ¤
                gen_tokens[i, 0] = info['open_id']

                # ì¢…ë£Œ ìœ„ì¹˜: content_len + 1 (content ë°”ë¡œ ë’¤)
                # ë˜ëŠ” max_length - 1 (ë ìœ„ì¹˜)
                end_pos = min(info['content_len'] + 1, gen_len - 1)
                gen_tokens[i, end_pos] = info['close_id']

                # ì €ì¥í•´ë‘  (ë‚˜ì¤‘ì— maskì—ì„œ ì œì™¸ìš©)
                info['open_pos'] = 0
                info['close_pos'] = end_pos

        # ì „ì²´ ì‹œí€€ìŠ¤ êµ¬ì„±
        full_ids = torch.cat([input_ids, gen_tokens], dim=1)

        gen_mask = torch.ones((batch_size, gen_len), device=self.device, dtype=attention_mask.dtype)
        full_attention_mask = torch.cat([attention_mask, gen_mask], dim=1)

        if is_mol_token is not None:
            is_mol_token_gen = torch.zeros((batch_size, gen_len), device=self.device, dtype=torch.bool)
            full_is_mol_token = torch.cat([is_mol_token, is_mol_token_gen], dim=1)
        else:
            full_is_mol_token = None

        # ============================================================
        # Step 3: Format tokensì„ ì œì™¸í•œ mask index ìƒì„±
        # ============================================================

        # ê¸°ë³¸ mask: ìƒì„± ì˜ì—­ì˜ mask token
        mask_index = (full_ids[:, prompt_len:] == self.mask_token_id)

        # Format tokens ìœ„ì¹˜ëŠ” maskì—ì„œ ì œì™¸ (ì´ë¯¸ ê³ ì •ë¨)
        # (ìœ„ì—ì„œ gen_tokensì— ì´ë¯¸ format tokenì„ ë„£ì—ˆìœ¼ë¯€ë¡œ,
        #  mask_token_idê°€ ì•„ë‹Œ ìœ„ì¹˜ëŠ” ìë™ìœ¼ë¡œ mask_index=Falseê°€ ë¨)

        num_transfer_tokens = self.get_num_transfer_tokens(mask_index, steps)

        # ============================================================
        # Step 4: Iterative Denoising (contentë§Œ)
        # ============================================================

        for step in range(steps):
            # í˜„ì¬ mask ìƒíƒœ (prompt ì œì™¸, format tokensë„ mask ì•„ë‹˜)
            cur_mask_index = (full_ids == self.mask_token_id)

            current_embeds = self.llm_embed_tokens(full_ids)

            # ê·¸ë˜í”„ ì£¼ì…
            if graphs is not None:
                current_embeds, _, _ = self.inject_graph_embeds2input_embeds(
                    input_embeds=current_embeds,
                    is_mol_token=full_is_mol_token,
                    graphs=graphs
                )

            outputs = self.llm_model(
                inputs_embeds=current_embeds,
                attention_mask=full_attention_mask,
                return_dict=True
            )
            logits = outputs.logits

            logits_with_noise = self.add_gumbel_noise(logits, temperature)
            x0_pred = torch.argmax(logits_with_noise, dim=-1)

            # Confidence ê³„ì‚°
            if remasking_strategy == 'low_confidence':
                p = F.softmax(logits, dim=-1)
                x0_p = torch.squeeze(
                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0_pred, -1)), -1
                )
            elif remasking_strategy == 'random':
                x0_p = torch.rand_like(logits[:, :, 0])
            else:
                raise NotImplementedError(f"Unknown remasking strategy: {remasking_strategy}")

            # Prompt ì˜ì—­ì€ confidence -inf (ìˆ˜ì • ì•ˆí•¨)
            x0_p[:, :prompt_len] = -np.inf

            # Format token ìœ„ì¹˜ë„ -inf (ì´ë¯¸ ê³ ì •ë¨)
            for i, info in enumerate(format_info):
                if info['has_format']:
                    x0_p[i, prompt_len + info['open_pos']] = -np.inf
                    x0_p[i, prompt_len + info['close_pos']] = -np.inf

            confidence = torch.where(cur_mask_index, x0_p, torch.tensor(-np.inf, device=self.device))

            # Top-k ì„ íƒí•˜ì—¬ unmask
            transfer_mask = torch.zeros_like(full_ids, dtype=torch.bool)

            for b in range(batch_size):
                k = num_transfer_tokens[b, step]
                if k > 0:
                    _, select_indices = torch.topk(confidence[b], k=k)
                    transfer_mask[b, select_indices] = True

            full_ids[transfer_mask] = x0_pred[transfer_mask]

        # ============================================================
        # Step 5: ê²°ê³¼ í›„ì²˜ë¦¬
        # ============================================================

        generated_tokens = full_ids[:, prompt_len:]
        generated_text = self.llm_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

        return AttrDict(
            predictions=generated_text,
            sequences=full_ids,
            logits=logits,
            attentions=None,
            format_info=format_info  # ë””ë²„ê¹…ìš©
        )

    @torch.no_grad()
    def generate(
        self,
        graphs,
        input_ids,
        attention_mask,
        is_mol_token=None,
        max_length=128,
        steps=64,
        temperature=0.0,
        remasking_strategy='low_confidence',
        use_semi_ar=False,  # Semi-AR ëª¨ë“œ í™œì„±í™” í”Œë˜ê·¸
        task_name=None,     # Semi-ARìš© task ì´ë¦„
        **kwargs
    ):
        """
        LLaDA Generation (ê¸°ë³¸ ë˜ëŠ” Semi-AR ëª¨ë“œ)

        Args:
            use_semi_ar: Trueì´ë©´ semi-autoregressive ëª¨ë“œ ì‚¬ìš©
            task_name: Semi-AR ëª¨ë“œì—ì„œ format ê²°ì •ì— ì‚¬ìš©
            (ë‚˜ë¨¸ì§€ ì¸ìëŠ” ê¸°ì¡´ê³¼ ë™ì¼)
        """
        # Semi-AR ëª¨ë“œ ë¶„ê¸°
        if use_semi_ar and task_name is not None:
            return self.generate_semi_ar(
                graphs=graphs,
                input_ids=input_ids,
                attention_mask=attention_mask,
                is_mol_token=is_mol_token,
                task_name=task_name,
                max_length=max_length,
                steps=steps,
                temperature=temperature,
                remasking_strategy=remasking_strategy,
                **kwargs
            )

        # ê¸°ì¡´ generation ë¡œì§
        batch_size = input_ids.shape[0]
        prompt_len = input_ids.shape[1]
        gen_len = max_length

        gen_tokens = torch.full((batch_size, gen_len), self.mask_token_id, device=self.device, dtype=torch.long)
        full_ids = torch.cat([input_ids, gen_tokens], dim=1)

        gen_mask = torch.ones((batch_size, gen_len), device=self.device, dtype=attention_mask.dtype)
        full_attention_mask = torch.cat([attention_mask, gen_mask], dim=1)

        if is_mol_token is not None:
            is_mol_token_gen = torch.zeros((batch_size, gen_len), device=self.device, dtype=torch.bool)
            full_is_mol_token = torch.cat([is_mol_token, is_mol_token_gen], dim=1)
        else:
            full_is_mol_token = None

        mask_index = (full_ids[:, prompt_len:] == self.mask_token_id)
        num_transfer_tokens = self.get_num_transfer_tokens(mask_index, steps)

        for step in range(steps):
            cur_mask_index = (full_ids == self.mask_token_id)

            current_embeds = self.llm_embed_tokens(full_ids)

            # ê·¸ë˜í”„ ì£¼ì… (ì˜¤ë²„ë¼ì´ë”©ëœ ë©”ì„œë“œ ì‚¬ìš©)
            if graphs is not None:
                current_embeds, _, _ = self.inject_graph_embeds2input_embeds(
                    input_embeds=current_embeds,
                    is_mol_token=full_is_mol_token,
                    graphs=graphs
                )

            outputs = self.llm_model(
                inputs_embeds=current_embeds,
                attention_mask=full_attention_mask,
                return_dict=True
            )
            logits = outputs.logits

            logits_with_noise = self.add_gumbel_noise(logits, temperature)
            x0_pred = torch.argmax(logits_with_noise, dim=-1)

            if remasking_strategy == 'low_confidence':
                p = F.softmax(logits, dim=-1)
                x0_p = torch.squeeze(
                    torch.gather(p, dim=-1, index=torch.unsqueeze(x0_pred, -1)), -1
                )
            elif remasking_strategy == 'random':
                x0_p = torch.rand_like(logits[:, :, 0])
            else:
                raise NotImplementedError(f"Unknown remasking strategy: {remasking_strategy}")

            x0_p[:, :prompt_len] = -np.inf
            confidence = torch.where(cur_mask_index, x0_p, torch.tensor(-np.inf, device=self.device))

            transfer_mask = torch.zeros_like(full_ids, dtype=torch.bool)

            for b in range(batch_size):
                k = num_transfer_tokens[b, step]
                if k > 0:
                    _, select_indices = torch.topk(confidence[b], k=k)
                    transfer_mask[b, select_indices] = True

            full_ids[transfer_mask] = x0_pred[transfer_mask]

        generated_tokens = full_ids[:, prompt_len:]
        generated_text = self.llm_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)

        # [ìˆ˜ì •] attentions=None ì¶”ê°€í•˜ì—¬ ì—ëŸ¬ ë°©ì§€
        return AttrDict(
            predictions=generated_text,
            sequences=full_ids,
            logits=logits,
            attentions=None
        )

    # ==========================================================================
    # LLaDA Paper Eq. 6: Monte Carlo Likelihood Estimation
    # ==========================================================================
    #
    # ë…¼ë¬¸ Section 2.4 Inference, Algorithm 3 (Appendix A), Appendix B.5 ì°¸ì¡°
    #
    # Eq. 6: log p(y|x) â‰ˆ (1/K) Î£_{k=1}^{K} Î£_{i=1}^{|y|} log p(y_i | x, y^{M_k})
    #
    # í•µì‹¬ ì•„ì´ë””ì–´:
    # 1. Kê°œì˜ Monte Carlo ìƒ˜í”Œì—ì„œ ë§ˆìŠ¤í‚¹ ë¹„ìœ¨ t_k ~ Uniform(0,1) ìƒ˜í”Œë§
    # 2. ì‘ë‹µ í† í° ì¤‘ t_k ë¹„ìœ¨ë§Œí¼ ëœë¤í•˜ê²Œ ë§ˆìŠ¤í‚¹
    # 3. Forward passë¡œ ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì˜ log-probability ê³„ì‚°
    # 4. í‰ê·  log-likelihood ë°˜í™˜
    # ==========================================================================

    @torch.no_grad()
    def compute_response_likelihood(
        self,
        graphs,
        input_ids,
        attention_mask,
        response_ids,
        response_attention_mask,
        is_mol_token=None,
        num_samples=128,  # ë…¼ë¬¸ Appendix B.5: K=128 for evaluation
    ):
        """
        LLaDA ë…¼ë¬¸ Eq. 6ì— ë”°ë¥¸ Monte Carlo Likelihood ì¶”ì •

        Args:
            graphs: ë¶„ì ê·¸ë˜í”„ (tuple of main_graph, additional_graph)
            input_ids: í”„ë¡¬í”„íŠ¸ í† í° ID [batch, prompt_len]
            attention_mask: í”„ë¡¬í”„íŠ¸ ì–´í…ì…˜ ë§ˆìŠ¤í¬ [batch, prompt_len]
            response_ids: ì‘ë‹µ í† í° ID [batch, response_len]
            response_attention_mask: ì‘ë‹µ ì–´í…ì…˜ ë§ˆìŠ¤í¬ [batch, response_len]
            is_mol_token: mol token ìœ„ì¹˜ ë§ˆìŠ¤í¬ [batch, prompt_len]
            num_samples: Monte Carlo ìƒ˜í”Œ ìˆ˜ (default: 128)

        Returns:
            torch.Tensor: ê° ìƒ˜í”Œì˜ í‰ê·  log-likelihood [batch]
        """
        batch_size = input_ids.shape[0]
        prompt_len = input_ids.shape[1]
        response_len = response_ids.shape[1]

        # ì „ì²´ ì‹œí€€ìŠ¤ êµ¬ì„±: [prompt | response]
        full_ids = torch.cat([input_ids, response_ids], dim=1)
        full_attention_mask = torch.cat([attention_mask, response_attention_mask], dim=1)

        if is_mol_token is not None:
            is_mol_token_resp = torch.zeros((batch_size, response_len), device=self.device, dtype=torch.bool)
            full_is_mol_token = torch.cat([is_mol_token, is_mol_token_resp], dim=1)
        else:
            full_is_mol_token = None

        # ì‘ë‹µ ì˜ì—­ ë§ˆìŠ¤í¬ (response_attention_maskê°€ 1ì¸ ìœ„ì¹˜)
        response_mask = response_attention_mask.bool()  # [batch, response_len]
        response_lengths = response_mask.sum(dim=1)  # [batch]

        # Monte Carlo ìƒ˜í”Œë§ìœ¼ë¡œ log-likelihood ì¶”ì •
        total_log_likelihood = torch.zeros(batch_size, device=self.device)

        for _ in range(num_samples):
            # Step 1: ê° ë°°ì¹˜ë³„ë¡œ ë§ˆìŠ¤í‚¹ ë¹„ìœ¨ t ~ Uniform(0, 1) ìƒ˜í”Œë§
            t = torch.rand(batch_size, device=self.device)

            # Step 2: ì‘ë‹µ ì˜ì—­ì—ì„œ t ë¹„ìœ¨ë§Œí¼ ëœë¤í•˜ê²Œ ë§ˆìŠ¤í‚¹
            noisy_full_ids = full_ids.clone()
            mask_probs = torch.rand((batch_size, response_len), device=self.device)

            # ê° ìƒ˜í”Œë³„ë¡œ ë§ˆìŠ¤í‚¹ ë¹„ìœ¨ ì ìš©
            for b in range(batch_size):
                # ì‘ë‹µ ì˜ì—­ ë‚´ì—ì„œë§Œ ë§ˆìŠ¤í‚¹
                valid_response = response_mask[b]
                should_mask = (mask_probs[b] < t[b]) & valid_response

                # ë§ˆìŠ¤í‚¹ ì ìš© (prompt_len ì´í›„ê°€ response ì˜ì—­)
                noisy_full_ids[b, prompt_len:][should_mask] = self.mask_token_id

            # Step 3: Forward pass
            current_embeds = self.llm_embed_tokens(noisy_full_ids)

            if graphs is not None:
                current_embeds, _, _ = self.inject_graph_embeds2input_embeds(
                    input_embeds=current_embeds,
                    is_mol_token=full_is_mol_token,
                    graphs=graphs
                )

            outputs = self.llm_model(
                inputs_embeds=current_embeds,
                attention_mask=full_attention_mask,
                return_dict=True
            )
            logits = outputs.logits  # [batch, seq_len, vocab_size]

            # Step 4: ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì˜ log-probability ê³„ì‚°
            # ì‘ë‹µ ì˜ì—­ì˜ logitsë§Œ ì¶”ì¶œ
            response_logits = logits[:, prompt_len:, :]  # [batch, response_len, vocab_size]
            log_probs = F.log_softmax(response_logits, dim=-1)  # [batch, response_len, vocab_size]

            # ì›ë³¸ ì‘ë‹µ í† í°ì˜ log-probability ì¶”ì¶œ
            # gatherë¥¼ ì‚¬ìš©í•´ ê° ìœ„ì¹˜ì˜ ì •ë‹µ í† í°ì— ëŒ€í•œ log-prob ì¶”ì¶œ
            target_log_probs = torch.gather(
                log_probs,
                dim=-1,
                index=response_ids.unsqueeze(-1)
            ).squeeze(-1)  # [batch, response_len]

            # ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ë§Œ í•©ì‚° (ì‹¤ì œ ì‘ë‹µ í† í°ì´ ìˆëŠ” ìœ„ì¹˜)
            # í˜„ì¬ ìƒ˜í”Œì—ì„œ ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ = noisy_full_idsì—ì„œ mask_token_idì¸ ìœ„ì¹˜
            masked_in_response = (noisy_full_ids[:, prompt_len:] == self.mask_token_id) & response_mask

            # ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì˜ log-prob í•©ì‚°
            sample_log_likelihood = (target_log_probs * masked_in_response.float()).sum(dim=1)

            # ë§ˆìŠ¤í‚¹ëœ í† í° ìˆ˜ë¡œ ì •ê·œí™” (0 division ë°©ì§€)
            num_masked = masked_in_response.sum(dim=1).float()
            sample_log_likelihood = sample_log_likelihood / (num_masked + 1e-8)

            # ë§ˆìŠ¤í‚¹ëœ í† í°ì´ ì—†ëŠ” ê²½ìš° (tâ‰ˆ0) ì²˜ë¦¬
            sample_log_likelihood = torch.where(
                num_masked > 0,
                sample_log_likelihood,
                torch.zeros_like(sample_log_likelihood)
            )

            total_log_likelihood += sample_log_likelihood

        # Monte Carlo í‰ê· 
        avg_log_likelihood = total_log_likelihood / num_samples

        return avg_log_likelihood

    @torch.no_grad()
    def compute_binary_prob_likelihood(
        self,
        graphs,
        input_ids,
        attention_mask,
        is_mol_token=None,
    ):
        """
        Binary classification (True/False)ì— ëŒ€í•œ í™•ë¥ ì„ Likelihood ë¹„êµë¡œ ê³„ì‚°

        ì§§ì€ ì‘ë‹µ(True/False)ì— ìµœì í™”ëœ ë²„ì „:
        - ì „ì²´ ì‘ë‹µì„ ë§ˆìŠ¤í‚¹í•˜ê³  1íšŒ forward passë¡œ log-likelihood ê³„ì‚°
        - 128ë²ˆ Monte Carlo ìƒ˜í”Œë§ì€ ê¸´ ì‘ë‹µì—ë§Œ ì˜ë¯¸ ìˆìŒ
        - 3~4 í† í°ì§œë¦¬ ì‘ë‹µì—ì„œëŠ” ì „ì²´ ë§ˆìŠ¤í‚¹ì´ ë” íš¨ìœ¨ì ì´ê³  ì •í™•

        Args:
            graphs: ë¶„ì ê·¸ë˜í”„ (tuple)
            input_ids: í”„ë¡¬í”„íŠ¸ í† í° ID [batch, prompt_len]
            attention_mask: í”„ë¡¬í”„íŠ¸ ì–´í…ì…˜ ë§ˆìŠ¤í¬ [batch, prompt_len]
            is_mol_token: mol token ìœ„ì¹˜ ë§ˆìŠ¤í¬ [batch, prompt_len]

        Returns:
            torch.Tensor: [P(False), P(True)] í™•ë¥  [batch, 2]
        """
        batch_size = input_ids.shape[0]
        prompt_len = input_ids.shape[1]

        # í›„ë³´ ì‘ë‹µ í† í°í™” (Training target í˜•ì‹ê³¼ ì¼ì¹˜ì‹œí‚´)
        # Training target: "<BOOLEAN> True </BOOLEAN><|eot_id|>" (ê³µë°± í¬í•¨)
        # Likelihood ë¹„êµë„ ë™ì¼í•œ í˜•ì‹ ì‚¬ìš©í•´ì•¼ ì •í™•í•œ í™•ë¥  ì¶”ì • ê°€ëŠ¥
        true_response = "<BOOLEAN> True </BOOLEAN>"
        false_response = "<BOOLEAN> False </BOOLEAN>"

        true_tokens = self.llm_tokenizer.encode(true_response, add_special_tokens=False)
        false_tokens = self.llm_tokenizer.encode(false_response, add_special_tokens=False)

        # ë‘ ì‘ë‹µì˜ ê¸¸ì´ë¥¼ ë§ì¶¤ (ë” ê¸´ ìª½ì— ë§ì¶° padding)
        max_resp_len = max(len(true_tokens), len(false_tokens))

        # Padding (mask_token_id ì‚¬ìš© - ì–´ì°¨í”¼ ì „ì²´ ë§ˆìŠ¤í‚¹í•  ê²ƒì´ë¯€ë¡œ)
        true_tokens_padded = true_tokens + [self.mask_token_id] * (max_resp_len - len(true_tokens))
        false_tokens_padded = false_tokens + [self.mask_token_id] * (max_resp_len - len(false_tokens))

        true_ids = torch.tensor([true_tokens_padded] * batch_size, device=self.device, dtype=torch.long)
        false_ids = torch.tensor([false_tokens_padded] * batch_size, device=self.device, dtype=torch.long)

        # ì‹¤ì œ í† í° ìœ„ì¹˜ ë§ˆìŠ¤í¬ (padding ì œì™¸)
        true_valid_mask = torch.zeros((batch_size, max_resp_len), device=self.device, dtype=torch.bool)
        false_valid_mask = torch.zeros((batch_size, max_resp_len), device=self.device, dtype=torch.bool)
        true_valid_mask[:, :len(true_tokens)] = True
        false_valid_mask[:, :len(false_tokens)] = True

        # ================================================================
        # ì „ì²´ ë§ˆìŠ¤í‚¹ í›„ 1íšŒ forward passë¡œ log-likelihood ê³„ì‚°
        # ================================================================

        def compute_full_mask_likelihood(response_ids, valid_mask):
            """ì „ì²´ ì‘ë‹µì„ ë§ˆìŠ¤í‚¹í•˜ê³  log-likelihood ê³„ì‚°"""
            # ì „ì²´ ì‹œí€€ìŠ¤ êµ¬ì„±: [prompt | masked_response]
            masked_response = torch.full_like(response_ids, self.mask_token_id)
            full_ids = torch.cat([input_ids, masked_response], dim=1)
            full_attention_mask = torch.cat([
                attention_mask,
                torch.ones((batch_size, max_resp_len), device=self.device, dtype=attention_mask.dtype)
            ], dim=1)

            if is_mol_token is not None:
                is_mol_token_resp = torch.zeros((batch_size, max_resp_len), device=self.device, dtype=torch.bool)
                full_is_mol_token = torch.cat([is_mol_token, is_mol_token_resp], dim=1)
            else:
                full_is_mol_token = None

            # Forward pass
            current_embeds = self.llm_embed_tokens(full_ids)

            if graphs is not None:
                current_embeds, _, _ = self.inject_graph_embeds2input_embeds(
                    input_embeds=current_embeds,
                    is_mol_token=full_is_mol_token,
                    graphs=graphs
                )

            outputs = self.llm_model(
                inputs_embeds=current_embeds,
                attention_mask=full_attention_mask,
                return_dict=True
            )
            logits = outputs.logits  # [batch, seq_len, vocab_size]

            # ì‘ë‹µ ì˜ì—­ì˜ log-probability ê³„ì‚°
            response_logits = logits[:, prompt_len:, :]  # [batch, max_resp_len, vocab_size]
            log_probs = F.log_softmax(response_logits, dim=-1)

            # ì›ë³¸ ì‘ë‹µ í† í°ì˜ log-probability ì¶”ì¶œ
            target_log_probs = torch.gather(
                log_probs,
                dim=-1,
                index=response_ids.unsqueeze(-1)
            ).squeeze(-1)  # [batch, max_resp_len]

            # validí•œ ìœ„ì¹˜ë§Œ í•©ì‚° (padding ì œì™¸)
            log_likelihood = (target_log_probs * valid_mask.float()).sum(dim=1)

            # í† í° ìˆ˜ë¡œ ì •ê·œí™” (ê¸¸ì´ê°€ ë‹¤ë¥¸ ì‘ë‹µ ê°„ ê³µì •í•œ ë¹„êµ)
            num_tokens = valid_mask.sum(dim=1).float()
            log_likelihood = log_likelihood / (num_tokens + 1e-8)

            return log_likelihood

        # True/False ê°ê°ì˜ log-likelihood ê³„ì‚°
        true_log_likelihood = compute_full_mask_likelihood(true_ids, true_valid_mask)
        false_log_likelihood = compute_full_mask_likelihood(false_ids, false_valid_mask)

        # Log-likelihoodë¥¼ í™•ë¥ ë¡œ ë³€í™˜ (softmax)
        # [batch, 2] where dim=1 is [P(False), P(True)]
        log_likelihoods = torch.stack([false_log_likelihood, true_log_likelihood], dim=1)
        probs = F.softmax(log_likelihoods, dim=1)

        return probs


class AttrDict(dict):
    def __init__(self, *args, **kwargs):
        super(AttrDict, self).__init__(*args, **kwargs)
        self.__dict__ = self