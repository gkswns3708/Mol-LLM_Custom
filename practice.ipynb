{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U huggingface_hub pandas\n",
    "from huggingface_hub import list_repo_files, hf_hub_download\n",
    "import json, os, re\n",
    "import pandas as pd\n",
    "\n",
    "REPO = \"QizhiPei/BioT5_finetune_dataset\"\n",
    "RAW_DATA_ROOT = \"/appdataset/train/raw\"   # 예: \"./data\"\n",
    "OUT_DIR = os.path.join(RAW_DATA_ROOT, \"raw\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1) 레포 파일 목록\n",
    "files = list_repo_files(REPO, repo_type=\"dataset\")\n",
    "\n",
    "# 2) BACE JSON 선택 로직 (tasks_plus > tasks > splits_plus > splits 우선)\n",
    "PREFS = [\"tasks_plus/\", \"tasks/\", \"splits_plus/\", \"splits/\"]\n",
    "def pick_bace_file(split):\n",
    "    # 'validation'과 'valid' 모두 허용\n",
    "    split_alias = [\"validation\", \"valid\"] if split == \"validation\" else [split]\n",
    "    cand = []\n",
    "    for f in files:\n",
    "        fl = f.lower()\n",
    "        if not fl.endswith(\".json\"):\n",
    "            continue\n",
    "        if \"bace\" not in fl:\n",
    "            continue\n",
    "        if not any(p in f for p in PREFS):\n",
    "            continue\n",
    "        if not any(s in fl for s in split_alias):\n",
    "            continue\n",
    "        cand.append(f)\n",
    "    if not cand:\n",
    "        raise FileNotFoundError(f\"BACE {split} 파일을 찾지 못함. repo={REPO}\")\n",
    "    # 우선순위 폴더 > 경로 짧은 순으로 선택\n",
    "    cand.sort(key=lambda x: (PREFS.index(next(p for p in PREFS if p in x)), len(x)))\n",
    "    return cand[0]\n",
    "\n",
    "paths = {\n",
    "    \"train\":      pick_bace_file(\"train\"),\n",
    "    \"validation\": pick_bace_file(\"validation\"),\n",
    "    \"test\":       pick_bace_file(\"test\"),\n",
    "}\n",
    "\n",
    "# 3) JSON을 \"무슨 형태든\" 리스트[dict]로 변환\n",
    "def load_json_flex(local_path):\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    # 3-1) jsonlines 시도\n",
    "    try:\n",
    "        lines = [ln for ln in raw.splitlines() if ln.strip()]\n",
    "        objs = [json.loads(ln) for ln in lines]\n",
    "        if isinstance(objs, list) and objs and isinstance(objs[0], dict):\n",
    "            return objs\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3-2) 일반 JSON 로드\n",
    "    data = json.loads(raw.decode(\"utf-8\"))\n",
    "\n",
    "    # list[dict]\n",
    "    if isinstance(data, list):\n",
    "        if data and isinstance(data[0], dict):\n",
    "            return data\n",
    "\n",
    "    # dict -> 안에 list[dict]인 키 찾기\n",
    "    if isinstance(data, dict):\n",
    "        for k, v in data.items():\n",
    "            if isinstance(v, list) and v and isinstance(v[0], dict):\n",
    "                return v\n",
    "        # dict of columns (각 key가 리스트)\n",
    "        col_keys = [k for k, v in data.items() if isinstance(v, list)]\n",
    "        if col_keys:\n",
    "            n = max(len(data[k]) for k in col_keys)\n",
    "            rows = []\n",
    "            for i in range(n):\n",
    "                row = {}\n",
    "                for k in col_keys:\n",
    "                    arr = data[k]\n",
    "                    if i < len(arr):\n",
    "                        row[k] = arr[i]\n",
    "                rows.append(row)\n",
    "            return rows\n",
    "\n",
    "    raise ValueError(f\"지원하지 않는 JSON 구조: {local_path}\")\n",
    "\n",
    "# 4) 샘플에서 SELFIES/label 컬럼 뽑아 표준화\n",
    "def to_biot5_bace_df(examples):\n",
    "    # examples: list of dict\n",
    "    df = pd.DataFrame(examples)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    selfies_col = cols.get(\"selfies\") or cols.get(\"<selfies>\") or cols.get(\"input\") \\\n",
    "                  or cols.get(\"raw_input\") or cols.get(\"x\") or cols.get(\"mol\")\n",
    "    label_col   = cols.get(\"label\") or cols.get(\"labels\") or cols.get(\"output\") \\\n",
    "                  or cols.get(\"y\") or cols.get(\"target\")\n",
    "\n",
    "    if selfies_col is None or label_col is None:\n",
    "        raise ValueError(f\"SELFIES/label 컬럼 식별 실패: {list(df.columns)}\")\n",
    "\n",
    "    out = df.rename(columns={selfies_col: \"SELFIES\", label_col: \"label\"})[[\"SELFIES\", \"label\"]]\n",
    "\n",
    "    # label을 \"True\"/\"False\" 문자열로 정규화\n",
    "    def to_tf(v):\n",
    "        if isinstance(v, str):\n",
    "            t = v.strip().lower()\n",
    "            if t in [\"true\",\"yes\",\"y\",\"1\"]: return \"True\"\n",
    "            if t in [\"false\",\"no\",\"n\",\"0\"]: return \"False\"\n",
    "        try:\n",
    "            return \"True\" if float(v) > 0 else \"False\"\n",
    "        except Exception:\n",
    "            return \"True\" if str(v).strip().lower() == \"true\" else \"False\"\n",
    "    out[\"label\"] = out[\"label\"].map(to_tf)\n",
    "\n",
    "    # SELFIES에 <SELFIES> 태그가 이미 있다면 그대로, 없으면 그대로 둡니다.\n",
    "    # (네 파이프라인에서 wrap 단계에서 태그를 추가하므로 여기선 추가/삭제 안 함)\n",
    "\n",
    "    # 결측/이상치 행 제거(선택)\n",
    "    out = out.dropna(subset=[\"SELFIES\", \"label\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# 5) 파일 다운로드 → 파싱 → CSV 저장\n",
    "for split, rel_path in paths.items():\n",
    "    local = hf_hub_download(REPO, rel_path, repo_type=\"dataset\")\n",
    "    ex = load_json_flex(local)\n",
    "    df = to_biot5_bace_df(ex)\n",
    "    csv_name = f\"BioT5_bace_{'valid' if split=='validation' else split}.csv\"\n",
    "    df.to_csv(os.path.join(OUT_DIR, csv_name), index=False)\n",
    "    print(split, rel_path, \"->\", csv_name)\n",
    "\n",
    "print(\"Done. CSVs saved to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26778ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bace = pd.read_csv(os.path.join(RAW_DATA_ROOT, \"raw/BioT5_bace_train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d34b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bace.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d543a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"QizhiPei/BioT5_finetune_dataset\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U huggingface_hub pandas selfies\n",
    "\n",
    "from huggingface_hub import list_repo_files, hf_hub_download\n",
    "import pandas as pd\n",
    "import json, os, re\n",
    "\n",
    "# ===== 설정 =====\n",
    "REPO = \"QizhiPei/BioT5_finetune_dataset\"\n",
    "OUT_DIR = \"/appdataset/train/raw\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ===== 레포 파일 나열 =====\n",
    "files = list_repo_files(REPO, repo_type=\"dataset\")\n",
    "\n",
    "# 우선순위: tasks_plus > tasks > splits_plus > splits\n",
    "PREFS = [\"tasks_plus/\", \"tasks/\", \"splits_plus/\", \"splits/\"]\n",
    "\n",
    "def pick_bace_file(split: str) -> str:\n",
    "    split_alias = [\"validation\", \"valid\"] if split == \"validation\" else [split]\n",
    "    best = None\n",
    "    best_key = (999, 10**9)\n",
    "    for f in files:\n",
    "        fl = f.lower()\n",
    "        if not fl.endswith(\".json\"):\n",
    "            continue\n",
    "        if \"bace\" not in fl:\n",
    "            continue\n",
    "        if not any(sa in fl for sa in split_alias):\n",
    "            continue\n",
    "        pref_idx = next((i for i, p in enumerate(PREFS) if p in f), 999)\n",
    "        key = (pref_idx, len(f))\n",
    "        if key < best_key:\n",
    "            best, best_key = f, key\n",
    "    if best is None:\n",
    "        raise FileNotFoundError(f\"BACE {split} JSON을 찾지 못했습니다. repo={REPO}\")\n",
    "    return best\n",
    "\n",
    "paths = {\n",
    "    \"train\"     : pick_bace_file(\"train\"),\n",
    "    \"validation\": pick_bace_file(\"validation\"),\n",
    "    \"test\"      : pick_bace_file(\"test\"),\n",
    "}\n",
    "\n",
    "# ===== 유틸 =====\n",
    "def load_json_flex(local_path: str):\n",
    "    \"\"\"무슨 형태든 list[dict](샘플 리스트)로 반환. Instances가 있으면 평탄화.\"\"\"\n",
    "    with open(local_path, \"rb\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    # 1) JSON Lines 시도\n",
    "    try:\n",
    "        lines = [ln for ln in raw.splitlines() if ln.strip()]\n",
    "        objs = [json.loads(ln) for ln in lines]\n",
    "        if objs and isinstance(objs[0], dict):\n",
    "            # 혹시 각 줄이 dataset-level dict(Instances 포함)인 경우도 있음 → 아래에서 다시 평탄화\n",
    "            data = objs\n",
    "        else:\n",
    "            data = json.loads(raw.decode(\"utf-8\"))\n",
    "    except Exception:\n",
    "        data = json.loads(raw.decode(\"utf-8\"))\n",
    "\n",
    "    # ---- 평탄화 로직 ----\n",
    "    def flatten_instances(x):\n",
    "        # x가 dict이면\n",
    "        if isinstance(x, dict):\n",
    "            if \"Instances\" in x and isinstance(x[\"Instances\"], list):\n",
    "                return x[\"Instances\"]\n",
    "            # dict 내부에서 list[dict] 탐색\n",
    "            for v in x.values():\n",
    "                if isinstance(v, list) and (not v or isinstance(v[0], dict)):\n",
    "                    return v\n",
    "            return [x]\n",
    "        # x가 list면\n",
    "        if isinstance(x, list):\n",
    "            # 리스트 요소가 dataset-level dict(=Instances를 가진)라면 전부 모아 평탄화\n",
    "            if x and isinstance(x[0], dict) and \"Instances\" in x[0]:\n",
    "                out = []\n",
    "                for d in x:\n",
    "                    if \"Instances\" in d and isinstance(d[\"Instances\"], list):\n",
    "                        out.extend(d[\"Instances\"])\n",
    "                return out\n",
    "            # 이미 list[dict](샘플)인 경우\n",
    "            return x\n",
    "        # 그 외 타입은 실패\n",
    "        raise ValueError(\"지원하지 않는 JSON 구조\")\n",
    "\n",
    "    return flatten_instances(data)\n",
    "\n",
    "# SELFIES만 뽑기\n",
    "def extract_selfies(raw: str) -> str | None:\n",
    "    if pd.isna(raw):\n",
    "        return None\n",
    "    s = str(raw)\n",
    "\n",
    "    m = re.search(r\"SELFIES:\\s*<bom>(.*)</eom>\", s, flags=re.DOTALL)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    m = re.search(r\"<bom>(.*)</eom>\", s, flags=re.DOTALL)\n",
    "    if m:\n",
    "        return m.group(1).strip()\n",
    "    for line in s.splitlines():\n",
    "        if \"SELFIES:\" in line:\n",
    "            line = line.split(\"SELFIES:\", 1)[1].strip()\n",
    "            line = re.sub(r\"^<bom>\\s*|\\s*</eom>$\", \"\", line).strip()\n",
    "            return line\n",
    "    return s.strip()\n",
    "\n",
    "def normalize_label(v) -> str:\n",
    "    if isinstance(v, str):\n",
    "        t = v.strip().lower()\n",
    "        if t in (\"true\",\"yes\",\"y\",\"1\"): return \"True\"\n",
    "        if t in (\"false\",\"no\",\"n\",\"0\"): return \"False\"\n",
    "    try:\n",
    "        return \"True\" if float(v) > 0 else \"False\"\n",
    "    except Exception:\n",
    "        return \"True\" if str(v).strip().lower() == \"true\" else \"False\"\n",
    "\n",
    "# 선택: SELFIES 유효성 검사\n",
    "try:\n",
    "    import selfies as sf\n",
    "    def is_valid_selfies(s: str) -> bool:\n",
    "        try:\n",
    "            _ = sf.decoder(s)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "except Exception:\n",
    "    def is_valid_selfies(s: str) -> bool:\n",
    "        return isinstance(s, str) and len(s) > 0\n",
    "\n",
    "def to_biot5_df(examples: list[dict]) -> pd.DataFrame:\n",
    "    \"\"\"Instances 항목 안의 input/output(대소문자 가리지 않음) → SELFIES/label로 정규화\"\"\"\n",
    "    rows = []\n",
    "    for ex in examples:\n",
    "        if not isinstance(ex, dict):\n",
    "            continue\n",
    "        # 입력/출력 키 탐색(대소문자 불문 + 다양한 별칭)\n",
    "        kl = {k.lower(): k for k in ex.keys()}\n",
    "        in_key = next((kl[k] for k in (\"input\",\"raw_input\",\"selfies\",\"x\",\"mol\") if k in kl), None)\n",
    "        out_key = next((kl[k] for k in (\"output\",\"label\",\"labels\",\"y\",\"target\",\"answer\") if k in kl), None)\n",
    "\n",
    "        # Natural-Instructions 스타일로 'input'/'output'이 또 내부 dict에 들어가 있는 경우 처리\n",
    "        cand = ex.get(in_key) if in_key else None\n",
    "        if isinstance(cand, dict):\n",
    "            # 흔한 후보\n",
    "            in_key2 = next((k for k in cand.keys() if k.lower() in (\"input\",\"raw_input\",\"selfies\",\"x\",\"mol\",\"text\")), None)\n",
    "            input_val = cand.get(in_key2)\n",
    "        else:\n",
    "            input_val = cand\n",
    "\n",
    "        cand = ex.get(out_key) if out_key else None\n",
    "        if isinstance(cand, dict):\n",
    "            out_key2 = next((k for k in cand.keys() if k.lower() in (\"output\",\"label\",\"labels\",\"y\",\"target\",\"answer\",\"text\")), None)\n",
    "            output_val = cand.get(out_key2)\n",
    "        else:\n",
    "            output_val = cand\n",
    "\n",
    "        if input_val is None or output_val is None:\n",
    "            # 못 찾았으면 스킵\n",
    "            continue\n",
    "\n",
    "        # output이 리스트/딕트일 수 있음 → 문자열로 축약\n",
    "        if isinstance(output_val, list):\n",
    "            output_val = output_val[0] if output_val else \"\"\n",
    "        if isinstance(output_val, dict):\n",
    "            output_val = next((v for v in output_val.values() if isinstance(v, (str,int,float,bool))), str(output_val))\n",
    "\n",
    "        rows.append({\"SELFIES\": extract_selfies(str(input_val)), \"label\": normalize_label(output_val)})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    # 무효 SELFIES 제거\n",
    "    mask = df[\"SELFIES\"].notna() & df[\"SELFIES\"].map(is_valid_selfies)\n",
    "    if (~mask).sum():\n",
    "        print(f\"[경고] 유효하지 않은 SELFIES {(~mask).sum()}개 제거\")\n",
    "    df = df[mask].reset_index(drop=True)\n",
    "    return df[[\"SELFIES\",\"label\"]]\n",
    "\n",
    "# ===== 다운로드 → 파싱 → CSV 저장 =====\n",
    "for split, rel in paths.items():\n",
    "    local_path = hf_hub_download(REPO, rel, repo_type=\"dataset\")\n",
    "    ex = load_json_flex(local_path)\n",
    "    df = to_biot5_df(ex)\n",
    "    fname = f\"BioT5_bace_{'valid' if split=='validation' else split}.csv\"\n",
    "    out_path = os.path.join(OUT_DIR, fname)\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"{split:11s} {rel}  ->  {out_path}  (rows={len(df)})\")\n",
    "\n",
    "print(\"완료! 이제 다음 분기가 그대로 동작합니다:\")\n",
    "print(f\"pd.read_csv(os.path.join('<raw_data_root>', 'raw', 'BioT5_bace_train.csv'))\")\n",
    "print(f\"pd.read_csv(os.path.join('<raw_data_root>', 'raw', 'BioT5_bace_valid.csv'))\")\n",
    "print(f\"pd.read_csv(os.path.join('<raw_data_root>', 'raw', 'BioT5_bace_test.csv'))\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c01b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U huggingface_hub pandas\n",
    "\n",
    "import os, re, json, textwrap\n",
    "import pandas as pd\n",
    "from huggingface_hub import list_repo_files, hf_hub_download\n",
    "\n",
    "REPO = \"QizhiPei/BioT5_finetune_dataset\"\n",
    "RAW_DATA_ROOT = \"/appdataset/train\"          # 네 환경에 맞게\n",
    "RAW_DIR = os.path.join(RAW_DATA_ROOT, \"raw\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1) BACE JSON 경로 선택 (tasks_plus > tasks > splits_plus > splits 우선)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "files = list_repo_files(REPO, repo_type=\"dataset\")\n",
    "PREFS = [\"tasks_plus/\", \"tasks/\", \"splits_plus/\", \"splits/\"]\n",
    "\n",
    "def pick_bace_file(split: str) -> str:\n",
    "    split_alias = [\"validation\", \"valid\"] if split == \"validation\" else [split]\n",
    "    best, best_key = None, (999, 10**9)\n",
    "    for f in files:\n",
    "        fl = f.lower()\n",
    "        if not fl.endswith(\".json\"): continue\n",
    "        if \"bace\" not in fl: continue\n",
    "        if not any(sa in fl for sa in split_alias): continue\n",
    "        pref_idx = next((i for i,p in enumerate(PREFS) if p in f), 999)\n",
    "        key = (pref_idx, len(f))\n",
    "        if key < best_key:\n",
    "            best, best_key = f, key\n",
    "    if best is None:\n",
    "        raise FileNotFoundError(f\"[bace] '{split}' 파일을 찾지 못했습니다.\")\n",
    "    return best\n",
    "\n",
    "REL = {\n",
    "    \"train\":      pick_bace_file(\"train\"),       # ex) tasks_plus/task31_bace_molnet_train.json\n",
    "    \"validation\": pick_bace_file(\"validation\"),  # ex) tasks_plus/task32_bace_molnet_valid.json\n",
    "    \"test\":       pick_bace_file(\"test\"),        # ex) tasks_plus/task33_bace_molnet_test.json\n",
    "}\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2) JSON 로딩 & Instances 평탄화\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def load_instances_any(json_path: str):\n",
    "    with open(json_path, \"rb\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    # jsonl 시도\n",
    "    try:\n",
    "        lines = [ln for ln in raw.splitlines() if ln.strip()]\n",
    "        objs = [json.loads(ln) for ln in lines]\n",
    "        data = objs if (objs and isinstance(objs[0], dict)) else json.loads(raw.decode(\"utf-8\"))\n",
    "    except Exception:\n",
    "        data = json.loads(raw.decode(\"utf-8\"))\n",
    "\n",
    "    # Instances 평탄화\n",
    "    if isinstance(data, dict):\n",
    "        if \"Instances\" in data and isinstance(data[\"Instances\"], list):\n",
    "            return data[\"Instances\"]\n",
    "        for v in data.values():\n",
    "            if isinstance(v, list) and (not v or isinstance(v[0], dict)):\n",
    "                return v\n",
    "        return [data]\n",
    "    if isinstance(data, list):\n",
    "        if data and isinstance(data[0], dict) and \"Instances\" in data[0]:\n",
    "            out = []\n",
    "            for d in data:\n",
    "                if \"Instances\" in d and isinstance(d[\"Instances\"], list):\n",
    "                    out.extend(d[\"Instances\"])\n",
    "            return out\n",
    "        return data\n",
    "    return []\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3) SELFIES/label 추출 (Mol-LLM에서 바로 쓰는 포맷)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "SELFIES_RE = re.compile(r\"SELFIES:\\s*<bom>(.*?)</eom>\", re.DOTALL)\n",
    "\n",
    "def extract_selfies_from_input(s: str) -> str | None:\n",
    "    \"\"\"'IUPAC: ...\\\\nSELFIES: <bom> ... </eom>' 형태에서 SELFIES 본문만 추출\"\"\"\n",
    "    if s is None: return None\n",
    "    m = SELFIES_RE.search(str(s))\n",
    "    if m:\n",
    "        selfies = m.group(1).strip()\n",
    "        # 내부 개행/따옴표 등은 CSV가 알아서 이스케이프하므로 그대로 둠\n",
    "        return selfies\n",
    "    # 백업: <bom>...</eom>만 있는 경우도 대비\n",
    "    m = re.search(r\"<bom>(.*?)</eom>\", str(s), re.DOTALL)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "def normalize_label(v) -> str:\n",
    "    \"\"\"Yes./No. → True/False 문자열\"\"\"\n",
    "    if isinstance(v, list) and v:\n",
    "        v = v[0]\n",
    "    t = str(v).strip().lower()\n",
    "    if \"yes\" in t or \"true\" in t:  return \"True\"\n",
    "    if \"no\" in t  or \"false\" in t: return \"False\"\n",
    "    # 숫자 등은 0/1 해석\n",
    "    try:\n",
    "        return \"True\" if float(t) > 0 else \"False\"\n",
    "    except Exception:\n",
    "        # 안전망: yes/no가 없으면 False로\n",
    "        return \"False\"\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4) 원본 JSON 보관 + CSV 생성 + 프리뷰\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "RAW_KEEP_DIR = os.path.join(RAW_DIR, \"biot5_bace\")  # 원본 저장 루트\n",
    "os.makedirs(RAW_KEEP_DIR, exist_ok=True)\n",
    "\n",
    "OUT_CSV = {\n",
    "    \"train\":      os.path.join(RAW_DIR, \"BioT5_bace_train.csv\"),\n",
    "    \"validation\": os.path.join(RAW_DIR, \"BioT5_bace_valid.csv\"),\n",
    "    \"test\":       os.path.join(RAW_DIR, \"BioT5_bace_test.csv\"),\n",
    "}\n",
    "\n",
    "for split, rel in REL.items():\n",
    "    # 4-1) HF 캐시에서 다운로드 & 원본 저장\n",
    "    cached = hf_hub_download(REPO, rel, repo_type=\"dataset\")\n",
    "    keep_path = os.path.join(RAW_KEEP_DIR, rel)  # 폴더 구조 유지\n",
    "    os.makedirs(os.path.dirname(keep_path), exist_ok=True)\n",
    "    with open(cached, \"rb\") as src, open(keep_path, \"wb\") as dst:\n",
    "        dst.write(src.read())\n",
    "    print(f\"[raw saved] {keep_path}\")\n",
    "\n",
    "    # 4-2) Instances -> SELFIES/label DataFrame\n",
    "    instances = load_instances_any(keep_path)\n",
    "    rows = []\n",
    "    for ex in instances:\n",
    "        if not isinstance(ex, dict): continue\n",
    "        # input/output 키 찾기(대소문자 무시)\n",
    "        kl = {k.lower(): k for k in ex.keys()}\n",
    "        in_key  = next((kl[x] for x in (\"input\",\"raw_input\") if x in kl), None)\n",
    "        out_key = next((kl[x] for x in (\"output\",\"label\",\"labels\") if x in kl), None)\n",
    "        if in_key is None or out_key is None: \n",
    "            continue\n",
    "        selfies = extract_selfies_from_input(ex[in_key])\n",
    "        label   = normalize_label(ex[out_key])\n",
    "        if selfies is None:\n",
    "            continue\n",
    "        rows.append({\"SELFIES\": selfies, \"label\": label})\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"SELFIES\",\"label\"])\n",
    "    out_csv = OUT_CSV[\"validation\" if split==\"validation\" else split]\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[csv saved] {out_csv} (rows={len(df)})\")\n",
    "\n",
    "    # 4-3) 프리뷰 5개만 출력\n",
    "    print(f\"\\n[{split}] preview:\")\n",
    "    for i, r in df.head(5).iterrows():\n",
    "        s = textwrap.shorten(r['SELFIES'].replace(\"\\n\",\"\\\\n\"), width=120, placeholder=\" …\")\n",
    "        print(f\"  #{i+1}: SELFIES={s} | label={r['label']}\")\n",
    "    print(\"-\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('/appdataset/train/raw/tasks_plus/task31_bace_molnet_train.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1368ea34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['task', 'x', 'edge_index', 'edge_attr', 'additional_x', 'additional_edge_index', 'additional_edge_attr', 'input_mol_string', 'prompt_text', 'target_text'],\n",
       "    num_rows: 58757\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "test_set = load_from_disk('/appdataset/test')\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf1eedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alchemy_homo',\n",
       " 'alchemy_homo_lumo_gap',\n",
       " 'alchemy_lumo',\n",
       " 'aqsol-logS',\n",
       " 'bace',\n",
       " 'chebi-20-mol2text',\n",
       " 'chebi-20-text2mol',\n",
       " 'forward_reaction_prediction',\n",
       " 'orderly-forward_reaction_prediction',\n",
       " 'orderly-retrosynthesis',\n",
       " 'presto-forward_reaction_prediction',\n",
       " 'presto-retrosynthesis',\n",
       " 'qm9_homo',\n",
       " 'qm9_homo_lumo_gap',\n",
       " 'qm9_lumo',\n",
       " 'reagent_prediction',\n",
       " 'retrosynthesis',\n",
       " 'smol-forward_synthesis',\n",
       " 'smol-molecule_captioning',\n",
       " 'smol-molecule_generation',\n",
       " 'smol-property_prediction-bbbp',\n",
       " 'smol-property_prediction-clintox',\n",
       " 'smol-property_prediction-esol',\n",
       " 'smol-property_prediction-hiv',\n",
       " 'smol-property_prediction-lipo',\n",
       " 'smol-property_prediction-sider',\n",
       " 'smol-retrosynthesis']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(test_set.unique('task'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bfee39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'alchemy_homo',\n",
       " 'x': [[5, 0, 4, 5, 3, 0, 2, 0, 0],\n",
       "  [7, 0, 2, 5, 0, 0, 2, 0, 0],\n",
       "  [5, 2, 4, 5, 1, 0, 2, 0, 1],\n",
       "  [5, 2, 4, 5, 1, 0, 2, 0, 1],\n",
       "  [5, 0, 4, 5, 2, 0, 2, 0, 1],\n",
       "  [5, 2, 4, 5, 1, 0, 2, 0, 1],\n",
       "  [5, 0, 3, 5, 1, 0, 1, 0, 1],\n",
       "  [5, 0, 3, 5, 0, 0, 1, 0, 1],\n",
       "  [5, 0, 4, 5, 3, 0, 2, 0, 0]],\n",
       " 'edge_index': [[0, 1, 1, 2, 2, 3, 2, 7, 3, 4, 3, 5, 4, 5, 5, 6, 6, 7, 7, 8],\n",
       "  [1, 0, 2, 1, 3, 2, 7, 2, 4, 3, 5, 3, 5, 4, 6, 5, 7, 6, 8, 7]],\n",
       " 'edge_attr': [[0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [1, 0, 0],\n",
       "  [1, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0]],\n",
       " 'additional_x': [[5, 0, 4, 5, 3, 0, 2, 0, 0],\n",
       "  [7, 0, 2, 5, 0, 0, 2, 0, 0],\n",
       "  [5, 2, 4, 5, 1, 0, 2, 0, 1],\n",
       "  [5, 2, 4, 5, 1, 0, 2, 0, 1],\n",
       "  [5, 0, 4, 5, 2, 0, 2, 0, 1],\n",
       "  [5, 2, 4, 5, 1, 0, 2, 0, 1],\n",
       "  [5, 0, 3, 5, 1, 0, 1, 0, 1],\n",
       "  [5, 0, 3, 5, 0, 0, 1, 0, 1],\n",
       "  [5, 0, 4, 5, 3, 0, 2, 0, 0]],\n",
       " 'additional_edge_index': [[0,\n",
       "   1,\n",
       "   1,\n",
       "   2,\n",
       "   2,\n",
       "   3,\n",
       "   2,\n",
       "   7,\n",
       "   3,\n",
       "   4,\n",
       "   3,\n",
       "   5,\n",
       "   4,\n",
       "   5,\n",
       "   5,\n",
       "   6,\n",
       "   6,\n",
       "   7,\n",
       "   7,\n",
       "   8],\n",
       "  [1, 0, 2, 1, 3, 2, 7, 2, 4, 3, 5, 3, 5, 4, 6, 5, 7, 6, 8, 7]],\n",
       " 'additional_edge_attr': [[0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [1, 0, 0],\n",
       "  [1, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0]],\n",
       " 'input_mol_string': '<SELFIES> [C][O][C@H1][C][Branch1][C][C][=C][C@H1][C][C@H1][Ring1][Ring1][Ring1][#Branch1] </SELFIES>',\n",
       " 'prompt_text': '<s>[INST] You are a helpful assistant for molecular chemistry, to address tasks including molecular property classification, molecular property regression, chemical reaction prediction, molecule captioning, molecule generation. \\n\\nPlease provide the HOMO energy value for this molecule: <SELFIES> [C][O][C@H1][C][Branch1][C][C][=C][C@H1][C][C@H1][Ring1][Ring1][Ring1][#Branch1] </SELFIES><GRAPH><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol></GRAPH>. [/INST] ',\n",
       " 'target_text': '<FLOAT> <|-|><|0|><|.|><|2|><|1|><|7|><|5|> </FLOAT> </s>'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a157c057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58757/58757 [00:15<00:00, 3700.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV 파일이 저장되었습니다: /appprompt/prompt.csv\n",
      "총 58757개의 instruction이 추출되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# [INST]로 시작해서 첫 줄바꿈 전까지 캡처 (앞에 <s> 있을 수도 있음)\n",
    "inst_pattern = re.compile(r'(?:<s>)?\\[INST\\]\\s*(.*?)\\r?\\n', re.DOTALL)\n",
    "\n",
    "instructions = []\n",
    "\n",
    "for x in tqdm(test_set):\n",
    "    sample = x.get('prompt_text', '')\n",
    "    match = inst_pattern.search(sample)\n",
    "    if match:\n",
    "        inst_text = match.group(1).strip()\n",
    "        instructions.append(inst_text)\n",
    "\n",
    "counts = Counter(instructions)\n",
    "\n",
    "output_path = '/appprompt/prompt.csv'\n",
    "with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['instruction', 'count'])\n",
    "    writer.writeheader()\n",
    "    for inst_text, count in counts.items():\n",
    "        writer.writerow({'instruction': inst_text, 'count': count})\n",
    "\n",
    "print(f\"✅ CSV 파일이 저장되었습니다: {output_path}\")\n",
    "print(f\"총 {len(instructions)}개의 instruction이 추출되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7969fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV 파일이 저장되었습니다: /appprompt/prompt.csv\n"
     ]
    }
   ],
   "source": [
    "result = results.sort(key=lambda x: (x['task'], -x['count']))\n",
    "with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['task', 'prompt_text', 'count'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)\n",
    "\n",
    "print(f\"✅ CSV 파일이 저장되었습니다: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f9b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
