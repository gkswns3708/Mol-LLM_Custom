{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3ddeb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hydra\n",
    "# from hydra import initialize, compose\n",
    "# from omegaconf import OmegaConf\n",
    "# import torch\n",
    "# import os\n",
    "\n",
    "# # 1. Hydra Ï¥àÍ∏∞Ìôî\n",
    "# hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "# # 2. Config Î°úÎìú\n",
    "# with initialize(version_base=None, config_path=\"configs\"):\n",
    "#     cfg = compose(config_name=\"test_CHJ.yaml\")\n",
    "\n",
    "# print(\"Original Config Structure Loaded.\")\n",
    "\n",
    "# # ==============================================================================\n",
    "# # [ÌïµÏã¨ Ìï¥Í≤∞Ï±Ö] Ï§ëÏ≤©Îêú ÏÑ§Ï†ï(Nested Config)ÏùÑ ÌèâÌèâÌïòÍ≤å Ìé¥Ï£ºÍ∏∞ (Flattening)\n",
    "# # ==============================================================================\n",
    "# # Î™®Îç∏ ÏΩîÎìúÍ∞Ä args.gnn_hidden_dim, args.num_beams Ï≤òÎüº Î∞îÎ°ú Ï†ëÍ∑ºÌïòÍ∏∞ ÎïåÎ¨∏Ïóê\n",
    "# # 'trainer'ÏôÄ 'gnn' ÏïàÏóê ÏûàÎäî ÏÑ§Ï†ïÎì§ÏùÑ ÏµúÏÉÅÏúÑ(root)Î°ú Í∫ºÎÇ¥ÏôÄ Ìï©Ï≥êÏïº Ìï©ÎãàÎã§.\n",
    "\n",
    "# OmegaConf.set_struct(cfg, False)  # Íµ¨Ï°∞ Î≥ÄÍ≤Ω ÌóàÏö© (Ïû†Í∏à Ìï¥Ï†ú)\n",
    "\n",
    "# # 1) Trainer ÏÑ§Ï†ï Í∫ºÎÇ¥Í∏∞ (num_beams, gen_max_len Îì± Ìï¥Í≤∞)\n",
    "# if \"trainer\" in cfg:\n",
    "#     print(\"Merging 'trainer' config into root...\")\n",
    "#     cfg = OmegaConf.merge(cfg, cfg.trainer)\n",
    "\n",
    "# # 2) GNN ÏÑ§Ï†ï Í∫ºÎÇ¥Í∏∞ (gnn_hidden_dim, gine, tokengt Îì± Ìï¥Í≤∞)\n",
    "# # ‚òÖ Ïù¥ Î∂ÄÎ∂ÑÏù¥ Ï∂îÍ∞ÄÎêòÏñ¥Ïïº 'gnn_hidden_dim' ÏóêÎü¨Í∞Ä ÏÇ¨ÎùºÏßëÎãàÎã§.\n",
    "# if \"gnn\" in cfg:\n",
    "#     print(\"Merging 'gnn' config into root...\")\n",
    "#     cfg = OmegaConf.merge(cfg, cfg.gnn)\n",
    "\n",
    "# # ==============================================================================\n",
    "# # [Í≤ΩÎ°ú ÏàòÏ†ï] GINE Checkpoint Í≤ΩÎ°ú Ïû¨ÏÑ§Ï†ï\n",
    "# # ==============================================================================\n",
    "# # Î≥ëÌï©(Merge) ÌõÑÏóêÎäî 'gine' ÏÑ§Ï†ïÏù¥ ÏµúÏÉÅÏúÑÏóê ÏúÑÏπòÌïòÍ≤å Îê©ÎãàÎã§ (cfg.gine).\n",
    "# # Îî∞ÎùºÏÑú cfg.gine.graph_encoder_ckpt Î•º ÏàòÏ†ïÌï¥Ïïº Ìï©ÎãàÎã§.\n",
    "\n",
    "# # Îã§Ïãú Íµ¨Ï°∞ Ïû†Í∏à (ÏïàÏ†ÑÏû•Ïπò)\n",
    "# OmegaConf.set_struct(cfg, True)\n",
    "\n",
    "# # ==============================================================================\n",
    "# # [Í≤ÄÏ¶ù] Ï£ºÏöî ÌÇ§Îì§Ïù¥ ÏµúÏÉÅÏúÑÏóê Ïûò ÎÇòÏôîÎäîÏßÄ ÌôïÏù∏\n",
    "# # ==============================================================================\n",
    "# print(\"-\" * 40)\n",
    "# print(f\"1. num_beams (from trainer): {cfg.get('num_beams', 'MISSING')}\") \n",
    "# print(f\"2. gnn_hidden_dim (from gnn): {cfg.get('gnn_hidden_dim', 'MISSING')}\")\n",
    "# print(f\"3. GINE Path: {cfg.gine.graph_encoder_ckpt if hasattr(cfg, 'gine') else 'MISSING'}\")\n",
    "# print(\"-\" * 40)\n",
    "\n",
    "# # ==============================================================================\n",
    "# # Î™®Îç∏ Î°úÎìú\n",
    "# # ==============================================================================\n",
    "# ckpt_path = \"/home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom/mol-llm.ckpt\"\n",
    "# correct_token_path = \"/home/jovyan/CHJ/Mol-LLM_Custom/model/selfies_dict.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e22a5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg_dict = {\n",
    "#     \"data\": {\n",
    "#         \"raw_data_root\": \"/home/jovyan/CHJ/Mol-LLM_Custom/dataset/real_train\",\n",
    "#         \"data_tag\": \"3.3M_0415\",\n",
    "#         \"tasks\": [\"reagent_prediction\"],\n",
    "#     },\n",
    "#     \"gnn\": {\n",
    "#         \"gnn_type\": \"gine_tokengt\",\n",
    "#         \"gnn_hidden_dim\": 1024,\n",
    "#         \"gine\": {\n",
    "#             \"gnn_hidden_dim\": 1024,\n",
    "#             \"gin_num_layers\": 5,\n",
    "#             \"drop_ratio\": 0.0,\n",
    "#             \"used_gnn_layer\": -1,\n",
    "#             \"gnn_jk\": \"last\",\n",
    "#             \"graph_encoder_ckpt\": \"/home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom/mol-llm.ckpt\",\n",
    "#             \"gnn_type\": \"gine\",\n",
    "#         },\n",
    "#         \"tokengt\": {\n",
    "#             \"input_feat_dim\": 9,\n",
    "#             \"gnn_hidden_dim\": 1024,\n",
    "#             \"num_layers\": 5,\n",
    "#             \"num_heads\": 8,\n",
    "#             \"method\": \"laplacian\",\n",
    "#             \"d_p\": 64,\n",
    "#             \"d_e\": 64,\n",
    "#             \"use_graph_token\": True,\n",
    "#             \"max_position_embeddings\": 102,\n",
    "#             \"graph_encoder_ckpt\": \"/home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom/mol-llm.ckpt\",\n",
    "#             \"gnn_type\": \"tokengt\",\n",
    "#         },\n",
    "#     },\n",
    "#     \"trainer\": {\n",
    "#         \"bert_hidden_dim\": 768,\n",
    "#         \"bert_name\": \"scibert\",\n",
    "#         \"cross_attention_freq\": 2,\n",
    "#         \"num_query_token\": 32,\n",
    "#         \"bert_num_hidden_layers\": 5,\n",
    "#         \"projector_type\": \"qformer\",\n",
    "#         \"llm_model\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "#         \"tune_llm\": \"lora\",\n",
    "#         \"peft_config\": None,\n",
    "#         \"peft_dir\": \"\",\n",
    "#         \"load_in_8bit\": False,\n",
    "#         \"lora_r\": 64,\n",
    "#         \"lora_alpha\": 32,\n",
    "#         \"lora_dropout\": 0.1,\n",
    "#         \"selfies_token_path\": \"Mol-LLM_Custom/model/selfies_dict.txt\",\n",
    "#         \"add_selfies_tokens\": True,\n",
    "#         \"prompt\": \"[START_I_SMILES]{}[END_I_SMILES]\",\n",
    "#         \"num_beams\": 1,\n",
    "#         \"strategy_name\": None,\n",
    "#         \"accelerator\": \"gpu\",\n",
    "#         \"devices\": \"0,1,2,3,4,5,6,7\",\n",
    "#         \"precision\": \"bf16-mixed\",\n",
    "#         \"max_steps\": -1,\n",
    "#         \"max_epochs\": 12,\n",
    "#         \"every_n_epochs\": 1,\n",
    "#         \"task\": None,\n",
    "#         \"logging_dir\": \"/home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom\",\n",
    "#         \"llava_pretraining\": 0,\n",
    "#         \"second_stage_start_epoch\": 4,\n",
    "#         \"num_workers\": 0,\n",
    "#         \"skip_sanity_check\": False,\n",
    "#         \"total_batch_size\": 512,\n",
    "#         \"batch_size\": 8,\n",
    "#         \"inference_batch_size\": 11,\n",
    "#         \"truncation\": 1,\n",
    "#         \"padding\": \"max_length\",\n",
    "#         \"max_length\": 512,\n",
    "#         \"inference_max_length\": 512,\n",
    "#         \"gen_max_len\": 256,\n",
    "#         \"min_len\": 8,\n",
    "#         \"apply_sequence_packing\": False,\n",
    "#         \"max_packing_size\": -1,\n",
    "#         \"weight_decay\": 0.05,\n",
    "#         \"min_lr\": 1e-05,\n",
    "#         \"init_lr\": 0.0001,\n",
    "#         \"warmup_lr\": 1e-05,\n",
    "#         \"warmup_epochs\": 0.25,\n",
    "#         \"scheduler\": \"linear_warmup_cosine_lr\",\n",
    "#         \"optimizer\": \"adamw\",\n",
    "#         \"log_every_n_steps\": 50,\n",
    "#         \"gradient_clip_val\": 0.5,\n",
    "#         \"val_check_interval\": 0.5,\n",
    "#         \"test_on_trainset\": False,\n",
    "#         \"mol_representation\": \"string+graph\",\n",
    "#         \"log_attn_score\": True,\n",
    "#         \"eval_modality_util\": None,\n",
    "#         \"tune_gnn\": True,\n",
    "#         \"train_molpo\": False,\n",
    "#         \"eval_molpo\": False,\n",
    "#         \"find_unused_parameters\": False,\n",
    "#         \"selfies_enumeration\": False,\n",
    "#         \"isomericSmiles\": False,\n",
    "#         \"canonical\": False,\n",
    "#         \"allHsExplicit\": False,\n",
    "#     },\n",
    "#     \"filename\": \"debugging\",\n",
    "#     \"seed\": 42,\n",
    "#     \"mode\": \"ft\",\n",
    "#     \"wandb_entity\": \"hj_ai\",\n",
    "#     \"wandb_project\": \"mol-llm\",\n",
    "#     \"wandb_log_freq\": 100,\n",
    "#     \"wandb_id\": None,\n",
    "#     \"debug\": False,\n",
    "#     \"ckpt_path\": None,\n",
    "#     \"pretrained_ckpt_path\": None,\n",
    "#     \"shuffle_selfies\": False,\n",
    "#     \"shuffle_graph\": False,\n",
    "#     \"process_disjoint\": True,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83444577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/miniconda3/envs/MolDA_CHJ/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'test_CHJ.yaml': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model...\n",
      "{'data': {'raw_data_root': '/home/jovyan/CHJ/Mol-LLM_Custom/dataset/real_train', 'data_tag': '3.3M_0415', 'tasks': ['reagent_prediction']}, 'gnn': {'gnn_type': 'gine_tokengt', 'gnn_hidden_dim': 1024, 'gine': {'gnn_hidden_dim': 1024, 'gin_num_layers': 5, 'drop_ratio': 0.0, 'used_gnn_layer': -1, 'gnn_jk': 'last', 'graph_encoder_ckpt': '/home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom/mol-llm.ckpt', 'gnn_type': 'gine'}, 'tokengt': {'input_feat_dim': 9, 'gnn_hidden_dim': 1024, 'num_layers': 5, 'num_heads': 8, 'method': 'laplacian', 'd_p': 64, 'd_e': 64, 'use_graph_token': True, 'max_position_embeddings': 102, 'graph_encoder_ckpt': '/home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom/mol-llm.ckpt', 'gnn_type': 'tokengt'}}, 'trainer': {'bert_hidden_dim': 768, 'bert_name': 'scibert', 'cross_attention_freq': 2, 'num_query_token': 32, 'bert_num_hidden_layers': 5, 'projector_type': 'qformer', 'llm_model': 'mistralai/Mistral-7B-Instruct-v0.3', 'tune_llm': 'lora', 'peft_config': None, 'peft_dir': '', 'load_in_8bit': False, 'lora_r': 64, 'lora_alpha': 32, 'lora_dropout': 0.1, 'selfies_token_path': 'Mol-LLM_Custom/model/selfies_dict.txt', 'add_selfies_tokens': True, 'prompt': '[START_I_SMILES]{}[END_I_SMILES]', 'num_beams': 1, 'strategy_name': None, 'accelerator': 'gpu', 'devices': '0,1,2,3,4,5,6,7', 'precision': 'bf16-mixed', 'max_steps': -1, 'max_epochs': 12, 'every_n_epochs': 1, 'task': None, 'logging_dir': '/home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom', 'llava_pretraining': 0, 'second_stage_start_epoch': 4, 'num_workers': 0, 'skip_sanity_check': False, 'total_batch_size': 512, 'batch_size': 8, 'inference_batch_size': 11, 'truncation': 1, 'padding': 'max_length', 'max_length': 512, 'inference_max_length': 512, 'gen_max_len': 256, 'min_len': 8, 'apply_sequence_packing': False, 'max_packing_size': -1, 'weight_decay': 0.05, 'min_lr': 1e-05, 'init_lr': 0.0001, 'warmup_lr': 1e-05, 'warmup_epochs': 0.25, 'scheduler': 'linear_warmup_cosine_lr', 'optimizer': 'adamw', 'log_every_n_steps': 50, 'gradient_clip_val': 0.5, 'val_check_interval': 0.5, 'test_on_trainset': False, 'mol_representation': 'string+graph', 'log_attn_score': True, 'eval_modality_util': None, 'tune_gnn': True, 'train_molpo': False, 'eval_molpo': False, 'find_unused_parameters': False, 'selfies_enumeration': False, 'isomericSmiles': False, 'canonical': False, 'allHsExplicit': False}, 'filename': 'debugging', 'seed': 42, 'mode': 'ft', 'wandb_entity': 'hj_ai', 'wandb_project': 'mol-llm', 'wandb_log_freq': 100, 'wandb_id': None, 'debug': False, 'ckpt_path': None, 'pretrained_ckpt_path': None, 'shuffle_selfies': False, 'shuffle_graph': False, 'process_disjoint': True, 'bert_hidden_dim': 768, 'bert_name': 'scibert', 'cross_attention_freq': 2, 'num_query_token': 32, 'bert_num_hidden_layers': 5, 'projector_type': 'qformer', 'llm_model': 'mistralai/Mistral-7B-Instruct-v0.3', 'tune_llm': 'lora', 'peft_config': None, 'peft_dir': '', 'load_in_8bit': False, 'lora_r': 64, 'lora_alpha': 32, 'lora_dropout': 0.1, 'selfies_token_path': '/home/jovyan/CHJ/Mol-LLM_Custom/model/selfies_dict.txt', 'add_selfies_tokens': True, 'prompt': '[START_I_SMILES]{}[END_I_SMILES]', 'num_beams': 1, 'strategy_name': None, 'accelerator': 'gpu', 'devices': '0,1,2,3,4,5,6,7', 'precision': 'bf16-mixed', 'max_steps': -1, 'max_epochs': 12, 'every_n_epochs': 1, 'task': None, 'logging_dir': '/home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom', 'llava_pretraining': 0, 'second_stage_start_epoch': 4, 'num_workers': 0, 'skip_sanity_check': False, 'total_batch_size': 512, 'batch_size': 8, 'inference_batch_size': 11, 'truncation': 1, 'padding': 'max_length', 'max_length': 512, 'inference_max_length': 512, 'gen_max_len': 256, 'min_len': 8, 'apply_sequence_packing': False, 'max_packing_size': -1, 'weight_decay': 0.05, 'min_lr': 1e-05, 'init_lr': 0.0001, 'warmup_lr': 1e-05, 'warmup_epochs': 0.25, 'scheduler': 'linear_warmup_cosine_lr', 'optimizer': 'adamw', 'log_every_n_steps': 50, 'gradient_clip_val': 0.5, 'val_check_interval': 0.5, 'test_on_trainset': False, 'mol_representation': 'string+graph', 'log_attn_score': True, 'eval_modality_util': None, 'tune_gnn': True, 'train_molpo': False, 'eval_molpo': False, 'find_unused_parameters': False, 'selfies_enumeration': False, 'isomericSmiles': False, 'canonical': False, 'allHsExplicit': False, 'gnn_type': 'gine_tokengt', 'gnn_hidden_dim': 1024, 'gine': {'gnn_hidden_dim': 1024, 'gin_num_layers': 5, 'drop_ratio': 0.0, 'used_gnn_layer': -1, 'gnn_jk': 'last', 'graph_encoder_ckpt': '/home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom/mol-llm.ckpt', 'gnn_type': 'gine'}, 'tokengt': {'input_feat_dim': 9, 'gnn_hidden_dim': 1024, 'num_layers': 5, 'num_heads': 8, 'method': 'laplacian', 'd_p': 64, 'd_e': 64, 'use_graph_token': True, 'max_position_embeddings': 102, 'graph_encoder_ckpt': '/home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom/mol-llm.ckpt', 'gnn_type': 'tokengt'}}  - args\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /home/jovyan/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71/tokenizer.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/jovyan/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/jovyan/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /home/jovyan/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71/tokenizer.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2944 selfies tokens to the tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/jovyan/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71/model.safetensors.index.json\n",
      "Instantiating MistralForCausalLM_custom model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  1.38s/it]\n",
      "All model checkpoint weights were used when initializing MistralForCausalLM_custom.\n",
      "\n",
      "All the weights of MistralForCausalLM_custom were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM_custom for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/jovyan/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/c170c708c41dac9275d15a8fff4eca08d52bab71/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 35745. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 167,772,160 || all params: 7,440,183,296 || trainable%: 2.2549\n",
      "/home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom/mol-llm.ckpt -args.gine.graph_encoder_ckpt\n",
      "load graph encoder from /home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom/mol-llm.ckpt\n",
      "load graph encoder from /home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom/mol-llm.ckpt\n",
      "bert load scibert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/config.json\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertLMHeadModel: ['bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.intermediate_query.dense.bias', 'bert.encoder.layer.0.intermediate_query.dense.weight', 'bert.encoder.layer.0.output_query.LayerNorm.bias', 'bert.encoder.layer.0.output_query.LayerNorm.weight', 'bert.encoder.layer.0.output_query.dense.bias', 'bert.encoder.layer.0.output_query.dense.weight', 'bert.encoder.layer.1.intermediate_query.dense.bias', 'bert.encoder.layer.1.intermediate_query.dense.weight', 'bert.encoder.layer.1.output_query.LayerNorm.bias', 'bert.encoder.layer.1.output_query.LayerNorm.weight', 'bert.encoder.layer.1.output_query.dense.bias', 'bert.encoder.layer.1.output_query.dense.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.intermediate_query.dense.bias', 'bert.encoder.layer.2.intermediate_query.dense.weight', 'bert.encoder.layer.2.output_query.LayerNorm.bias', 'bert.encoder.layer.2.output_query.LayerNorm.weight', 'bert.encoder.layer.2.output_query.dense.bias', 'bert.encoder.layer.2.output_query.dense.weight', 'bert.encoder.layer.3.intermediate_query.dense.bias', 'bert.encoder.layer.3.intermediate_query.dense.weight', 'bert.encoder.layer.3.output_query.LayerNorm.bias', 'bert.encoder.layer.3.output_query.LayerNorm.weight', 'bert.encoder.layer.3.output_query.dense.bias', 'bert.encoder.layer.3.output_query.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.4.intermediate_query.dense.bias', 'bert.encoder.layer.4.intermediate_query.dense.weight', 'bert.encoder.layer.4.output_query.LayerNorm.bias', 'bert.encoder.layer.4.output_query.LayerNorm.weight', 'bert.encoder.layer.4.output_query.dense.bias', 'bert.encoder.layer.4.output_query.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "loading file vocab.txt from cache at /home/jovyan/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading file tokenizer.json from cache at None\n",
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--allenai--scibert_scivocab_uncased/snapshots/24f92d32b1bfb0bcaf9ab193ff3ad01e87732fc1/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"allenai/scibert_scivocab_uncased\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "/home/jovyan/miniconda3/envs/MolDA_CHJ/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_257761/3572432358.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Repair Loader] Loading weights from /home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom/mol-llm.ckpt...\n",
      "[Repair Loader] Checkpoint weights loaded.\n",
      "   Input Embeddings Shape: torch.Size([35745, 4096])\n",
      "   Output LM Head Shape  : torch.Size([35745, 4096])\n",
      "üîß [FIX] Copying Embeddings to LM Head (Fixing Garbage Output)...\n",
      "‚úÖ [SUCCESS] LM Head fixed!\n",
      "Moving model to cuda and converting to bfloat16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/miniconda3/envs/MolDA_CHJ/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating response...\n",
      "\n",
      "==================================================\n",
      "Prediction Result:\n",
      "[C@@H1] [125Te] [6Li] [33ClH1] [\\14C@H1] [\\14C@H1] [Rf] [Rf] [Rf] [82Se] [/ClH1+1] [82Se] [82Se] [=La] [115Cd] [246Pu] [38K] [146Sm] [/ClH1+1] [Tc] [11B-1] [11B-1] [230U] [47V] [82Se] [194Os] [194Os] [145Pm] [123I-1] [233U+4] [233U+4] [82Se] [82Se] [/S@] [TlH1] [N@@] [N@@] [154Sm] [N@@] [6Li+1] [Sn@@] [171Yb] [=FeH1] [\\PH1+1] [171Tm] [89Rb] [89Rb] [154Sm] [79Rb] [82Se] [Ho] [Cr-1] [194Os] [110Sn] [236U] [152Gd] [152Gd] [152Gd] [152Gd] [152Gd] [152Gd] [=Fe] [137Xe] [137Xe] [=Fe] [246Pu] [=Fe] [65Zn] [194Os] [Te+1] [Te+1] [136Xe] [33ClH1] [33ClH1] [33ClH1] [=Fe] [81Rb] [150Eu] [185Ir] [#Ti] [82Se] [/Cl-1] [/Cl-1] [/Cl-1] [230U] [81Rb] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [194Os] [54Cr] [45K+1] [230U] [6Li+1] [40PH3] [40PH3] [/ClH1+1] [81Rb] [18CH1] [81Rb] [BH1-1] [#14C] [82Se] [82Se] [Co-3] [6Li+1] [246Pu] [33ClH1] [33ClH1] [127Sb+3] [127Sb+3] [11B-1] [\\Sn] [\\Sn-1] [82Se] [92Nb] [69Ga+3] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [\\Sn-1] [82Se] [45K+1] [33ClH1] [71Se] [71Se] [81Rb] [\\-Ring3] [Cl+2] [95Nb] [82Se] [82Se] [82Se] [Co-3] [Ba+1] [Ba+1] [87Rb] [EuH3] [152Gd] [/Sn] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se] [33ClH1] [AsH3+1] [O-2] [Ga] [=SbH1] [=SbH1] [=SbH1] [=SbH1] [=SbH1] [=SbH1] [=18CH2] [=18CH2] [82Se] [34Cl-1] [Ti-2] [AsH3+1] [249Cf] [22Na+1] [15NH4+1] [231U] [99Ru] [FeH4] [#Si] [11B-1] [209Tl] [=Th] <mol> [87Sr] [70Zn] [157Sm] [6Li+1] [71Se] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] [33ClH1] <mol> <mol> [/Sn] [114In+3] [=ReH1] [15NH4+1] [Ni+2] [Ni+2] [Ni+2] [I] [82Se] [82Se] [82Se] [82Se] [82Se] [82Se]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "import selfies as sf\n",
    "from ogb.utils import smiles2graph\n",
    "from torch_geometric.data import Data, Batch\n",
    "from model.blip2_stage3 import Blip2Stage3\n",
    "import logging\n",
    "\n",
    "# ==============================================================================\n",
    "# 0. Î™®Îç∏ Î°úÎìú Î∞è ÏàòÏà† Ìï®Ïàò (Weight Tying & Repair)\n",
    "# ==============================================================================\n",
    "def load_and_fix_model(model, ckpt_path):\n",
    "    print(f\"\\n[Repair Loader] Loading weights from {ckpt_path}...\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    state_dict = checkpoint[\"state_dict\"] if \"state_dict\" in checkpoint else checkpoint\n",
    "    \n",
    "    # 1. Í∞ÄÏ§ëÏπò Î°úÎìú (Í∏∞Ï°¥ Î∞©Ïãù)\n",
    "    model_state_dict = model.state_dict()\n",
    "    new_state_dict = {}\n",
    "    \n",
    "    # ÌÇ§ Îß§Ìïë Î∞è ÌïÑÌÑ∞ÎßÅ\n",
    "    for k, v in state_dict.items():\n",
    "        # Ï†ëÎëêÏÇ¨ Ï≤òÎ¶¨ (blip2model. Îì±)\n",
    "        target_key = k\n",
    "        if k not in model_state_dict:\n",
    "            if k.startswith(\"blip2model.\") and k[11:] in model_state_dict:\n",
    "                target_key = k[11:]\n",
    "            elif \"blip2model.\" + k in model_state_dict:\n",
    "                target_key = \"blip2model.\" + k\n",
    "        \n",
    "        if target_key in model_state_dict:\n",
    "            new_state_dict[target_key] = v\n",
    "\n",
    "    # Î°úÎìú Ïã§Ìñâ (Base ModelÏùò Frozen WeightÎäî Î¨¥Ïãú)\n",
    "    missing, _ = model.load_state_dict(new_state_dict, strict=False)\n",
    "    print(\"[Repair Loader] Checkpoint weights loaded.\")\n",
    "\n",
    "    # 2. [ÌïµÏã¨] LM Head ÏàòÏà† (Weight Tying)\n",
    "    # LoRA Î™®Îç∏ Íµ¨Ï°∞ÏÉÅ llm_model ÎÇ¥Î∂Ä ÍπäÏàôÌïú Í≥≥Ïóê Ï†ëÍ∑ºÌï¥Ïïº Ìï®\n",
    "    try:\n",
    "        # Í≤ΩÎ°ú Ï∞æÍ∏∞: model.blip2model.llm_model ...\n",
    "        llm = model.blip2model.llm_model\n",
    "        \n",
    "        # PeftModelÎ°ú Í∞êÏã∏Ï†∏ ÏûàÎäî Í≤ΩÏö∞ base_model ÏïàÏúºÎ°ú ÏßÑÏûÖ\n",
    "        if hasattr(llm, \"base_model\"):\n",
    "            base = llm.base_model.model \n",
    "        else:\n",
    "            base = llm.model\n",
    "\n",
    "        # Embed Tokens (ÏûÖÎ†•) ÏôÄ LM Head (Ï∂úÎ†•) Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        # Íµ¨Ï°∞: MistralForCausalLM -> model(MistralModel) -> embed_tokens\n",
    "        # Íµ¨Ï°∞: MistralForCausalLM -> lm_head\n",
    "        \n",
    "        # Ï†ïÌôïÌïú Î™®Îìà Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        if hasattr(base, \"model\") and hasattr(base.model, \"embed_tokens\"):\n",
    "            embed_tokens = base.model.embed_tokens\n",
    "        elif hasattr(base, \"embed_tokens\"):\n",
    "            embed_tokens = base.embed_tokens\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Cannot find embed_tokens module.\")\n",
    "            embed_tokens = None\n",
    "\n",
    "        if hasattr(base, \"lm_head\"):\n",
    "            lm_head = base.lm_head\n",
    "        elif hasattr(llm, \"lm_head\"): # PeftModel wrapper Î∞îÍπ•Ïóê ÏûàÏùÑ ÏàòÎèÑ ÏûàÏùå\n",
    "            lm_head = llm.lm_head\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Cannot find lm_head module.\")\n",
    "            lm_head = None\n",
    "\n",
    "        # Í∞ÄÏ§ëÏπò Î≥µÏÇ¨ (Tying)\n",
    "        if embed_tokens is not None and lm_head is not None:\n",
    "            print(f\"   Input Embeddings Shape: {embed_tokens.weight.shape}\")\n",
    "            print(f\"   Output LM Head Shape  : {lm_head.weight.shape}\")\n",
    "            \n",
    "            # ÌÅ¨Í∏∞Í∞Ä Í∞ôÎã§Î©¥ Í∞ÄÏ§ëÏπò Î≥µÏÇ¨ (ÌïôÏäµÎêú ÏûÖÎ†• ÏûÑÎ≤†Îî©ÏùÑ Ï∂úÎ†•ÏóêÎèÑ Ï†ÅÏö©)\n",
    "            if embed_tokens.weight.shape == lm_head.weight.shape:\n",
    "                print(\"üîß [FIX] Copying Embeddings to LM Head (Fixing Garbage Output)...\")\n",
    "                # CloneÌïòÏó¨ Î≥µÏÇ¨ (Î©îÎ™®Î¶¨ Í≥µÏú†Í∞Ä ÏïÑÎãàÎùº Í∞í Î≥µÏÇ¨)\n",
    "                lm_head.weight.data = embed_tokens.weight.data.clone()\n",
    "                print(\"‚úÖ [SUCCESS] LM Head fixed!\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Shape mismatch! Cannot tie weights.\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Modules not found for tying.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error during model repair: {e}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. Config & Path Setup\n",
    "# ==============================================================================\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "with initialize(version_base=None, config_path=\"configs\"):\n",
    "    cfg = compose(config_name=\"test_CHJ.yaml\")\n",
    "\n",
    "OmegaConf.set_struct(cfg, False)\n",
    "if \"trainer\" in cfg: cfg = OmegaConf.merge(cfg, cfg.trainer)\n",
    "if \"gnn\" in cfg: cfg = OmegaConf.merge(cfg, cfg.gnn)\n",
    "\n",
    "# Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "root_dir = \"/home/jovyan/CHJ/Mol-LLM_Custom\"\n",
    "main_ckpt_path = os.path.join(root_dir, \"checkpoint/Custom/mol-llm.ckpt\")\n",
    "token_path = os.path.join(root_dir, \"model/selfies_dict.txt\")\n",
    "\n",
    "if hasattr(cfg, \"gine\"): cfg.gine.graph_encoder_ckpt = main_ckpt_path\n",
    "if hasattr(cfg, \"tokengt\"): cfg.tokengt.graph_encoder_ckpt = main_ckpt_path\n",
    "cfg.selfies_token_path = token_path\n",
    "OmegaConf.set_struct(cfg, True)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Î™®Îç∏ Ï¥àÍ∏∞Ìôî Î∞è Î°úÎìú\n",
    "# ==============================================================================\n",
    "print(f\"Initializing Model...\")\n",
    "model = Blip2Stage3(cfg) \n",
    "\n",
    "# [ÏàòÏ†ïÎêú Î°úÎçî Ìò∏Ï∂ú]\n",
    "load_and_fix_model(model, main_ckpt_path)\n",
    "\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Moving model to {device} and converting to bfloat16...\")\n",
    "model.to(device)\n",
    "model.to(torch.bfloat16)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
    "# ==============================================================================\n",
    "tokenizer = model.blip2model.llm_tokenizer\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "mol_token_id = tokenizer.mol_token_id\n",
    "mol_token = \"<mol>\"\n",
    "num_query_tokens = 32\n",
    "\n",
    "# ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞\n",
    "input_selfies = \"[C][O][C@H1][C][Branch1][C][C][=C][C@H1][C][C@H1][Ring1][Ring1][Ring1][#Branch1]\"\n",
    "instruction_template = \"Please provide the HOMO energy value for this molecule: : <INPUT> \"\n",
    "system_prompt = \"You are a helpful assistant for molecular chemistry, to address tasks including molecular property classification, molecular property regression, chemical reaction prediction, molecule captioning, molecule generation.\"\n",
    "\n",
    "\"\"\"\n",
    "<s>[INST] You are a helpful assistant for molecular chemistry, to address tasks including molecular property classification, molecular property regression, chemical reaction prediction, molecule captioning, molecule generation. \\n\\nPlease provide the HOMO energy value for this molecule: <SELFIES> [C][O][C@H1][C][Branch1][C][C][=C][C@H1][C][C@H1][Ring1][Ring1][Ring1][#Branch1] </SELFIES><GRAPH><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol></GRAPH>. [/INST] \n",
    "\"\"\"\n",
    "\n",
    "def smiles2data(smiles):\n",
    "    try:\n",
    "        graph = smiles2graph(smiles)\n",
    "        x = torch.from_numpy(graph[\"node_feat\"]).long()\n",
    "        edge_index = torch.from_numpy(graph[\"edge_index\"]).long()\n",
    "        edge_attr = torch.from_numpy(graph[\"edge_feat\"]).long()\n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "smiles = sf.decoder(input_selfies)\n",
    "graph = smiles2data(smiles)\n",
    "graph_batch = Batch.from_data_list([graph]).to(device)\n",
    "additional_graph_batch = Batch.from_data_list([graph]).to(device)\n",
    "\n",
    "formatted_selfies = f\"<SELFIES> {input_selfies} </SELFIES>\"\n",
    "graph_placeholder = \"<GRAPH>\" + mol_token * num_query_tokens + \"</GRAPH>\"\n",
    "input_mol_string = formatted_selfies + graph_placeholder\n",
    "final_instruction = instruction_template.replace(\"<INPUT>\", input_mol_string)\n",
    "full_prompt = f\"<s>[INST] {system_prompt} \\n\\n{final_instruction} [/INST]\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [full_prompt], \n",
    "    return_tensors=\"pt\", \n",
    "    add_special_tokens=False, \n",
    "    padding=True\n",
    ")\n",
    "input_ids = inputs.input_ids.to(device)\n",
    "attention_mask = inputs.attention_mask.to(device)\n",
    "is_mol_token = (input_ids == mol_token_id)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Ï∂îÎ°† Ïã§Ìñâ\n",
    "# ==============================================================================\n",
    "print(\"\\nGenerating response...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        outputs = model.blip2model.generate(\n",
    "            graphs=(graph_batch, additional_graph_batch),\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            is_mol_token=is_mol_token,\n",
    "            num_beams=5,             \n",
    "            max_length=256, \n",
    "            min_length=1,\n",
    "            do_sample=False, \n",
    "            repetition_penalty=1.0\n",
    "        )\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. Í≤∞Í≥º Ï∂úÎ†•\n",
    "# ==============================================================================\n",
    "raw_prediction = outputs.predictions[0]\n",
    "clean_prediction = raw_prediction.replace(tokenizer.pad_token, \"\").replace(\"</s>\", \"\").strip()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Prediction Result:\")\n",
    "print(clean_prediction)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91da0410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'alchemy_homo',\n",
       " 'x': [[5, 0, 4, 5, 3, 0, 2, 0, 0],\n",
       "  [7, 0, 2, 5, 0, 0, 2, 0, 0],\n",
       "  [5, 2, 4, 5, 1, 0, 2, 0, 1],\n",
       "  [5, 2, 4, 5, 1, 0, 2, 0, 1],\n",
       "  [5, 0, 4, 5, 2, 0, 2, 0, 1],\n",
       "  [5, 2, 4, 5, 1, 0, 2, 0, 1],\n",
       "  [5, 0, 3, 5, 1, 0, 1, 0, 1],\n",
       "  [5, 0, 3, 5, 0, 0, 1, 0, 1],\n",
       "  [5, 0, 4, 5, 3, 0, 2, 0, 0]],\n",
       " 'edge_index': [[0, 1, 1, 2, 2, 3, 2, 7, 3, 4, 3, 5, 4, 5, 5, 6, 6, 7, 7, 8],\n",
       "  [1, 0, 2, 1, 3, 2, 7, 2, 4, 3, 5, 3, 5, 4, 6, 5, 7, 6, 8, 7]],\n",
       " 'edge_attr': [[0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [1, 0, 0],\n",
       "  [1, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0]],\n",
       " 'additional_x': [[5, 0, 4, 5, 3, 0, 2, 0, 0],\n",
       "  [7, 0, 2, 5, 0, 0, 2, 0, 0],\n",
       "  [5, 2, 4, 5, 1, 0, 2, 0, 1],\n",
       "  [5, 2, 4, 5, 1, 0, 2, 0, 1],\n",
       "  [5, 0, 4, 5, 2, 0, 2, 0, 1],\n",
       "  [5, 2, 4, 5, 1, 0, 2, 0, 1],\n",
       "  [5, 0, 3, 5, 1, 0, 1, 0, 1],\n",
       "  [5, 0, 3, 5, 0, 0, 1, 0, 1],\n",
       "  [5, 0, 4, 5, 3, 0, 2, 0, 0]],\n",
       " 'additional_edge_index': [[0,\n",
       "   1,\n",
       "   1,\n",
       "   2,\n",
       "   2,\n",
       "   3,\n",
       "   2,\n",
       "   7,\n",
       "   3,\n",
       "   4,\n",
       "   3,\n",
       "   5,\n",
       "   4,\n",
       "   5,\n",
       "   5,\n",
       "   6,\n",
       "   6,\n",
       "   7,\n",
       "   7,\n",
       "   8],\n",
       "  [1, 0, 2, 1, 3, 2, 7, 2, 4, 3, 5, 3, 5, 4, 6, 5, 7, 6, 8, 7]],\n",
       " 'additional_edge_attr': [[0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [1, 0, 0],\n",
       "  [1, 0, 0],\n",
       "  [0, 0, 0],\n",
       "  [0, 0, 0]],\n",
       " 'input_mol_string': '<SELFIES> [C][O][C@H1][C][Branch1][C][C][=C][C@H1][C][C@H1][Ring1][Ring1][Ring1][#Branch1] </SELFIES>',\n",
       " 'prompt_text': '<s>[INST] You are a helpful assistant for molecular chemistry, to address tasks including molecular property classification, molecular property regression, chemical reaction prediction, molecule captioning, molecule generation. \\n\\nPlease provide the HOMO energy value for this molecule: <SELFIES> [C][O][C@H1][C][Branch1][C][C][=C][C@H1][C][C@H1][Ring1][Ring1][Ring1][#Branch1] </SELFIES><GRAPH><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol><mol></GRAPH>. [/INST] ',\n",
       " 'target_text': '<FLOAT> <|-|><|0|><|.|><|2|><|1|><|7|><|5|> </FLOAT> </s>'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "data = load_from_disk('/home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/mol-llm_testset')\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c138a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MolDA_CHJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
