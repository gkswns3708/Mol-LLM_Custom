# Checkpoint ì €ì¥ ë° ì¬ê°œ ê°€ì´ë“œ

## ğŸ“ Checkpoint ì €ì¥ ì„¤ì •

### 1. Step ê¸°ë°˜ ìë™ ì €ì¥

`configs/trainer/llada8b.yaml` íŒŒì¼ì—ì„œ ì„¤ì •:

```yaml
# N stepë§ˆë‹¤ checkpoint ì €ì¥
save_on_n_steps: 500              # 500 stepë§ˆë‹¤ ì €ì¥
save_top_k_checkpoints: 5         # ìµœê·¼ 5ê°œë§Œ ìœ ì§€ (-1 = ëª¨ë‘ ì €ì¥)

# Best model ì €ì¥
save_top_k_best: 3                # ìƒìœ„ 3ê°œ best ëª¨ë¸ ìœ ì§€

# Epoch ê¸°ë°˜ ì¶”ê°€ ì €ì¥ (ì„ íƒ)
save_every_n_epochs: 1            # 1 epochë§ˆë‹¤ ì¶”ê°€ ì €ì¥ (0 = ë¹„í™œì„±í™”)
```

### 2. ì €ì¥ë˜ëŠ” Checkpoint íŒŒì¼

í•™ìŠµ ì¤‘ ë‹¤ìŒê³¼ ê°™ì€ checkpointë“¤ì´ ìë™ ì €ì¥ë©ë‹ˆë‹¤:

```
checkpoint/Custom_LLaDA/stage1_llm_pretraining/
â”œâ”€â”€ epoch=00-step=000500-train.ckpt    # 500 step
â”œâ”€â”€ epoch=00-step=001000-train.ckpt    # 1000 step
â”œâ”€â”€ epoch=01-step=001500-train.ckpt    # 1500 step
â”œâ”€â”€ last.ckpt                          # ê°€ì¥ ìµœê·¼ checkpoint (ìë™ ì—…ë°ì´íŠ¸)
â”œâ”€â”€ best_20231231_epoch=02_step=003500_loss=0.1234.ckpt  # Best #1
â”œâ”€â”€ best_20231231_epoch=03_step=004200_loss=0.1567.ckpt  # Best #2
â””â”€â”€ best_20231231_epoch=04_step=005100_loss=0.1892.ckpt  # Best #3
```

---

## ğŸ”„ í•™ìŠµ ì¬ê°œ (Resume Training)

### ë°©ë²• 1: Config íŒŒì¼ì—ì„œ ì„¤ì •

`configs/train_llada.yaml` íŒŒì¼ ìˆ˜ì •:

```yaml
# íŠ¹ì • step checkpointì—ì„œ ì¬ê°œ
ckpt_path: "/home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom_LLaDA/stage1_llm_pretraining/epoch=03-step=001500-train.ckpt"

# ë˜ëŠ” ê°€ì¥ ìµœê·¼ checkpointì—ì„œ ì¬ê°œ
ckpt_path: "/home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom_LLaDA/stage1_llm_pretraining/last.ckpt"
```

ê·¸ í›„ í•™ìŠµ ì‹¤í–‰:
```bash
python stage3.py
```

### ë°©ë²• 2: ì»¤ë§¨ë“œë¼ì¸ì—ì„œ ì§€ì •

```bash
python stage3.py ckpt_path="/path/to/checkpoint/epoch=03-step=001500-train.ckpt"
```

### Resume ì‹œ ë³µì›ë˜ëŠ” í•­ëª©

- âœ… **ëª¨ë¸ ê°€ì¤‘ì¹˜** (Model weights)
- âœ… **Optimizer ìƒíƒœ** (í•™ìŠµë¥ , momentum ë“±)
- âœ… **Learning rate scheduler ìƒíƒœ**
- âœ… **í˜„ì¬ epoch/step ë²ˆí˜¸**
- âœ… **Best validation loss**
- âœ… **ë‚œìˆ˜ ìƒì„±ê¸° ìƒíƒœ** (ì¬í˜„ì„± ë³´ì¥)

---

## ğŸ¯ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ë¡œë“œ (Fine-tuning)

ì²˜ìŒë¶€í„° í•™ìŠµì„ ì‹œì‘í•˜ì§€ë§Œ íŠ¹ì • ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë§Œ ë¡œë“œí•˜ëŠ” ê²½ìš°:

### Config íŒŒì¼ ì„¤ì •

```yaml
# pretrained model ê°€ì¤‘ì¹˜ë§Œ ë¡œë“œ (optimizerëŠ” ì´ˆê¸°í™”)
pretrained_ckpt_path: "/home/jovyan/CHJ/Mol-LLM_Custom/checkpoint/Custom_LLaDA/stage1_llm_pretraining/epoch=07-step=051600-train.ckpt"

# ckpt_pathëŠ” nullë¡œ ìœ ì§€
ckpt_path: null
```

### âš ï¸ ì£¼ì˜ì‚¬í•­

- `ckpt_path`ì™€ `pretrained_ckpt_path`ëŠ” **ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤**
- `pretrained_ckpt_path` ì‚¬ìš© ì‹œ:
  - ëª¨ë¸ ê°€ì¤‘ì¹˜ë§Œ ë¡œë“œ
  - Optimizer, schedulerëŠ” ì´ˆê¸°í™”
  - Epoch/Stepì€ 0ë¶€í„° ì‹œì‘

---

## ğŸ“Š Checkpoint íŒŒì¼ëª… í˜•ì‹

### Step-based Checkpoint
```
epoch={epoch:02d}-step={step:06d}-train.ckpt
ì˜ˆ: epoch=03-step=001500-train.ckpt
```
- `epoch`: í˜„ì¬ epoch (2ìë¦¬)
- `step`: ì „ì—­ step ë²ˆí˜¸ (6ìë¦¬)
- `train`: í•™ìŠµ ì¤‘ ì €ì¥ëœ checkpoint

### Best Checkpoint
```
best_{ë‚ ì§œ}_epoch={epoch:02d}_step={step:06d}_loss={val_loss:.4f}.ckpt
ì˜ˆ: best_20231231_epoch=02_step=003500_loss=0.1234.ckpt
```
- ë‚ ì§œ: checkpoint ì €ì¥ ë‚ ì§œ
- `val_loss`: Validation loss ê°’

---

## ğŸ’¡ í™œìš© ì˜ˆì‹œ

### 1. í•™ìŠµ ì¤‘ë‹¨ í›„ ì¬ê°œ

í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆì„ ë•Œ:

```bash
# ê°€ì¥ ìµœê·¼ checkpointì—ì„œ ì¬ê°œ
python stage3.py ckpt_path="checkpoint/Custom_LLaDA/stage1_llm_pretraining/last.ckpt"
```

### 2. íŠ¹ì • Stepì—ì„œ ì¬ê°œ

íŠ¹ì • stepë¶€í„° ë‹¤ì‹œ ì‹¤í—˜í•˜ê³  ì‹¶ì„ ë•Œ:

```bash
python stage3.py ckpt_path="checkpoint/Custom_LLaDA/stage1_llm_pretraining/epoch=03-step=001500-train.ckpt"
```

### 3. Best ëª¨ë¸ë¡œ ì¶”ê°€ í•™ìŠµ

Best validation lossë¥¼ ê¸°ë¡í•œ ëª¨ë¸ì—ì„œ ê³„ì† í•™ìŠµ:

```bash
python stage3.py ckpt_path="checkpoint/Custom_LLaDA/stage1_llm_pretraining/best_20231231_epoch=02_step=003500_loss=0.1234.ckpt"
```

### 4. Checkpoint ì •ë¦¬ (ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½)

í•„ìš” ì—†ëŠ” ì¤‘ê°„ checkpoint ì‚­ì œ:

```bash
cd checkpoint/Custom_LLaDA/stage1_llm_pretraining/

# Step checkpointë§Œ ì‚­ì œ (best, lastëŠ” ìœ ì§€)
rm epoch=*-step=*-train.ckpt

# ë˜ëŠ” íŠ¹ì • step ì´ì „ checkpointë§Œ ì‚­ì œ
rm epoch=00-step=00*.ckpt
```

---

## ğŸ” Checkpoint ì •ë³´ í™•ì¸

ì €ì¥ëœ checkpointì˜ ì •ë³´ë¥¼ í™•ì¸í•˜ë ¤ë©´:

```python
import torch

ckpt = torch.load("checkpoint/path/to/file.ckpt", map_location="cpu")
print(f"Epoch: {ckpt['epoch']}")
print(f"Global step: {ckpt['global_step']}")
print(f"Best validation loss: {ckpt.get('callbacks', {}).get('ModelCheckpoint', {}).get('best_model_score', 'N/A')}")
```

---

## âš™ï¸ ê³ ê¸‰ ì„¤ì •

### ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½

ìì£¼ ì €ì¥í•˜ë˜ ì˜¤ë˜ëœ checkpointëŠ” ìë™ ì‚­ì œ:

```yaml
save_on_n_steps: 100              # ìì£¼ ì €ì¥
save_top_k_checkpoints: 10        # ìµœê·¼ 10ê°œë§Œ ìœ ì§€
```

### ëª¨ë“  Checkpoint ë³´ê´€

ì‹¤í—˜ ì¬í˜„ì„ ìœ„í•´ ëª¨ë“  checkpoint ì €ì¥:

```yaml
save_on_n_steps: 500
save_top_k_checkpoints: -1        # ëª¨ë‘ ì €ì¥ (ì£¼ì˜: ë””ìŠ¤í¬ ê³µê°„ ë§ì´ ì‚¬ìš©)
```

### Validation ê¸°ë°˜ ì €ì¥ë§Œ ì‚¬ìš©

Step ì €ì¥ ë¹„í™œì„±í™”í•˜ê³  best modelë§Œ ì €ì¥:

```yaml
save_on_n_steps: 0                # Step ì €ì¥ ë¹„í™œì„±í™”
save_top_k_best: 5                # Best 5ê°œë§Œ ìœ ì§€
```

---

## ğŸ“ ê¶Œì¥ ì„¤ì •

ì¼ë°˜ì ì¸ í•™ìŠµ:
```yaml
save_on_n_steps: 500              # 500 stepë§ˆë‹¤
save_top_k_checkpoints: 5         # ìµœê·¼ 5ê°œ ìœ ì§€
save_top_k_best: 3                # Best 3ê°œ ìœ ì§€
```

ê¸´ í•™ìŠµ (ë©°ì¹ ):
```yaml
save_on_n_steps: 1000             # ëœ ìì£¼ ì €ì¥
save_top_k_checkpoints: 3         # ë””ìŠ¤í¬ ê³µê°„ ì ˆì•½
save_top_k_best: 5                # ë” ë§ì€ best ëª¨ë¸ ìœ ì§€
```

ì§§ì€ ì‹¤í—˜:
```yaml
save_on_n_steps: 100              # ìì£¼ ì €ì¥
save_top_k_checkpoints: -1        # ëª¨ë‘ ì €ì¥
save_top_k_best: 1                # Best 1ê°œë§Œ
```
