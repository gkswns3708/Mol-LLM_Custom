# Global Option
debug: true
print_line: 100
# Q-Former Configuration
bert_hidden_dim: 768
bert_name: scibert
cross_attention_freq: 2
num_query_token: 32
bert_num_hidden_layers: 5

projector_type: qformer

# LLM Configuration
llm_model: "GSAI-ML/LLaDA-8B-Instruct"
tune_llm: lora
peft_config: "Mol-LLM_Custom/lora_config_llada.json"
peft_dir: ""
load_in_8bit: false
lora_r: 64
lora_alpha: 32
lora_dropout: 0.1
selfies_token_path: Mol-LLM_Custom/model/selfies_dict.txt
add_selfies_tokens: true
prompt: "[START_I_SMILES]{}[END_I_SMILES]" #! 이거 안 쓰기는 하는데, 일단 사용 중

num_beams: 1

strategy_name: null
accelerator: gpu
devices: "0,1,2,3,4,5,6,7"
precision: bf16-mixed
max_steps: -1
max_epochs: 30
every_n_epochs: 1
task: null
logging_dir: Mol-LLM_Custom/checkpoint/Custom_LLaDA
llava_pretraining: 0
second_stage_start_epoch: 0
num_workers: 0
skip_sanity_check: true 
num_sanity_val_steps: 0

# total_batch_size: 1024
# batch_size: 8
# inference_batch_size: 11
accumulate_grad_batches: 16
total_batch_size: 256
batch_size: 4

# ==================== Checkpoint Configuration ====================
# Step-based checkpoint saving
save_on_n_steps: 500              # N step마다 checkpoint 저장 (0 = 비활성화)
save_top_k_checkpoints: -1        # -1 = 모두 저장 (주의: 디스크 공간 많이 사용)
                                   # 참고: save_top_k > 0 설정 시 monitor 필요 (현재 미구현)

# Best model checkpoints (validation loss 기준)
save_top_k_best: 3                # 상위 N개 best 모델 유지

# Optional: Epoch-based checkpoint saving
save_every_n_epochs: 0            # N epoch마다 추가 저장 (0 = 비활성화)
# ==================================================================

# Inference Conficuation
inference_batch_size: 11
sampling_steps: 16 #! 추후 최종적인 Metric 계산시에는 sequence 길이로 수정 예정

truncation: 1
padding: max_length
max_length: 512
inference_max_length: 512

gen_max_len: 256
min_len: 8

apply_sequence_packing: false
max_packing_size: -1

weight_decay: 0.1               
min_lr: 0.0000025               
init_lr: 0.00025               
warmup_lr: 0.0                  
warmup_steps: 10                
# warmup_epochs: 0.5            
scheduler: warmup_stable_decay_lr 
optimizer: adamw
log_every_n_steps: 10
gradient_clip_val: 1.0

# Validation Configuration
val_check_interval: 1.0
check_val_every_n_epoch: 1  # 10 epoch마다 validation
test_on_trainset: false

# Test Inference Configuration
# val_check_interval: 0.0001
# check_val_every_n_epoch: 1
# test_on_trainset: false
# limit_val_batches: 0.01

# multimodal training
mol_representation: string_only
log_attn_score: true
eval_modality_util: null
tune_gnn: false

# molpo
train_molpo: false
eval_molpo: false

find_unused_parameters: false

selfies_enumeration: false
isomericSmiles: false
canonical: false
allHsExplicit: false


# Special LLaDA Configuration
remasking_strategy: 'random' # low_confidence도 존재.

# Semi-Autoregressive Generation Configuration
# Multi-task 환경에서 format token 혼란 문제를 해결하기 위한 설정
use_semi_ar: true              # Semi-AR 생성 모드 활성화 (단일 전략 사용 시)
                               # true: format tokens를 먼저 고정한 후 content만 diffusion
                               # false: 기존 방식 (전체 시퀀스 diffusion)

# ==================== Multi-Strategy Validation Configuration ====================
# Validation 시 여러 generation 전략을 동시에 평가
# 각 전략별로 별도의 prediction 파일과 metric이 생성됨
#
# 사용 가능한 전략:
#   - random: 기존 방식 (전체 diffusion + random remasking)
#   - semi_ar: Semi-AR + random remasking
#   - low_confidence: 전체 diffusion + low_confidence remasking
#   - semi_ar_low_confidence: Semi-AR + low_confidence remasking
#
# 예시:
#   val_strategies: ["random", "semi_ar"]  # 두 전략 비교
#   val_strategies: ["semi_ar", "semi_ar_low_confidence"]  # Semi-AR with 다른 remasking
#   val_strategies: null  # use_semi_ar 설정에 따라 단일 전략 사용
# ===============================================================================
val_strategies: ["random", "semi_ar"]  # null이면 use_semi_ar 설정 따름

# ==================== Debug Logging Configuration ====================
# Special token embedding 학습 상태 로깅
log_embedding_status: true           # embedding 상태 로깅 활성화
embedding_log_interval: 500          # N step마다 로깅 (0 = 비활성화)

# 모델 초기화 시 상세 로깅 (requires_grad 상태 등)
log_model_init_details: true         # 모델 초기화 상세 로깅

# NaN/Inf 발생 시 상세 로깅
log_nan_details: true                # NaN 발생 시 상세 정보 저장
nan_log_dir: './nan_logs'            # NaN 로그 저장 디렉토리

# ==================== NEW TOKEN DEBUG ANALYSIS ====================
# 새로 추가된 토큰(SELFIES 등)의 학습 상태 분석
# wandb에 다음 항목들이 로깅됩니다:
#   - token_ratio/*: 새 토큰 비율 (input, masked)
#   - loss/*: 새 토큰 vs 기존 토큰 loss 비교
#   - accuracy/*: 새 토큰 예측 정확도
#   - grad/*: embedding/head gradient 분석
log_new_token_debug: true            # 새 토큰 디버깅 로깅 활성화
new_token_debug_interval: 100        # N step마다 분석 (0 = 비활성화)
original_vocab_size: 128256          # LLaDA 기본 vocab size (새 토큰 구분용)
# ==================================================================