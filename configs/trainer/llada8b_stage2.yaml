# Global Option
debug: true
print_line: 100
# Q-Former Configuration
bert_hidden_dim: 768
bert_name: scibert
cross_attention_freq: 2
num_query_token: 32
bert_num_hidden_layers: 5

projector_type: qformer

# LLM Configuration
llm_model: "GSAI-ML/LLaDA-8B-Instruct"
tune_llm: lora
peft_config: "Mol-LLM_Custom/lora_config_llada.json"
peft_dir: ""
load_in_8bit: false
lora_r: 64
lora_alpha: 32
lora_dropout: 0.1
selfies_token_path: Mol-LLM_Custom/model/selfies_dict.txt
add_selfies_tokens: true
prompt: "[START_I_SMILES]{}[END_I_SMILES]" #! 이거 안 쓰기는 하는데, 일단 사용 중

num_beams: 1

strategy_name: null
accelerator: gpu
devices: "0,1,2,3,4,5,6,7"
precision: bf16-mixed
max_steps: -1
max_epochs: 12
every_n_epochs: 1
task: null
logging_dir: Mol-LLM_Custom/checkpoint/Custom_LLaDA
llava_pretraining: 0
second_stage_start_epoch: 0
num_workers: 0
skip_sanity_check: true 
num_sanity_val_steps: 0

# total_batch_size: 1024
# batch_size: 8
# inference_batch_size: 11
accumulate_grad_batches: 11
total_batch_size: 512
batch_size: 4
save_on_n_steps: 1000 # 100일 때 막 1epoch에 50개씩 저장됨.
# Inference Conficuation
inference_batch_size: 11
sampling_steps: 16 #! 추후 최종적인 Metric 계산시에는 sequence 길이로 수정 예정

truncation: 1
padding: max_length
max_length: 512
inference_max_length: 512

gen_max_len: 256
min_len: 8

apply_sequence_packing: false
max_packing_size: -1

weight_decay: 0.1               
min_lr: 0.0000025               
init_lr: 0.000025               
warmup_lr: 0.0                  
warmup_steps: 50                
# warmup_epochs: 0.5            
scheduler: warmup_stable_decay_lr 
optimizer: adamw
log_every_n_steps: 10
gradient_clip_val: 1.0          

# Real validation Configuration
val_check_interval: 1.0 # 무조건 소수로 해야함.
check_val_every_n_epoch: 1
test_on_trainset: false

# Test Inference Configuration
# val_check_interval: 0.0001
# check_val_every_n_epoch: 1
# test_on_trainset: false
# limit_val_batches: 0.01

# multimodal training
mol_representation: string_only
log_attn_score: true
eval_modality_util: null
tune_gnn: false

# molpo
train_molpo: false
eval_molpo: false

find_unused_parameters: false

selfies_enumeration: false
isomericSmiles: false
canonical: false
allHsExplicit: false


# Special LLaDA Configuration
# remasking_strategy 옵션:
#   - 'low_confidence': Algorithm 5 - 낮은 confidence 토큰을 다시 mask (LLaDA 논문 권장)
#   - 'random': Algorithm 4 - 랜덤하게 다시 mask
#   - 'none': Remasking 없이 매 step마다 top-k 토큰만 unmask (기존 방식)
remasking_strategy: 'random'

# Semi-Autoregressive 생성 모드
use_semi_ar: false
semi_ar_block_size: 32
val_strategies: null